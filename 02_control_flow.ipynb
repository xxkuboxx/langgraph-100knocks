{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2章: グラフの制御フロー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "以下のセルを順番に実行して、演習に必要な環境をセットアップします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "\n",
    "このセルは、LangGraphおよび関連するLangChainライブラリをインストールします。実行には数分かかる場合があります。\n",
    "ご利用になるLLMプロバイダーに応じて、コメントアウトを解除して必要なライブラリをインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ライブラリのインストール ===\n",
    "# ご利用になるLLMプロバイダーに応じて、以下のコメントを解除して実行してください。\n",
    "!%pip install -qU langchain langgraph langchain_openai langchain_google_vertexai langchain_google_genai langchain_anthropic langchain_aws boto3 ipython_genutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMプロバイダーの選択\n",
    "\n",
    "このセルでは、使用するLLMプロバイダーを選択します。\n",
    "`LLM_PROVIDER` 変数に、利用したいプロバイダー名を設定してください。\n",
    "選択可能なプロバイダー: `\"openai\"`, `\"azure\"`, `\"google\"` (Vertex AI), `\"google_genai\"` (Gemini API), `\"anthropic\"`, `\"bedrock\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMプロバイダーの選択 ===\n",
    "# 利用したいLLMプロバイダーを以下の変数で指定してください。\n",
    "# \"openai\", \"azure\", \"google\" (Vertex AI), \"google_genai\" (Gemini API), \"anthropic\", \"bedrock\" のいずれかを選択できます。\n",
    "LLM_PROVIDER = \"openai\"  # 例: OpenAI を利用する場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIキー/環境変数の設定\n",
    "\n",
    "以下のセルを実行する前に、選択したLLMプロバイダーに応じたAPIキーまたは環境変数を設定する必要があります。\n",
    "\n",
    "**一般的な手順:**\n",
    "1.  `.env.sample` ファイルをコピーして `.env` ファイルを作成します。\n",
    "2.  `.env` ファイルを開き、選択したLLMプロバイダーに対応するAPIキーや必要な情報を記述します。\n",
    "    *   **OpenAI:** `OPENAI_API_KEY`\n",
    "    *   **Azure OpenAI:** `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `OPENAI_API_VERSION`, `AZURE_OPENAI_DEPLOYMENT_NAME`\n",
    "    *   **Google (Vertex AI):** `GOOGLE_CLOUD_PROJECT_ID`, `GOOGLE_CLOUD_LOCATION` (Colab環境外で実行する場合、`GOOGLE_APPLICATION_CREDENTIALS` 環境変数の設定も必要になることがあります)\n",
    "    *   **Google (Gemini API):** `GOOGLE_API_KEY`\n",
    "    *   **Anthropic:** `ANTHROPIC_API_KEY`\n",
    "    *   **AWS Bedrock:** `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION_NAME` (IAMロールを使用する場合は、これらのキー設定は不要な場合がありますが、リージョン名は必須です)\n",
    "3.  ファイルを保存します。\n",
    "\n",
    "**Google Colab を使用している場合:**\n",
    "上記の `.env` ファイルを使用する代わりに、Colabのシークレットマネージャーに必要なキーを登録してください。\n",
    "例えば、OpenAIを使用する場合は `OPENAI_API_KEY` という名前でシークレットを登録します。\n",
    "Vertex AI を利用する場合は、Colab上での認証 (`google.colab.auth.authenticate_user()`) が実行されます。\n",
    "\n",
    "このセルは、設定された情報に基づいて環境変数をロードし、LLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === APIキー/環境変数の設定 ===\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envファイルから環境変数を読み込む (存在する場合)\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# --- OpenAI ---\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY and IS_COLAB:\n",
    "        OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"OpenAI APIキーが設定されていません。環境変数 OPENAI_API_KEY を設定するか、Colab環境の場合はシークレットに OPENAI_API_KEY を設定してください。\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# --- Azure OpenAI ---\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not AZURE_OPENAI_API_KEY: AZURE_OPENAI_API_KEY = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not AZURE_OPENAI_ENDPOINT: AZURE_OPENAI_ENDPOINT = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        if not OPENAI_API_VERSION: OPENAI_API_VERSION = userdata.get(\"OPENAI_API_VERSION\") # 例: \"2023-07-01-preview\"\n",
    "        if not AZURE_OPENAI_DEPLOYMENT_NAME: AZURE_OPENAI_DEPLOYMENT_NAME = userdata.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if not AZURE_OPENAI_API_KEY: raise ValueError(\"Azure OpenAI APIキー (AZURE_OPENAI_API_KEY) が設定されていません。\")\n",
    "    if not AZURE_OPENAI_ENDPOINT: raise ValueError(\"Azure OpenAI エンドポイント (AZURE_OPENAI_ENDPOINT) が設定されていません。\")\n",
    "    if not OPENAI_API_VERSION: OPENAI_API_VERSION = \"2023-07-01-preview\" # デフォルトを設定することも可能\n",
    "    if not AZURE_OPENAI_DEPLOYMENT_NAME: raise ValueError(\"Azure OpenAI デプロイメント名 (AZURE_OPENAI_DEPLOYMENT_NAME) が設定されていません。\")\n",
    "\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "    os.environ[\"OPENAI_API_VERSION\"] = OPENAI_API_VERSION\n",
    "\n",
    "# --- Google Cloud Vertex AI (Gemini) ---\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT_ID\") # .env 用に修正\n",
    "    LOCATION = os.environ.get(\"GOOGLE_CLOUD_LOCATION\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not PROJECT_ID: PROJECT_ID = userdata.get(\"GOOGLE_CLOUD_PROJECT_ID\")\n",
    "        if not LOCATION: LOCATION = userdata.get(\"GOOGLE_CLOUD_LOCATION\") # 例: \"us-central1\"\n",
    "        from google.colab import auth as google_auth\n",
    "        google_auth.authenticate_user() # Vertex AI を使う場合は Colab での認証を推奨\n",
    "    else: # Colab外の場合、.envから読み込んだ値で環境変数を設定\n",
    "        if PROJECT_ID: os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID # Vertex AI SDKが参照する標準的な環境変数名\n",
    "        if LOCATION: os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "\n",
    "    if not PROJECT_ID: raise ValueError(\"Google Cloud Project ID が設定されていません。環境変数 GOOGLE_CLOUD_PROJECT_ID を設定するか、Colab環境の場合はシークレットに GOOGLE_CLOUD_PROJECT_ID を設定してください。\")\n",
    "    if not LOCATION: LOCATION = \"us-central1\" # デフォルトロケーション\n",
    "\n",
    "# --- Google Gemini API (langchain-google-genai) ---\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY and IS_COLAB:\n",
    "        GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"Google APIキーが設定されていません。環境変数 GOOGLE_API_KEY を設定するか、Colab環境の場合はシークレットに GOOGLE_API_KEY を設定してください。\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# --- Anthropic (Claude) ---\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY and IS_COLAB:\n",
    "        ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        raise ValueError(\"Anthropic APIキーが設定されていません。環境変数 ANTHROPIC_API_KEY を設定するか、Colab環境の場合はシークレットに ANTHROPIC_API_KEY を設定してください。\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "# --- Amazon Bedrock (Claude) ---\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    AWS_REGION_NAME = os.environ.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if IS_COLAB: \n",
    "        if not AWS_ACCESS_KEY_ID: AWS_ACCESS_KEY_ID = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
    "        if not AWS_SECRET_ACCESS_KEY: AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        if not AWS_REGION_NAME: AWS_REGION_NAME = userdata.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if not AWS_REGION_NAME:\n",
    "         raise ValueError(\"AWSリージョン名 (AWS_REGION_NAME) が設定されていません。Bedrock利用にはリージョン指定が必要です。\")\n",
    "\n",
    "    # 環境変数に設定 (boto3がこれらを自動で読み込む)\n",
    "    if AWS_ACCESS_KEY_ID: os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "    if AWS_SECRET_ACCESS_KEY: os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION_NAME # boto3が参照する標準的なリージョン環境変数名\n",
    "    os.environ[\"AWS_REGION\"] = AWS_REGION_NAME # いくつかのライブラリはこちらを参照することもある\n",
    "\n",
    "print(f\"APIキー/環境変数の設定完了 (プロバイダー: {LLM_PROVIDER})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMクライアントの初期化\n",
    "\n",
    "このセルは、上で選択・設定したLLMプロバイダーに基づいて、対応するLLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMクライアントの動的初期化 ===\n",
    "llm = None\n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), # 環境変数から取得\n",
    "        openai_api_version=os.environ.get(\"OPENAI_API_VERSION\"), # 環境変数から取得\n",
    "        temperature=0,\n",
    "    )\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    from langchain_google_vertexai import ChatVertexAI\n",
    "    # PROJECT_ID, LOCATION は前のセルで環境変数に設定済みか、Colabの場合は直接利用\n",
    "    llm = ChatVertexAI(model_name=\"gemini-1.5-flash-001\", temperature=0, project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"), location=os.environ.get(\"GOOGLE_CLOUD_LOCATION\"))\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0, convert_system_message_to_human=True)\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    from langchain_anthropic import ChatAnthropic\n",
    "    llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    from langchain_aws import ChatBedrock # langchain_community.chat_models から langchain_aws に変更の可能性あり\n",
    "    # AWS_REGION_NAME は前のセルで環境変数 AWS_DEFAULT_REGION に設定済み\n",
    "    llm = ChatBedrock( # BedrockChat ではなく ChatBedrock が一般的\n",
    "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        # region_name=os.environ.get(\"AWS_DEFAULT_REGION\"), # 通常、boto3が環境変数から自動で読み込む\n",
    "        model_kwargs={\"temperature\": 0},\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Unsupported LLM_PROVIDER: {LLM_PROVIDER}. \"\n",
    "        \"Please choose from 'openai', 'azure', 'google', 'google_genai', 'anthropic', or 'bedrock'.\"\n",
    "    )\n",
    "\n",
    "print(f\"LLM Provider: {LLM_PROVIDER}\")\n",
    "if llm:\n",
    "    print(f\"LLM Client Type: {type(llm)}\")\n",
    "    # モデル名取得の試行を汎用的に\n",
    "    model_attr = getattr(llm, 'model', None) or \\ \n",
    "                 getattr(llm, 'model_name', None) or \\ \n",
    "                 getattr(llm, 'model_id', None) or \\ \n",
    "                 (hasattr(llm, 'llm') and getattr(llm.llm, 'model', None)) # 一部のLLMクライアントのネスト構造に対応\n",
    "    if hasattr(llm, 'azure_deployment') and not model_attr: # Azure特有の属性\n",
    "        model_attr = llm.azure_deployment\n",
    "        \n",
    "    if model_attr:\n",
    "        print(f\"LLM Model: {model_attr}\")\n",
    "    else:\n",
    "        print(\"LLM Model: (Could not determine model name from client attributes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この章では、条件付きエッジやループなど、グラフの流れを制御する方法を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題001: シンプルなカウンターによるループ\n",
    "\n",
    "指定された回数だけ特定の処理を繰り返す、基本的なループ構造を構築します。状態にカウンターを持ち、カウンターが上限に達するまでノードの実行を繰り返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001\n",
    "from typing import TypedDict, Annotated, Union\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class CounterState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    counter: int\n",
    "    max_count: int\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def increment_counter(state: CounterState):\n",
    "    # カウンターを1増やすノード\n",
    "    current_count = state.get(\"counter\", 0) + 1\n",
    "    print(f\"increment_counter: カウンター = {current_count}\")\n",
    "    return {\"messages\": [AIMessage(content=f\"カウント {current_count}\")], \"counter\": ____}\n",
    "\n",
    "def process_data(state: CounterState):\n",
    "    # 何らかの処理を行うノード（ここではメッセージを追加するだけ）\n",
    "    count = state[\"counter\"]\n",
    "    print(f\"process_data: 現在のカウント {count} で処理を実行中...\")\n",
    "    return {\"messages\": [AIMessage(content=f\"処理 {count} を実行しました。\")]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def should_continue(state: CounterState):\n",
    "    # カウンターが上限に達したかどうかを判定する\n",
    "    if state[\"counter\"] < ____:\n",
    "        print(f\"should_continue: ループ継続 (カウンター: {state['counter']}, 上限: {state['max_count']})\")\n",
    "        return \"continue_loop\" # ループを続ける場合の次のノード\n",
    "    else:\n",
    "        print(f\"should_continue: ループ終了 (カウンター: {state['counter']}, 上限: {state['max_count']})\")\n",
    "        return \"end_loop\" # ループを終了する場合\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(____)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"entry_node\", ____)\n",
    "workflow.add_node(\"processing_node\", ____)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(____)\n",
    "\n",
    "# 通常のエッジ\n",
    "workflow.add_edge(\"entry_node\", ____)\n",
    "\n",
    "# 条件付きエッジの追加 (ループまたは終了)\n",
    "workflow.add_conditional_edges(\n",
    "    \"processing_node\", # 遷移元のノード\n",
    "    should_continue,   # ルーター関数\n",
    "    {\n",
    "        \"continue_loop\": ____, # ルーター関数の戻り値とノード名のマッピング\n",
    "        \"end_loop\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- カウンターループのテスト (上限3回) ---\")\n",
    "initial_input = {\"messages\": [HumanMessage(content=\"ループ開始\")], \"max_count\": 3, \"counter\": 0}\n",
    "for s in app.stream(____):\n",
    "    print(s)\n",
    "\n",
    "final_state = app.invoke(____)\n",
    "print(f\"Final State: {final_state}\")\n",
    "\n",
    "print(\"\\n--- カウンターループのテスト (上限1回) ---\")\n",
    "initial_input_short = {\"messages\": [HumanMessage(content=\"短いループ開始\")], \"max_count\": 1, \"counter\": 0}\n",
    "for s in app.stream(____):\n",
    "    print(s)\n",
    "\n",
    "final_state_short = app.invoke(____)\n",
    "print(f\"Final State (Short Loop): {final_state_short}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答001</summary>\n",
    "\n",
    "``````python\n",
    "# 解答001\n",
    "from typing import TypedDict, Annotated, Union\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class CounterState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    counter: int\n",
    "    max_count: int\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def increment_counter(state: CounterState):\n",
    "    # カウンターを1増やすノード\n",
    "    current_count = state.get(\"counter\", 0) + 1\n",
    "    print(f\"increment_counter: カウンター = {current_count}\")\n",
    "    return {\"messages\": [AIMessage(content=f\"カウント {current_count}\")], \"counter\": current_count}\n",
    "\n",
    "def process_data(state: CounterState):\n",
    "    # 何らかの処理を行うノード（ここではメッセージを追加するだけ）\n",
    "    count = state[\"counter\"]\n",
    "    print(f\"process_data: 現在のカウント {count} で処理を実行中...\")\n",
    "    return {\"messages\": [AIMessage(content=f\"処理 {count} を実行しました。\")]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def should_continue(state: CounterState):\n",
    "    # カウンターが上限に達したかどうかを判定する\n",
    "    if state[\"counter\"] < state[\"max_count\"]:\n",
    "        print(f\"should_continue: ループ継続 (カウンター: {state['counter']}, 上限: {state['max_count']})\")\n",
    "        return \"continue_loop\" # ループを続ける場合の次のノード (entry_nodeへ戻る)\n",
    "    else:\n",
    "        print(f\"should_continue: ループ終了 (カウンター: {state['counter']}, 上限: {state['max_count']})\")\n",
    "        return \"end_loop\" # ループを終了する場合 (ENDへ)\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(CounterState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"entry_node\", increment_counter)\n",
    "workflow.add_node(\"processing_node\", process_data)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"entry_node\")\n",
    "\n",
    "# 通常のエッジ\n",
    "workflow.add_edge(\"entry_node\", \"processing_node\")\n",
    "\n",
    "# 条件付きエッジの追加 (ループまたは終了)\n",
    "workflow.add_conditional_edges(\n",
    "    \"processing_node\", # 遷移元のノード\n",
    "    should_continue,   # ルーター関数\n",
    "    {\n",
    "        \"continue_loop\": \"entry_node\", # ルーター関数の戻り値とノード名のマッピング\n",
    "        \"end_loop\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- カウンターループのテスト (上限3回) ---\")\n",
    "initial_input = {\"messages\": [HumanMessage(content=\"ループ開始\")], \"max_count\": 3, \"counter\": 0}\n",
    "for s in app.stream(initial_input):\n",
    "    print(s)\n",
    "\n",
    "final_state = app.invoke(initial_input)\n",
    "print(f\"Final State: {final_state}\")\n",
    "\n",
    "print(\"\\n--- カウンターループのテスト (上限1回) ---\")\n",
    "initial_input_short = {\"messages\": [HumanMessage(content=\"短いループ開始\")], \"max_count\": 1, \"counter\": 0}\n",
    "for s in app.stream(initial_input_short):\n",
    "    print(s)\n",
    "\n",
    "final_state_short = app.invoke(initial_input_short)\n",
    "print(f\"Final State (Short Loop): {final_state_short}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説001</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** `add_conditional_edges` を使用して、ノードから自身または他のノードへループバックする基本的なループ構造の作り方を学びます。状態(`State`)にカウンターとループの上限回数を保持し、ルーター関数でループを継続するか終了するかを判断します。\n",
    "*   **コード解説:**\n",
    "    *   `CounterState` には、メッセージ履歴(`messages`)に加え、現在のカウント数を保持する `counter` と、ループの最大回数を指定する `max_count` を定義します。\n",
    "    *   `increment_counter` ノード: グラフのループサイクルの開始点です。現在の `counter` を1増やし、状態を更新します。\n",
    "    *   `process_data` ノード: ループ内で実行したい主処理を行うノードです。ここでは単純にメッセージを追加しています。\n",
    "    *   `should_continue` ルーター関数: `processing_node` の後に呼び出されます。`counter` が `max_count` 未満であれば `\"continue_loop\"` を返し、`entry_node` に戻ってループを継続します。そうでなければ `\"end_loop\"` を返し、グラフは `END` に遷移して終了します。\n",
    "    *   `workflow.add_conditional_edges` の設定: `processing_node` から `should_continue` 関数の結果に応じて、`\"continue_loop\"` なら `entry_node` へ、`\"end_loop\"` なら `END` へと分岐するように定義しています。これがループ構造の核となります。\n",
    "    *   グラフ実行時には、`initial_input` で `max_count` と初期 `counter` (通常は0) を設定します。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題002: LLMによるループ継続判断（自己反省ループ）\n",
    "\n",
    "LLM自身が生成した内容を評価し、改善が必要であればループして再生成を行う「自己反省ループ」を構築します。LLMが特定のキーワード（例：「完璧」）を生成するまで、または指定回数ループするまで処理を繰り返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002\n",
    "from typing import TypedDict, Annotated, Union, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class SelfReflectionState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    current_iteration: int\n",
    "    max_iterations: int\n",
    "    target_keyword: str\n",
    "    current_answer: Optional[str] # LLMの最新の回答を保持\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def generate_answer_node(state: SelfReflectionState):\n",
    "    # LLMに回答を生成させるノード\n",
    "    iteration = state.get(\"current_iteration\", 0) + 1\n",
    "    print(f\"generate_answer_node: イテレーション {iteration}\")\n",
    "    \n",
    "    # プロンプトテンプレートの準備\n",
    "    if iteration == 1:\n",
    "        prompt_text = \"日本の首都はどこですか？簡潔に答えてください。\"\n",
    "    else:\n",
    "        previous_answer = state.get(\"current_answer\", \"\")\n",
    "        prompt_text = f\"前回の回答「{previous_answer}」は不十分でした。もっと良い回答を生成してください。特に「完璧」というキーワードを含めるように努力してください。日本の首都はどこですか？\"\n",
    "        \n",
    "    # LLM呼び出し\n",
    "    # 実際には、state[\"messages\"]にコンテキストを追加してLLMに渡すが、ここではシンプルにする\n",
    "    response = llm.invoke([HumanMessage(content=prompt_text)])\n",
    "    answer_content = response.content\n",
    "    print(f\"generate_answer_node: LLMの回答「{answer_content}」\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=answer_content)], \n",
    "        \"current_iteration\": ____, \n",
    "        \"current_answer\": ____\n",
    "    }\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def should_reflect_or_finish(state: SelfReflectionState):\n",
    "    # LLMの回答を評価し、ループを継続するか終了するかを判断する\n",
    "    current_answer = state.get(\"current_answer\", \"\")\n",
    "    current_iteration = state[\"current_iteration\"]\n",
    "    max_iterations = state[\"max_iterations\"]\n",
    "    target_keyword = state[\"target_keyword\"]\n",
    "    \n",
    "    print(f\"should_reflect_or_finish: 現在の回答「{current_answer}」、イテレーション {current_iteration}/{max_iterations}\")\n",
    "    \n",
    "    if ____ in current_answer:\n",
    "        print(f\"should_reflect_or_finish: ターゲットキーワード「{target_keyword}」を発見。ループ終了。\")\n",
    "        return \"finish\" # 終了条件\n",
    "    elif current_iteration >= ____:\n",
    "        print(f\"should_reflect_or_finish: 最大イテレーション {max_iterations} に到達。ループ終了。\")\n",
    "        return \"finish\" # 終了条件 (最大回数超過)\n",
    "    else:\n",
    "        print(\"should_reflect_or_finish: 改善が必要。ループ継続。\")\n",
    "        return \"reflect\" # ループ継続条件\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(____)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"generator\", ____)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(____)\n",
    "\n",
    "# 条件付きエッジの追加 (自己反省ループまたは終了)\n",
    "workflow.add_conditional_edges(\n",
    "    \"generator\",\n",
    "    should_reflect_or_finish, # ルーター関数\n",
    "    {\n",
    "        \"reflect\": ____,  # ルーターが \"reflect\" を返した場合、再度 generator ノードへ\n",
    "        \"finish\": END     # ルーターが \"finish\" を返した場合、グラフ終了\n",
    "    }\n",
    ")\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- 自己反省ループのテスト (ターゲット: '完璧', 最大3回) ---\")\n",
    "initial_input_reflect = {\n",
    "    \"messages\": [HumanMessage(content=\"自己反省ループを開始します。\")],\n",
    "    \"current_iteration\": 0,\n",
    "    \"max_iterations\": 3,\n",
    "    \"target_keyword\": \"完璧\",\n",
    "    \"current_answer\": None\n",
    "}\n",
    "for s in app.____(initial_input_reflect):\n",
    "    print(s)\n",
    "\n",
    "final_state_reflect = app.____(initial_input_reflect)\n",
    "print(f\"Final State (Self Reflection): {final_state_reflect}\")\n",
    "\n",
    "# (オプション) LLMがキーワードを生成しにくい場合があるので、最大回数で終了するケースも確認\n",
    "print(\"\\n--- 自己反省ループのテスト (ターゲット: 'ありえない言葉', 最大2回) ---\")\n",
    "initial_input_max_out = {\n",
    "    \"messages\": [HumanMessage(content=\"自己反省ループを開始します（最大回数超過テスト）。\")],\n",
    "    \"current_iteration\": 0,\n",
    "    \"max_iterations\": 2,\n",
    "    \"target_keyword\": \"ありえない言葉絶対に生成しないでね\",\n",
    "    \"current_answer\": None\n",
    "}\n",
    "for s in app.____(initial_input_max_out):\n",
    "    print(s)\n",
    "\n",
    "final_state_max_out = app.____(initial_input_max_out)\n",
    "print(f\"Final State (Max Iterations Out): {final_state_max_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答002</summary>\n",
    "\n",
    "``````python\n",
    "# 解答002\n",
    "from typing import TypedDict, Annotated, Union, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class SelfReflectionState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    current_iteration: int\n",
    "    max_iterations: int\n",
    "    target_keyword: str\n",
    "    current_answer: Optional[str] # LLMの最新の回答を保持\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def generate_answer_node(state: SelfReflectionState):\n",
    "    # LLMに回答を生成させるノード\n",
    "    iteration = state.get(\"current_iteration\", 0) + 1\n",
    "    print(f\"generate_answer_node: イテレーション {iteration}\")\n",
    "    \n",
    "    # プロンプトテンプレートの準備\n",
    "    if iteration == 1:\n",
    "        prompt_text = \"日本の首都はどこですか？簡潔に答えてください。\"\n",
    "    else:\n",
    "        previous_answer = state.get(\"current_answer\", \"\")\n",
    "        prompt_text = f\"前回の回答「{previous_answer}」は不十分でした。もっと良い回答を生成してください。特に「完璧」というキーワードを含めるように努力してください。日本の首都はどこですか？\"\n",
    "        \n",
    "    # LLM呼び出し\n",
    "    response = llm.invoke([HumanMessage(content=prompt_text)])\n",
    "    answer_content = response.content\n",
    "    print(f\"generate_answer_node: LLMの回答「{answer_content}」\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=answer_content)], \n",
    "        \"current_iteration\": iteration, \n",
    "        \"current_answer\": answer_content\n",
    "    }\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def should_reflect_or_finish(state: SelfReflectionState):\n",
    "    # LLMの回答を評価し、ループを継続するか終了するかを判断する\n",
    "    current_answer = state.get(\"current_answer\", \"\")\n",
    "    current_iteration = state[\"current_iteration\"]\n",
    "    max_iterations = state[\"max_iterations\"]\n",
    "    target_keyword = state[\"target_keyword\"]\n",
    "    \n",
    "    print(f\"should_reflect_or_finish: 現在の回答「{current_answer}」、イテレーション {current_iteration}/{max_iterations}\")\n",
    "    \n",
    "    if target_keyword in current_answer:\n",
    "        print(f\"should_reflect_or_finish: ターゲットキーワード「{target_keyword}」を発見。ループ終了。\")\n",
    "        return \"finish\" # 終了条件\n",
    "    elif current_iteration >= max_iterations:\n",
    "        print(f\"should_reflect_or_finish: 最大イテレーション {max_iterations} に到達。ループ終了。\")\n",
    "        return \"finish\" # 終了条件 (最大回数超過)\n",
    "    else:\n",
    "        print(\"should_reflect_or_finish: 改善が必要。ループ継続。\")\n",
    "        return \"reflect\" # ループ継続条件\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(SelfReflectionState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"generator\", generate_answer_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"generator\")\n",
    "\n",
    "# 条件付きエッジの追加 (自己反省ループまたは終了)\n",
    "workflow.add_conditional_edges(\n",
    "    \"generator\",\n",
    "    should_reflect_or_finish, # ルーター関数\n",
    "    {\n",
    "        \"reflect\": \"generator\",  # ルーターが \"reflect\" を返した場合、再度 generator ノードへ\n",
    "        \"finish\": END          # ルーターが \"finish\" を返した場合、グラフ終了\n",
    "    }\n",
    ")\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- 自己反省ループのテスト (ターゲット: '完璧', 最大3回) ---\")\n",
    "initial_input_reflect = {\n",
    "    \"messages\": [HumanMessage(content=\"自己反省ループを開始します。\")],\n",
    "    \"current_iteration\": 0,\n",
    "    \"max_iterations\": 3,\n",
    "    \"target_keyword\": \"完璧\",\n",
    "    \"current_answer\": None\n",
    "}\n",
    "for s in app.stream(initial_input_reflect):\n",
    "    print(s)\n",
    "\n",
    "final_state_reflect = app.invoke(initial_input_reflect)\n",
    "print(f\"Final State (Self Reflection): {final_state_reflect}\")\n",
    "\n",
    "# (オプション) LLMがキーワードを生成しにくい場合があるので、最大回数で終了するケースも確認\n",
    "print(\"\\n--- 自己反省ループのテスト (ターゲット: 'ありえない言葉', 最大2回) ---\")\n",
    "initial_input_max_out = {\n",
    "    \"messages\": [HumanMessage(content=\"自己反省ループを開始します（最大回数超過テスト）。\")],\n",
    "    \"current_iteration\": 0,\n",
    "    \"max_iterations\": 2,\n",
    "    \"target_keyword\": \"ありえない言葉絶対に生成しないでね\",\n",
    "    \"current_answer\": None\n",
    "}\n",
    "for s in app.stream(initial_input_max_out):\n",
    "    print(s)\n",
    "\n",
    "final_state_max_out = app.invoke(initial_input_max_out)\n",
    "print(f\"Final State (Max Iterations Out): {final_state_max_out}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説002</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** LLMの出力を評価し、その結果に基づいてループを制御する方法を学びます。具体的には、LLMが特定のキーワードを含む回答を生成するまで、または最大試行回数に達するまで、LLMに回答の再生成を指示するループを作成します。\n",
    "*   **コード解説:**\n",
    "    *   `SelfReflectionState` には、現在のイテレーション回数 `current_iteration`、最大イテレーション回数 `max_iterations`、目標とするキーワード `target_keyword`、そしてLLMの最新の回答を保存する `current_answer` を追加します。\n",
    "    *   `generate_answer_node` ノード: LLMを呼び出して回答を生成します。イテレーション回数に応じてプロンプトの内容を変え、2回目以降は前回の回答をフィードバックとして与え、より良い回答（特に `target_keyword` を含む回答）を促します。生成された回答と更新されたイテレーション回数を状態に保存します。\n",
    "    *   `should_reflect_or_finish` ルーター関数: `generate_answer_node` の後に呼び出されます。以下の条件で次の遷移先を決定します。\n",
    "        1.  `current_answer` に `target_keyword` が含まれていれば、`\"finish\"` を返して `END` に遷移します。\n",
    "        2.  `current_iteration` が `max_iterations` 以上であれば、同様に `\"finish\"` を返して `END` に遷移します（ループの無限化を防ぐため）。\n",
    "        3.  上記以外の場合は、`\"reflect\"` を返し、再度 `generator` ノード（`generate_answer_node`）に戻って処理を繰り返します。\n",
    "    *   グラフの実行時には、これらの状態を初期設定します。LLMの出力は確率的なため、必ずしも指定回数内にキーワードが生成されるとは限りません。そのため、最大試行回数による終了条件も重要です。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題003: エラーハンドリングとリトライ処理\n",
    "\n",
    "外部API呼び出しなど、失敗する可能性のある処理をグラフに組み込む場合、エラーハンドリングとリトライ処理は不可欠です。ここでは、模擬的な「不安定なAPI呼び出し」ノードを作成し、それが失敗した場合に指定回数リトライするループを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003\n",
    "import random\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage, BaseMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ErrorHandlingState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    api_call_attempts: int\n",
    "    max_api_attempts: int\n",
    "    api_data: Optional[str] # APIから取得したデータ\n",
    "    error_message: Optional[str] # エラー発生時のメッセージ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def unstable_api_call_node(state: ErrorHandlingState):\n",
    "    # 模擬的な不安定なAPI呼び出し\n",
    "    attempts = state.get(\"api_call_attempts\", 0) + 1\n",
    "    print(f\"unstable_api_call_node: API呼び出し試行 {attempts}回目\")\n",
    "    \n",
    "    # 70%の確率で成功し、30%の確率で失敗すると仮定\n",
    "    if random.random() < ____: # 成功確率 (例: 0.7)\n",
    "        print(\"unstable_api_call_node: API呼び出し成功！\")\n",
    "        api_result = \"APIから取得した重要なデータです。\"\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=\"API呼び出しに成功しました。\")],\n",
    "            \"api_call_attempts\": ____,\n",
    "            \"api_data\": ____,\n",
    "            \"error_message\": None # 成功時はエラーメッセージをクリア\n",
    "        }\n",
    "    else:\n",
    "        print(\"unstable_api_call_node: API呼び出し失敗...\")\n",
    "        error_msg = f\"API呼び出し失敗 (試行 {attempts}回目)\"\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=error_msg)],\n",
    "            \"api_call_attempts\": ____,\n",
    "            \"api_data\": None, # 失敗時はデータをクリア\n",
    "            \"error_message\": ____\n",
    "        }\n",
    "\n",
    "def error_handler_node(state: ErrorHandlingState):\n",
    "    # API呼び出しが最終的に失敗した場合の処理\n",
    "    error_msg = state.get(\"error_message\", \"不明なエラーが発生しました。\")\n",
    "    print(f\"error_handler_node: {error_msg} 最大リトライ回数に達しました。\")\n",
    "    return {\"messages\": [AIMessage(content=f\"処理失敗: {error_msg}\")]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def should_retry_or_fail(state: ErrorHandlingState):\n",
    "    # API呼び出しの結果と試行回数に基づいてリトライするか、エラー処理に進むかを判断\n",
    "    api_data = state.get(\"api_data\")\n",
    "    attempts = state[\"api_call_attempts\"]\n",
    "    max_attempts = state[\"max_api_attempts\"]\n",
    "    \n",
    "    if state.get(____) is not None: # api_data に何か値があれば成功とみなす\n",
    "        print(\"should_retry_or_fail: API成功。処理終了へ。\")\n",
    "        return \"succeed\" # 成功ルート\n",
    "    elif attempts < ____: # max_attempts\n",
    "        print(f\"should_retry_or_fail: API失敗。リトライします ({attempts}/{max_attempts})。\")\n",
    "        return \"retry\" # リトライルート\n",
    "    else:\n",
    "        print(f\"should_retry_or_fail: API失敗。最大試行回数 ({max_attempts}) に到達。エラー処理へ。\")\n",
    "        return \"fail\" # 失敗ルート (エラーハンドラへ)\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(____)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"api_caller\", ____)\n",
    "workflow.add_node(\"error_handler\", ____)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(____)\n",
    "\n",
    "# 条件付きエッジの追加 (リトライ、成功、または最終失敗)\n",
    "workflow.add_conditional_edges(\n",
    "    \"api_caller\",\n",
    "    should_retry_or_fail, # ルーター関数\n",
    "    {\n",
    "        \"retry\": ____,    # \"retry\" の場合、再度 api_caller ノードへ\n",
    "        \"succeed\": END,   # \"succeed\" の場合、グラフ終了\n",
    "        \"fail\": ____      # \"fail\" の場合、error_handler ノードへ\n",
    "    }\n",
    ")\n",
    "\n",
    "# エラーハンドラノードからの終了\n",
    "workflow.add_edge(____, END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- エラーハンドリングとリトライ処理のテスト (最大3回試行) ---\")\n",
    "initial_input_retry = {\n",
    "    \"messages\": [HumanMessage(content=\"不安定なAPI呼び出しを開始します。\")],\n",
    "    \"api_call_attempts\": 0,\n",
    "    \"max_api_attempts\": 3,\n",
    "    \"api_data\": None,\n",
    "    \"error_message\": None\n",
    "}\n",
    "\n",
    "# 複数回実行して、成功するケースと失敗するケースを確認\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- 実行試行 {i+1} ---\")\n",
    "    # 状態をリセットするために、毎回新しい入力辞書を作成 (ただし、内容は同じで良い)\n",
    "    current_input = initial_input_retry.copy() # shallow copyで十分\n",
    "    # stream実行時の状態変化を追跡するために、api_call_attemptsは都度リセットされる前提\n",
    "    # invokeの場合は入力が毎回同じなら結果も同じになるが、streamなら途中経過が見れる\n",
    "    # ここではinvokeを使って最終結果のみ確認するが、streamで途中経過を見るのも有効\n",
    "    final_state_retry = app.invoke(current_input, {\"recursion_limit\": 10})\n",
    "    print(f\"Final State (Attempt {i+1}): {final_state_retry}\")\n",
    "    if final_state_retry.get(\"api_data\"):\n",
    "        print(f\"実行試行 {i+1}: API呼び出し成功！ データ: {final_state_retry['api_data']}\")\n",
    "    else:\n",
    "        print(f\"実行試行 {i+1}: API呼び出し最終失敗。エラー: {final_state_retry.get('error_message')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答003</summary>\n",
    "\n",
    "``````python\n",
    "# 解答003\n",
    "import random\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage, BaseMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ErrorHandlingState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    api_call_attempts: int\n",
    "    max_api_attempts: int\n",
    "    api_data: Optional[str] # APIから取得したデータ\n",
    "    error_message: Optional[str] # エラー発生時のメッセージ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def unstable_api_call_node(state: ErrorHandlingState):\n",
    "    # 模擬的な不安定なAPI呼び出し\n",
    "    attempts = state.get(\"api_call_attempts\", 0) + 1\n",
    "    print(f\"unstable_api_call_node: API呼び出し試行 {attempts}回目\")\n",
    "    \n",
    "    # 70%の確率で成功し、30%の確率で失敗すると仮定\n",
    "    if random.random() < 0.7:\n",
    "        print(\"unstable_api_call_node: API呼び出し成功！\")\n",
    "        api_result = \"APIから取得した重要なデータです。\"\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=\"API呼び出しに成功しました。\")],\n",
    "            \"api_call_attempts\": attempts,\n",
    "            \"api_data\": api_result,\n",
    "            \"error_message\": None # 成功時はエラーメッセージをクリア\n",
    "        }\n",
    "    else:\n",
    "        print(\"unstable_api_call_node: API呼び出し失敗...\")\n",
    "        error_msg = f\"API呼び出し失敗 (試行 {attempts}回目)\"\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=error_msg)],\n",
    "            \"api_call_attempts\": attempts,\n",
    "            \"api_data\": None, # 失敗時はデータをクリア\n",
    "            \"error_message\": error_msg\n",
    "        }\n",
    "\n",
    "def error_handler_node(state: ErrorHandlingState):\n",
    "    # API呼び出しが最終的に失敗した場合の処理\n",
    "    error_msg = state.get(\"error_message\", \"不明なエラーが発生しました。\")\n",
    "    print(f\"error_handler_node: {error_msg} 最大リトライ回数に達しました。\")\n",
    "    return {\"messages\": [AIMessage(content=f\"処理失敗: {error_msg}\")]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def should_retry_or_fail(state: ErrorHandlingState):\n",
    "    # API呼び出しの結果と試行回数に基づいてリトライするか、エラー処理に進むかを判断\n",
    "    api_data = state.get(\"api_data\")\n",
    "    attempts = state[\"api_call_attempts\"]\n",
    "    max_attempts = state[\"max_api_attempts\"]\n",
    "    \n",
    "    if api_data is not None: # api_data に何か値があれば成功とみなす\n",
    "        print(\"should_retry_or_fail: API成功。処理終了へ。\")\n",
    "        return \"succeed\" # 成功ルート\n",
    "    elif attempts < max_attempts:\n",
    "        print(f\"should_retry_or_fail: API失敗。リトライします ({attempts}/{max_attempts})。\")\n",
    "        return \"retry\" # リトライルート\n",
    "    else:\n",
    "        print(f\"should_retry_or_fail: API失敗。最大試行回数 ({max_attempts}) に到達。エラー処理へ。\")\n",
    "        return \"fail\" # 失敗ルート (エラーハンドラへ)\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(ErrorHandlingState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"api_caller\", unstable_api_call_node)\n",
    "workflow.add_node(\"error_handler\", error_handler_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"api_caller\")\n",
    "\n",
    "# 条件付きエッジの追加 (リトライ、成功、または最終失敗)\n",
    "workflow.add_conditional_edges(\n",
    "    \"api_caller\",\n",
    "    should_retry_or_fail, # ルーター関数\n",
    "    {\n",
    "        \"retry\": \"api_caller\",    # \"retry\" の場合、再度 api_caller ノードへ\n",
    "        \"succeed\": END,          # \"succeed\" の場合、グラフ終了\n",
    "        \"fail\": \"error_handler\" # \"fail\" の場合、error_handler ノードへ\n",
    "    }\n",
    ")\n",
    "\n",
    "# エラーハンドラノードからの終了\n",
    "workflow.add_edge(\"error_handler\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- エラーハンドリングとリトライ処理のテスト (最大3回試行) ---\")\n",
    "initial_input_retry = {\n",
    "    \"messages\": [HumanMessage(content=\"不安定なAPI呼び出しを開始します。\")],\n",
    "    \"api_call_attempts\": 0,\n",
    "    \"max_api_attempts\": 3,\n",
    "    \"api_data\": None,\n",
    "    \"error_message\": None\n",
    "}\n",
    "\n",
    "# 複数回実行して、成功するケースと失敗するケースを確認\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- 実行試行 {i+1} ---\")\n",
    "    # 状態をリセットするために、毎回新しい入力辞書を作成 (ただし、内容は同じで良い)\n",
    "    # LangGraphのinvoke/streamは入力状態を変更しないため、ループ内で同じinputを使い回しても問題ない。\n",
    "    # ただし、api_call_attemptsはグラフ内で更新されるため、グラフの実行ごとに初期化された状態で開始される。\n",
    "    final_state_retry = app.invoke(initial_input_retry.copy(), {\"recursion_limit\": 10})\n",
    "    print(f\"Final State (Attempt {i+1}): {final_state_retry}\")\n",
    "    if final_state_retry.get(\"api_data\"):\n",
    "        print(f\"実行試行 {i+1}: API呼び出し成功！ データ: {final_state_retry['api_data']}\")\n",
    "    else:\n",
    "        print(f\"実行試行 {i+1}: API呼び出し最終失敗。エラー: {final_state_retry.get('error_message')}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説003</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** 失敗する可能性のある処理（ここでは模擬API呼び出し）に対して、リトライ処理を組み込む方法を学びます。状態に試行回数と最大試行回数を保持し、ルーター関数でリトライ、成功、最終失敗の3つのパスに分岐させます。\n",
    "*   **コード解説:**\n",
    "    *   `ErrorHandlingState` には、API呼び出しの試行回数 `api_call_attempts`、最大試行回数 `max_api_attempts`、APIから取得したデータ `api_data`、エラーメッセージ `error_message` を定義します。\n",
    "    *   `unstable_api_call_node` ノード: `random.random()` を使って、確率的に成功または失敗するAPI呼び出しをシミュレートします。成功時は `api_data` に結果を格納し、失敗時は `error_message` を設定します。どちらの場合も `api_call_attempts` を更新します。\n",
    "    *   `error_handler_node` ノード: リトライ上限に達してもAPI呼び出しが成功しなかった場合に呼び出され、最終的なエラー処理（ここではエラーメッセージの表示）を行います。\n",
    "    *   `should_retry_or_fail` ルーター関数: `unstable_api_call_node` の後に呼び出されます。\n",
    "        1.  `api_data` が存在すれば（API呼び出し成功）、`\"succeed\"` を返して `END` に遷移します。\n",
    "        2.  `api_data` が存在せず、かつ `api_call_attempts` が `max_api_attempts` 未満であれば、`\"retry\"` を返し、再度 `api_caller` ノード（`unstable_api_call_node`）に戻ってリトライします。\n",
    "        3.  上記以外（`api_data` が存在せず、試行回数が上限に達した）の場合は、`\"fail\"` を返し、`error_handler` ノードに遷移します。\n",
    "    *   グラフの実行: `random` を使用しているため、実行ごとに結果が変わる可能性があります。複数回実行することで、成功するケース、数回のリトライ後に成功するケース、最終的に失敗するケースを確認できます。`recursion_limit` は、グラフの最大遷移回数を設定するもので、意図しない無限ループを防ぐために重要です。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題004: Human-in-the-Loop (人間による介在)\n",
    "\n",
    "LangGraphでは、`Interrupt` を使用してグラフの実行を一時停止し、人間の確認や入力を待つことができます。この問題では、LLMがメールの下書きを作成した後、ユーザーが内容を確認・承認するまで処理を中断し、承認後に次のステップ（ここでは「送信」のシミュレーション）に進むグラフを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004\n",
    "from typing import TypedDict, Annotated, Optional, List\n",
    "from langgraph.graph import StateGraph, END, Interrupt\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "import uuid\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class HumanApprovalState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    draft_email: Optional[str] # LLMが生成したメール下書き\n",
    "    user_feedback: Optional[str] # ユーザーからのフィードバックや承認 (\"approve\" または修正指示)\n",
    "    is_approved: Optional[bool] # 承認されたかどうか\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def draft_email_node(state: HumanApprovalState):\n",
    "    # LLMにメールの下書きを生成させるノード\n",
    "    print(\"draft_email_node: メール下書きを生成中...\")\n",
    "    # ユーザーの最初の要望、または修正指示に基づいて下書きを作成\n",
    "    if state.get(\"user_feedback\") and not state.get(\"is_approved\"):\n",
    "        # 修正指示がある場合\n",
    "        instruction = f\"前回のメール下書き「{state['draft_email']}」に対して、以下の修正指示があります。\\n\\n修正指示: {state['user_feedback']}\\n\\nこの指示に基づいてメール下書きを修正してください。\"\n",
    "    else:\n",
    "        # 初回の下書き作成\n",
    "        instruction = state[\"messages\"][-1].content # 最初のユーザーメッセージ\n",
    "    \n",
    "    prompt = f\"以下の指示に基づいて、丁寧なビジネスメールの下書きを作成してください。\\n\\n指示: {instruction}\\n\\n下書き:\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    email_draft = response.content\n",
    "    print(f\"draft_email_node: 生成された下書き「{email_draft}」\")\n",
    "    \n",
    "    # ユーザーフィードバックと承認状態をリセット\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"下書きを作成しました。ご確認ください。\\n---\\n{email_draft}\\n---\")], \n",
    "        \"draft_email\": ____, \n",
    "        \"user_feedback\": None, \n",
    "        \"is_approved\": False # 確認待ちなのでFalseに\n",
    "    }\n",
    "\n",
    "def send_email_node(state: HumanApprovalState):\n",
    "    # メール送信をシミュレートするノード\n",
    "    final_email = state.get(\"draft_email\", \"\")\n",
    "    print(f\"send_email_node: メールを送信します...\\n内容:\\n{final_email}\")\n",
    "    return {\"messages\": [AIMessage(content=f\"メールを送信しました。\\n内容：{final_email}\")]}\n",
    "\n",
    "def request_human_approval_node(state: HumanApprovalState):\n",
    "    # ユーザーに承認を求めるノード（Interruptの前に配置）\n",
    "    print(\"request_human_approval_node: ユーザーの承認待ちです。Interruptが発生します。承認する場合は 'approve' と入力してください。修正する場合は修正内容を記述してください。\")\n",
    "    # このノードは状態を直接変更せず、Interruptが続くことを示す\n",
    "    return {}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def route_after_approval_request(state: HumanApprovalState):\n",
    "    # ユーザーのフィードバックに基づいて次のアクションを決定\n",
    "    # このルーターはInterruptの後に呼び出される想定\n",
    "    user_input = state.get(\"user_feedback\", \"\").strip().lower()\n",
    "    print(f\"route_after_approval_request: ユーザー入力「{user_input}」\")\n",
    "    \n",
    "    if user_input == \"approve\" and state.get(\"is_approved\"):\n",
    "        print(\"route_after_approval_request: 承認されました。メール送信へ。\")\n",
    "        return \"send\" # 承認ルート\n",
    "    elif user_input and user_input != \"approve\": # 何か入力があり、それが 'approve' でなければ修正とみなす\n",
    "        print(\"route_after_approval_request: 修正指示がありました。下書き修正へ。\")\n",
    "        return \"revise\" # 修正ルート (再度draft_email_nodeへ)\n",
    "    else: # 入力が空など、予期せぬ場合は再度確認を求める\n",
    "        print(\"route_after_approval_request: 不明な入力または未承認。再度確認を求めます。\")\n",
    "        return \"re_request\" # 再度確認を促すルート\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(____)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"draft_email\", ____)\n",
    "workflow.add_node(\"request_approval\", ____)\n",
    "workflow.add_node(\"send_email\", ____)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(____)\n",
    "\n",
    "# 通常のエッジ\n",
    "workflow.add_edge(____, \"request_approval\") # 下書き作成後、承認依頼へ\n",
    "\n",
    "# 承認依頼ノードの後にInterruptを挟む\n",
    "# `request_approval` ノードが完了すると、Interruptが発生し、人間からの入力を待つ\n",
    "# 人間が `app.update_state(config, {\"user_feedback\": \"approve\"})` のように状態を更新して再開すると、\n",
    "# `route_after_approval_request` が呼び出される\n",
    "workflow.add_conditional_edges(\n",
    "    \"request_approval\",\n",
    "    route_after_approval_request,\n",
    "    {\n",
    "        \"send\": ____,\n",
    "        \"revise\": ____,\n",
    "        \"re_request\": ____ # 不明な入力の場合、再度承認依頼へ\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"send_email\", END) # 送信後終了\n",
    "\n",
    "# グラフのコンパイル (Interruptを指定)\n",
    "# Interruptは、指定されたノードの実行 *後* に発生します。\n",
    "# ここでは 'request_approval' ノードの後に中断を挿入します。\n",
    "app = workflow.compile(interrupt_after=[____])\n",
    "\n",
    "# --- グラフの可視化 (オプション) ---\n",
    "try:\n",
    "    img_bytes = app.get_graph().draw_png()\n",
    "    display(Image(img_bytes))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- グラフの実行と人間による介在のシミュレーション ---\n",
    "print(\"\\n--- Human-in-the-Loop テスト開始 ---\")\n",
    "initial_email_request = \"来週の月曜日に予定しているプロジェクトAに関する進捗会議のリマインダーメールを作成してください。参加者はBさんとCさんです。\"\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "# 1. 最初の実行 (下書き作成 -> 承認依頼 -> Interrupt)\n",
    "print(\"\\n--- ステップ1: 初期リクエストと下書き生成 ---\")\n",
    "events = []\n",
    "for event in app.stream(\n",
    "    {\"messages\": [HumanMessage(content=initial_email_request)]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    events.append(event)\n",
    "    print(f\"イベント: {event}\")\n",
    "    # messagesの最後がAIMessageで、かつInterruptの前なら表示\n",
    "    if isinstance(event.get(\"messages\", [])[-1], AIMessage) and \"request_approval\" in event:\n",
    "        print(f\"AIからのメッセージ: {event['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n--- ステップ1完了: グラフは中断状態のはずです。 ---\")\n",
    "assert events[-1][\"request_approval\"] is not None # request_approvalノードで止まっていることを確認\n",
    "current_draft = events[-1][\"request_approval\"][\"draft_email\"]\n",
    "print(f\"現在のメール下書き:\\n{current_draft}\")\n",
    "\n",
    "# 2. 人間が内容を確認し、修正を指示して再開\n",
    "print(\"\\n--- ステップ2: ユーザーが修正を指示して再開 ---\")\n",
    "user_correction = \"会議の場所として第3会議室を追記してください。\"\n",
    "events_after_correction = []\n",
    "for event in app.stream(\n",
    "    None, # 入力はNoneで、現在の状態から再開\n",
    "    # 正しくは app.update_state で状態を更新してから stream(None, config) を呼び出す\n",
    "    # config={\"configurable\": {\"thread_id\": config[\"configurable\"][\"thread_id\"], \"user_feedback\": user_correction, \"is_approved\": False}},\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    events_after_correction.append(event)\n",
    "    print(f\"イベント: {event}\")\n",
    "    if isinstance(event.get(\"messages\", [])[-1], AIMessage) and \"request_approval\" in event:\n",
    "        print(f\"AIからのメッセージ (修正後): {event['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n--- ステップ2完了: グラフは再度中断状態のはずです。 ---\")\n",
    "assert events_after_correction[-1][\"request_approval\"] is not None\n",
    "corrected_draft = events_after_correction[-1][\"request_approval\"][\"draft_email\"]\n",
    "print(f\"修正後のメール下書き:\\n{corrected_draft}\")\n",
    "\n",
    "# 3. 人間が内容を承認して再開\n",
    "print(\"\\n--- ステップ3: ユーザーが承認して再開 ---\")\n",
    "events_after_approval = []\n",
    "for event in app.stream(\n",
    "    None, # 入力はNoneで、現在の状態から再開\n",
    "    # 正しくは app.update_state で状態を更新してから stream(None, config) を呼び出す\n",
    "    # config={\"configurable\": {\"thread_id\": config[\"configurable\"][\"thread_id\"], \"user_feedback\": \"approve\", \"is_approved\": True}},\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    events_after_approval.append(event)\n",
    "    print(f\"イベント: {event}\")\n",
    "    if \"send_email\" in event:\n",
    "        print(f\"AIからのメッセージ (送信後): {event['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n--- ステップ3完了: グラフは終了しているはずです。 ---\")\n",
    "assert \"__end__\" in events_after_approval[-1] # 最終的に終了したことを確認\n",
    "final_sent_email_details = events_after_approval[-1][\"__end__\"]\n",
    "print(f\"最終送信メール情報: {final_sent_email_details}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答004</summary>\n",
    "\n",
    "``````python\n",
    "# 解答004\n",
    "from typing import TypedDict, Annotated, Optional, List\n",
    "from langgraph.graph import StateGraph, END, Interrupt\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "import uuid\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class HumanApprovalState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    draft_email: Optional[str] # LLMが生成したメール下書き\n",
    "    user_feedback: Optional[str] # ユーザーからのフィードバックや承認 (\"approve\" または修正指示)\n",
    "    is_approved: Optional[bool] # 承認されたかどうか\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def draft_email_node(state: HumanApprovalState):\n",
    "    # LLMにメールの下書きを生成させるノード\n",
    "    print(\"draft_email_node: メール下書きを生成中...\")\n",
    "    # ユーザーの最初の要望、または修正指示に基づいて下書きを作成\n",
    "    if state.get(\"user_feedback\") and not state.get(\"is_approved\"):\n",
    "        # 修正指示がある場合 (user_feedbackに修正内容が入っている想定)\n",
    "        instruction = f\"前回のメール下書き「{state['draft_email']}」に対して、以下の修正指示があります。\\n\\n修正指示: {state['user_feedback']}\\n\\nこの指示に基づいてメール下書きを修正してください。\"\n",
    "    else:\n",
    "        # 初回の下書き作成 (messagesの最後のHumanMessageから取得)\n",
    "        user_request_message = next((msg for msg in reversed(state[\"messages\"]) if isinstance(msg, HumanMessage)), None)\n",
    "        instruction = user_request_message.content if user_request_message else \"一般的なビジネスメールを作成してください。\"\n",
    "    \n",
    "    prompt = f\"以下の指示に基づいて、丁寧なビジネスメールの下書きを作成してください。\\n\\n指示: {instruction}\\n\\n下書き:\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    email_draft = response.content\n",
    "    print(f\"draft_email_node: 生成された下書き「{email_draft}」\")\n",
    "    \n",
    "    # ユーザーフィードバックと承認状態をリセット（あるいは明確に設定）\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"下書きを作成しました。ご確認ください。\\n---\\n{email_draft}\\n---\")], \n",
    "        \"draft_email\": email_draft, \n",
    "        \"user_feedback\": None, # フィードバックは消費されたのでクリア\n",
    "        \"is_approved\": False # 新しい下書きなので未承認状態に\n",
    "    }\n",
    "\n",
    "def send_email_node(state: HumanApprovalState):\n",
    "    # メール送信をシミュレートするノード\n",
    "    final_email = state.get(\"draft_email\", \"\")\n",
    "    print(f\"send_email_node: メールを送信します...\\n内容:\\n{final_email}\")\n",
    "    return {\"messages\": [AIMessage(content=f\"メールを送信しました。\\n内容：{final_email}\")]}\n",
    "\n",
    "def request_human_approval_node(state: HumanApprovalState):\n",
    "    # ユーザーに承認を求めるノード（Interruptの前に配置）\n",
    "    print(\"request_human_approval_node: ユーザーの承認待ちです。Interruptが発生します。承認する場合は 'approve' と入力してください。修正する場合は修正内容を記述してください。\")\n",
    "    # このノードは状態を直接変更せず、Interruptが続くことを示す\n",
    "    return {}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def route_after_approval_request(state: HumanApprovalState):\n",
    "    # ユーザーのフィードバックに基づいて次のアクションを決定\n",
    "    # このルーターはInterruptの後に、状態が更新された後に呼び出される\n",
    "    user_input = state.get(\"user_feedback\", \"\").strip().lower()\n",
    "    is_approved_flag = state.get(\"is_approved\", False)\n",
    "    print(f\"route_after_approval_request: ユーザー入力「{user_input}」, 承認状態: {is_approved_flag}\")\n",
    "    \n",
    "    if is_approved_flag and user_input == \"approve\":\n",
    "        print(\"route_after_approval_request: 承認されました。メール送信へ。\")\n",
    "        return \"send\" # 承認ルート\n",
    "    elif user_input and user_input != \"approve\": # 何か入力があり、それが 'approve' でなければ修正とみなす\n",
    "        print(\"route_after_approval_request: 修正指示がありました。下書き修正へ。\")\n",
    "        return \"revise\" # 修正ルート (再度draft_email_nodeへ)\n",
    "    else: # 入力が空、または'approve'だがis_approvedフラグがFalseなど、予期せぬ場合は再度確認を求める\n",
    "        print(\"route_after_approval_request: 不明な入力または状態の不整合。再度確認を求めます。\")\n",
    "        # 状態をリセットして再要求するのが安全かもしれない\n",
    "        # state[\"user_feedback\"] = None \n",
    "        # state[\"is_approved\"] = False\n",
    "        return \"re_request\" # 再度確認を促すルート\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(HumanApprovalState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"draft_email\", draft_email_node)\n",
    "workflow.add_node(\"request_approval\", request_human_approval_node) \n",
    "workflow.add_node(\"send_email\", send_email_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"draft_email\")\n",
    "\n",
    "# 通常のエッジ\n",
    "workflow.add_edge(\"draft_email\", \"request_approval\") # 下書き作成後、承認依頼へ\n",
    "\n",
    "# 承認依頼ノードの後にInterruptを挟む\n",
    "# `request_approval` ノードが完了すると、Interruptが発生し、人間からの入力を待つ\n",
    "# 人間が `app.update_state(config, {\"user_feedback\": \"approve\", \"is_approved\": True})` のように状態を更新して再開すると、\n",
    "# `route_after_approval_request` が呼び出される\n",
    "workflow.add_conditional_edges(\n",
    "    \"request_approval\", # このノードの後にInterruptが入り、再開時にこのルーターが呼ばれる\n",
    "    route_after_approval_request,\n",
    "    {\n",
    "        \"send\": \"send_email\",\n",
    "        \"revise\": \"draft_email\",\n",
    "        \"re_request\": \"request_approval\" # 不明な入力の場合、再度承認依頼へ\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"send_email\", END) # 送信後終了\n",
    "\n",
    "# グラフのコンパイル (Interruptを指定)\n",
    "# Interruptは、指定されたノードの実行 *後* に発生します。\n",
    "# ここでは 'request_approval' ノードの後に中断を挿入します。\n",
    "app = workflow.compile(interrupt_after=[\"request_approval\"])\n",
    "\n",
    "# --- グラフの可視化 (オプション) ---\n",
    "try:\n",
    "    img_bytes = app.get_graph().draw_png()\n",
    "    display(Image(img_bytes))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- グラフの実行と人間による介在のシミュレーション ---\n",
    "print(\"\\n--- Human-in-the-Loop テスト開始 ---\")\n",
    "initial_email_request = \"来週の月曜日に予定しているプロジェクトAに関する進捗会議のリマインダーメールを作成してください。参加者はBさんとCさんです。\"\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# 1. 最初の実行 (下書き作成 -> 承認依頼 -> Interrupt)\n",
    "print(\"\\n--- ステップ1: 初期リクエストと下書き生成 ---\")\n",
    "events = []\n",
    "for event in app.stream(\n",
    "    {\"messages\": [HumanMessage(content=initial_email_request)]},\n",
    "    config,\n",
    "    stream_mode=\"values\" # valuesモードで各ノードの出力状態を取得\n",
    "):\n",
    "    events.append(event)\n",
    "    # print(f\"イベント: {list(event.keys())}\") # デバッグ用にキーを表示\n",
    "    # messagesの最後がAIMessageで、かつInterruptの前なら表示\n",
    "    last_message = event.get(\"messages\", [])[-1] if event.get(\"messages\") else None\n",
    "    if isinstance(last_message, AIMessage) and \"request_approval\" in event:\n",
    "        print(f\"AIからのメッセージ: {last_message.content}\")\n",
    "\n",
    "print(\"\\n--- ステップ1完了: グラフは中断状態のはずです。 ---\")\n",
    "current_state_after_step1 = app.get_state(config) # 中断時の状態を取得\n",
    "assert \"request_approval\" in current_state_after_step1.next # 次に実行されるべきノードが request_approval の後の中断点であることを確認\n",
    "current_draft = current_state_after_step1.values[\"draft_email\"]\n",
    "print(f\"現在のメール下書き:\\n{current_draft}\")\n",
    "\n",
    "# 2. 人間が内容を確認し、修正を指示して再開\n",
    "print(\"\\n--- ステップ2: ユーザーが修正を指示して再開 ---\")\n",
    "user_correction = \"会議の場所として第3会議室を追記してください。\"\n",
    "events_after_correction = []\n",
    "# update_stateではなく、stream/invokeの入力として渡すことで状態を更新\n",
    "for event in app.stream(\n",
    "    None, # 入力はNoneで、現在の状態から再開\n",
    "    # configに直接更新したい状態を含めるのではなく、\n",
    "    # app.update_state を使うか、あるいは次のノードの入力として渡すのが一般的。\n",
    "    # ここではデモのため、streamの入力として直接更新値を渡すのではなく、\n",
    "    # HumanApprovalStateのuser_feedbackとis_approvedを更新した入力で再開するイメージで記述します。\n",
    "    # LangGraphの stream/invoke は通常、最初の入力のみを受け付け、\n",
    "    # 2回目以降の呼び出しで `None` 以外の入力を渡すと新しい実行として扱われることがあるため注意。\n",
    "    # 正しくは、app.update_state を使用するか、グラフが次の入力を期待する設計にする。\n",
    "    # ここでは、Interrupt後の再開は app.update_state で状態を更新してから app.stream(None, config) が推奨される\n",
    "    input_after_correction={\"user_feedback\": user_correction, \"is_approved\": False}, # この形式は正しくない。update_stateを使うべき\n",
    "    config=config, # thread_idは同じものを使用\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    # 正しい再開方法:\n",
    "    # app.update_state(config, {\"user_feedback\": user_correction, \"is_approved\": False})\n",
    "    # for event in app.stream(None, config, stream_mode=\"values\"):\n",
    "    events_after_correction.append(event)\n",
    "    # print(f\"イベント: {list(event.keys())}\")\n",
    "    last_message = event.get(\"messages\", [])[-1] if event.get(\"messages\") else None\n",
    "    if isinstance(last_message, AIMessage) and \"request_approval\" in event:\n",
    "        print(f\"AIからのメッセージ (修正後): {last_message.content}\")\n",
    "\n",
    "print(\"\\n--- ステップ2完了: グラフは再度中断状態のはずです。 (正しい実装なら) ---\")\n",
    "# 上記のstream呼び出しは正しくないため、アサーションはコメントアウト\n",
    "# current_state_after_step2 = app.get_state(config)\n",
    "# assert \"request_approval\" in current_state_after_step2.next\n",
    "# corrected_draft = current_state_after_step2.values[\"draft_email\"]\n",
    "# print(f\"修正後のメール下書き:\\n{corrected_draft}\")\n",
    "\n",
    "# 正しい方法でステップ2を再実行\n",
    "print(\"\\n--- ステップ2 (正しい方法): ユーザーが修正を指示して再開 ---\")\n",
    "app.update_state(config, {\"user_feedback\": user_correction, \"is_approved\": False})\n",
    "events_after_correction_correct = []\n",
    "for event in app.stream(None, config, stream_mode=\"values\"):\n",
    "    events_after_correction_correct.append(event)\n",
    "    # print(f\"イベント: {list(event.keys())}\")\n",
    "    last_message = event.get(\"messages\", [])[-1] if event.get(\"messages\") else None\n",
    "    if isinstance(last_message, AIMessage) and \"request_approval\" in event:\n",
    "         print(f\"AIからのメッセージ (修正後): {last_message.content}\")\n",
    "\n",
    "current_state_after_step2_correct = app.get_state(config)\n",
    "assert \"request_approval\" in current_state_after_step2_correct.next\n",
    "corrected_draft_correct = current_state_after_step2_correct.values[\"draft_email\"]\n",
    "print(f\"修正後のメール下書き:\\n{corrected_draft_correct}\")\n",
    "\n",
    "# 3. 人間が内容を承認して再開\n",
    "print(\"\\n--- ステップ3: ユーザーが承認して再開 ---\")\n",
    "app.update_state(config, {\"user_feedback\": \"approve\", \"is_approved\": True})\n",
    "events_after_approval = []\n",
    "for event in app.stream(None, config, stream_mode=\"values\"):\n",
    "    events_after_approval.append(event)\n",
    "    # print(f\"イベント: {list(event.keys())}\")\n",
    "    if \"send_email\" in event and event.get(\"messages\"):\n",
    "        print(f\"AIからのメッセージ (送信後): {event['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n--- ステップ3完了: グラフは終了しているはずです。 ---\")\n",
    "final_state_after_step3 = app.get_state(config)\n",
    "assert final_state_after_step3.next == () # nextが空タプルなら終了\n",
    "final_sent_email_details = final_state_after_step3.values\n",
    "print(f\"最終送信メール情報: {final_sent_email_details}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題005: 複数の制御フローの組み合わせ (反復的な改善と最終承認)\n",
    "\n",
    "これまでに学んだ複数の制御フロー技術（LLMによる判断ループ、人間による介在）を組み合わせて、より実践的なシナリオを構築します。具体的には、LLMが提案を生成し、ユーザーがその提案を評価します。ユーザーが「完璧」と評価するまで、または最大試行回数に達するまでLLMによる改善ループが実行されます。ループが終了した後、最終的な提案について再度ユーザーに承認を求め、承認されれば処理を完了します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "from langgraph.graph import StateGraph, END, Interrupt\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "import uuid\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class IterativeApprovalState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    original_request: str # ユーザーの最初の要求\n",
    "    current_proposal: Optional[str] # LLMによる現在の提案\n",
    "    user_critique: Optional[str] # ユーザーからの批判や改善点\n",
    "    iteration_count: int # 改善ループの現在のイテレーション回数\n",
    "    max_iterations: int # 改善ループの最大イテレーション回数\n",
    "    is_proposal_accepted_by_llm: bool # LLMが提案は十分と判断したか (改善ループ終了条件)\n",
    "    is_final_proposal_approved_by_human: Optional[bool] # 人間による最終承認\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def generate_proposal_node(state: IterativeApprovalState):\n",
    "    # LLMが提案を生成または改善するノード\n",
    "    iteration = state.get(\"iteration_count\", 0) + 1\n",
    "    print(f\"generate_proposal_node: イテレーション {iteration}\")\n",
    "\n",
    "    if iteration == 1:\n",
    "        prompt_text = f\"以下の要望に基づいて、詳細な提案書を作成してください。\\n\\n要望: {state['original_request']}\"\n",
    "    else:\n",
    "        prompt_text = f\"前回の提案「{state['current_proposal']}」に対して、ユーザーから以下の改善点が指摘されました。「{state['user_critique']}」。これらの点を踏まえて提案を大幅に改善してください。もし改善点が特にない、または提案が完璧だと思ったら、提案の最後に「提案は完璧です。」と記述してください。\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt_text)])\n",
    "    proposal = response.content\n",
    "    print(f\"generate_proposal_node: 生成された提案「{proposal}」\")\n",
    "    \n",
    "    accepted_by_llm = \"提案は完璧です。\" in proposal\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=proposal)],\n",
    "        \"current_proposal\": ____,\n",
    "        \"iteration_count\": ____,\n",
    "        \"is_proposal_accepted_by_llm\": ____,\n",
    "        \"user_critique\": None # 消費したのでクリア\n",
    "    }\n",
    "\n",
    "def request_human_critique_node(state: IterativeApprovalState):\n",
    "    # ユーザーに提案の評価（批判や改善点）を求めるノード (改善ループ用Interrupt)\n",
    "    print(\"request_human_critique_node: 生成された提案について評価を入力してください。改善ループを終了し最終承認に進む場合は '完璧' と入力してください。\")\n",
    "    # Interruptはこのノードの後に発生\n",
    "    return {}\n",
    "\n",
    "def request_final_approval_node(state: IterativeApprovalState):\n",
    "    # 最終提案についてユーザーに承認を求めるノード (最終承認用Interrupt)\n",
    "    proposal = state.get(\"current_proposal\", \"\")\n",
    "    print(f\"request_final_approval_node: 最終提案が完成しました。ご確認ください。承認する場合は 'approve' と入力してください。\\n---\\n{proposal}\\n---\")\n",
    "    # Interruptはこのノードの後に発生\n",
    "    return {}\n",
    "\n",
    "def process_complete_node(state: IterativeApprovalState):\n",
    "    # 全てのプロセスが完了したことを示すノード\n",
    "    print(\"process_complete_node: 提案は最終承認され、プロセスは完了しました。\")\n",
    "    return {\"messages\": [AIMessage(content=\"提案は最終承認され、プロセスは完了しました。\")]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def route_after_critique(state: IterativeApprovalState):\n",
    "    # ユーザーの評価とLLMの判断に基づいて改善ループを継続するか、最終承認に進むかを判断\n",
    "    user_critique = state.get(\"user_critique\", \"\").strip().lower()\n",
    "    iteration = state[\"iteration_count\"]\n",
    "    max_iter = state[\"max_iterations\"]\n",
    "    llm_thinks_perfect = state.get(\"is_proposal_accepted_by_llm\", False)\n",
    "\n",
    "    print(f\"route_after_critique: ユーザー評価「{user_critique}」, LLM判断「{llm_thinks_perfect}」, イテレーション {iteration}/{max_iter}\")\n",
    "\n",
    "    if user_critique == \"完璧\" or ____ or ____ >= ____:\n",
    "        print(\"route_after_critique: 改善ループ終了。最終承認へ。\")\n",
    "        return \"proceed_to_final_approval\" # 最終承認へ\n",
    "    else:\n",
    "        print(\"route_after_critique: 改善が必要。改善ループ継続。\")\n",
    "        return \"continue_improvement\" # 改善ループ継続 (generate_proposal_nodeへ)\n",
    "\n",
    "def route_after_final_approval(state: IterativeApprovalState):\n",
    "    # ユーザーの最終承認に基づいてプロセスを完了するか、再度検討を促す（ここでは簡略化）\n",
    "    # このルーターはfinal_approval_request_nodeの後のInterruptの後に呼び出される\n",
    "    final_approval_feedback = state.get(\"user_critique\", \"\").strip().lower() # user_critiqueを最終承認のフィードバックにも使う\n",
    "    print(f\"route_after_final_approval: 最終承認フィードバック「{final_approval_feedback}」\")\n",
    "\n",
    "    if final_approval_feedback == \"approve\" and state.get(\"is_final_proposal_approved_by_human\"):\n",
    "        print(\"route_after_final_approval: 最終承認。プロセス完了へ。\")\n",
    "        return \"complete_process\" # プロセス完了へ\n",
    "    else:\n",
    "        # ここでは簡略化のため、非承認の場合も終了するが、実際は最初のプロセスに戻るなどの処理も考えられる\n",
    "        print(\"route_after_final_approval: 最終的に非承認。プロセス終了（要改善）。\")\n",
    "        return \"end_process_unapproved\" # プロセス終了 (ENDへ)\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(____)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"generate_proposal\", ____)\n",
    "workflow.add_node(\"request_critique\", ____)\n",
    "workflow.add_node(\"request_final_approval\", ____)\n",
    "workflow.add_node(\"process_complete\", ____)\n",
    "\n",
    "# エントリポイント\n",
    "workflow.set_entry_point(____)\n",
    "\n",
    "# 改善ループのフロー\n",
    "workflow.add_edge(____, \"request_critique\") # 提案生成後、人間による評価依頼へ\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"request_critique\", # このノードの後にInterruptが入り、再開時にこのルーターが呼ばれる\n",
    "    ____, # route_after_critique\n",
    "    {\n",
    "        \"continue_improvement\": ____, # \"generate_proposal\"\n",
    "        \"proceed_to_final_approval\": ____ # \"request_final_approval\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 最終承認フロー\n",
    "workflow.add_conditional_edges(\n",
    "    \"request_final_approval\", # このノードの後にInterruptが入り、再開時にこのルーターが呼ばれる\n",
    "    ____, # route_after_final_approval\n",
    "    {\n",
    "        \"complete_process\": ____, # \"process_complete\"\n",
    "        \"end_process_unapproved\": ____ # END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(____, END) # \"process_complete\"\n",
    "\n",
    "# グラフのコンパイル (複数のInterruptポイントを指定)\n",
    "app = workflow.compile(interrupt_after=[____, ____]) # \"request_critique\", \"request_final_approval\"\n",
    "\n",
    "# --- グラフの可視化 (オプション) ---\n",
    "try:\n",
    "    img_bytes = app.get_graph().draw_png()\n",
    "    display(Image(img_bytes))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}\")\n",
    "\n",
    "# --- グラフ実行シミュレーション ---\n",
    "print(\"\\n--- 複合制御フローテスト開始 ---\")\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "initial_user_request = \"新しいエコフレンドリーなコーヒーカップのデザイン案を作成してください。ターゲットは若年層で、持ち運びやすさが重要です。\"\n",
    "\n",
    "# ステップ1: 最初の提案生成 -> 人間による評価依頼 (Interrupt)\n",
    "print(\"\\n--- 改善ループ ステップ1: 最初の提案と評価依頼 ---\")\n",
    "current_state = app.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=initial_user_request)],\n",
    "        \"original_request\": initial_user_request,\n",
    "        \"iteration_count\": 0,\n",
    "        \"max_iterations\": 3,\n",
    "        \"is_proposal_accepted_by_llm\": False,\n",
    "    },\n",
    "    config\n",
    ")\n",
    "print(f\"AIからの提案(1回目):\\n{current_state['current_proposal']}\")\n",
    "assert current_state[\"messages\"][-1].type == \"ai\" # AIからのメッセージがあるはず\n",
    "assert app.get_state(config).next == (\"request_critique\",) # request_critiqueの後で止まっているはず\n",
    "\n",
    "# ステップ2: 人間が改善点を入力 -> LLMが改善案を生成 -> 再度評価依頼 (Interrupt)\n",
    "print(\"\\n--- 改善ループ ステップ2: ユーザーが改善点を入力し、LLMが改善 ---\")\n",
    "app.update_state(config, {\"user_critique\": \"もっと具体的な素材の提案と、ユニークな形状のアイデアを加えてください。\"})\n",
    "current_state = app.invoke(None, config) # configを渡して再開\n",
    "print(f\"AIからの提案(2回目):\\n{current_state['current_proposal']}\")\n",
    "assert app.get_state(config).next == (\"request_critique\",)\n",
    "\n",
    "# ステップ3: 人間が「完璧」と評価 -> 改善ループ終了 -> 最終承認依頼 (Interrupt)\n",
    "print(\"\\n--- 改善ループ ステップ3: ユーザーが「完璧」と評価し、最終承認へ ---\")\n",
    "app.update_state(config, {\"user_critique\": \"完璧\"})\n",
    "current_state = app.invoke(None, config)\n",
    "print(f\"最終承認前の提案:\\n{current_state['current_proposal']}\")\n",
    "assert app.get_state(config).next == (\"request_final_approval\",) # request_final_approvalの後で止まっているはず\n",
    "\n",
    "# ステップ4: 人間が最終承認 -> プロセス完了 (END)\n",
    "print(\"\\n--- 最終承認ステップ: ユーザーが最終承認 ---\")\n",
    "app.update_state(config, {\"user_critique\": \"approve\", \"is_final_proposal_approved_by_human\": True})\n",
    "final_state = app.invoke(None, config)\n",
    "print(f\"最終状態のメッセージ: {final_state['messages'][-1].content}\")\n",
    "assert app.get_state(config).next == () # グラフが終了している\n",
    "print(\"\\n--- 複合制御フローテスト完了 ---\")\n",
    "\n",
    "# (オプション) 最大イテレーションで改善ループが終了するケース\n",
    "print(\"\\n--- 複合制御フローテスト (最大イテレーションケース) ---\")\n",
    "thread_id_max_iter = str(uuid.uuid4())\n",
    "config_max_iter = {\"configurable\": {\"thread_id\": thread_id_max_iter}}\n",
    "current_state_max_iter = app.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=initial_user_request)],\n",
    "        \"original_request\": initial_user_request,\n",
    "        \"iteration_count\": 0,\n",
    "        \"max_iterations\": 1, # 最大1イテレーションに設定\n",
    "        \"is_proposal_accepted_by_llm\": False,\n",
    "    },\n",
    "    config_max_iter\n",
    ")\n",
    "print(f\"AIからの提案(1回目):\\n{current_state_max_iter['current_proposal']}\")\n",
    "app.update_state(config_max_iter, {\"user_critique\": \"もっとカラフルにできないか？\"}) # 完璧とは言わない\n",
    "current_state_max_iter = app.invoke(None, config_max_iter)\n",
    "print(f\"最大イテレーション後の提案:\\n{current_state_max_iter['current_proposal']}\") \n",
    "# この時点で iteration_count が max_iterations に達しているので、次は final_approval に進むはず\n",
    "assert app.get_state(config_max_iter).next == (\"request_final_approval\",)\n",
    "print(\"最大イテレーションで改善ループが終了し、最終承認に進むことを確認。\")\n",
    "app.update_state(config_max_iter, {\"user_critique\": \"approve\", \"is_final_proposal_approved_by_human\": True})\n",
    "final_state_max_iter = app.invoke(None, config_max_iter)\n",
    "assert app.get_state(config_max_iter).next == () \n",
    "print(\"最大イテレーションケースも正常に完了。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答005</summary>\n",
    "\n",
    "``````python\n",
    "# 解答005\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "from langgraph.graph import StateGraph, END, Interrupt\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "import uuid\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class IterativeApprovalState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    original_request: str # ユーザーの最初の要求\n",
    "    current_proposal: Optional[str] # LLMによる現在の提案\n",
    "    user_critique: Optional[str] # ユーザーからの批判や改善点 (改善ループと最終承認の両方で利用)\n",
    "    iteration_count: int # 改善ループの現在のイテレーション回数\n",
    "    max_iterations: int # 改善ループの最大イテレーション回数\n",
    "    is_proposal_accepted_by_llm: bool # LLMが提案は十分と判断したか (改善ループ終了条件)\n",
    "    is_final_proposal_approved_by_human: Optional[bool] # 人間による最終承認\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def generate_proposal_node(state: IterativeApprovalState):\n",
    "    # LLMが提案を生成または改善するノード\n",
    "    iteration = state.get(\"iteration_count\", 0) + 1\n",
    "    print(f\"generate_proposal_node: イテレーション {iteration}\")\n",
    "\n",
    "    if iteration == 1:\n",
    "        prompt_text = f\"以下の要望に基づいて、詳細な提案書を作成してください。\\n\\n要望: {state['original_request']}\"\n",
    "    else:\n",
    "        prompt_text = f\"前回の提案「{state['current_proposal']}」に対して、ユーザーから以下の改善点が指摘されました。「{state['user_critique']}」。これらの点を踏まえて提案を大幅に改善してください。もし改善点が特にない、または提案が完璧だと思ったら、提案の最後に「提案は完璧です。」と記述してください。\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt_text)])\n",
    "    proposal = response.content\n",
    "    print(f\"generate_proposal_node: 生成された提案「{proposal}」\")\n",
    "    \n",
    "    accepted_by_llm = \"提案は完璧です。\" in proposal\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=proposal)],\n",
    "        \"current_proposal\": proposal,\n",
    "        \"iteration_count\": iteration,\n",
    "        \"is_proposal_accepted_by_llm\": accepted_by_llm,\n",
    "        \"user_critique\": None # 消費したのでクリア\n",
    "    }\n",
    "\n",
    "def request_human_critique_node(state: IterativeApprovalState):\n",
    "    # ユーザーに提案の評価（批判や改善点）を求めるノード (改善ループ用Interrupt)\n",
    "    print(\"request_human_critique_node: 生成された提案について評価を入力してください。改善ループを終了し最終承認に進む場合は '完璧' と入力してください。\")\n",
    "    # Interruptはこのノードの後に発生\n",
    "    return {}\n",
    "\n",
    "def request_final_approval_node(state: IterativeApprovalState):\n",
    "    # 最終提案についてユーザーに承認を求めるノード (最終承認用Interrupt)\n",
    "    proposal = state.get(\"current_proposal\", \"\")\n",
    "    print(f\"request_final_approval_node: 最終提案が完成しました。ご確認ください。承認する場合は 'approve' と入力してください。\\n---\\n{proposal}\\n---\")\n",
    "    # Interruptはこのノードの後に発生\n",
    "    return {}\n",
    "\n",
    "def process_complete_node(state: IterativeApprovalState):\n",
    "    # 全てのプロセスが完了したことを示すノード\n",
    "    print(\"process_complete_node: 提案は最終承認され、プロセスは完了しました。\")\n",
    "    return {\"messages\": [AIMessage(content=\"提案は最終承認され、プロセスは完了しました。\")]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def route_after_critique(state: IterativeApprovalState):\n",
    "    # ユーザーの評価とLLMの判断に基づいて改善ループを継続するか、最終承認に進むかを判断\n",
    "    user_critique = state.get(\"user_critique\", \"\").strip().lower()\n",
    "    iteration = state[\"iteration_count\"]\n",
    "    max_iter = state[\"max_iterations\"]\n",
    "    llm_thinks_perfect = state.get(\"is_proposal_accepted_by_llm\", False)\n",
    "\n",
    "    print(f\"route_after_critique: ユーザー評価「{user_critique}」, LLM判断「{llm_thinks_perfect}」, イテレーション {iteration}/{max_iter}\")\n",
    "\n",
    "    if user_critique == \"完璧\" or llm_thinks_perfect or iteration >= max_iter:\n",
    "        print(\"route_after_critique: 改善ループ終了。最終承認へ。\")\n",
    "        return \"proceed_to_final_approval\" # 最終承認へ\n",
    "    else:\n",
    "        print(\"route_after_critique: 改善が必要。改善ループ継続。\")\n",
    "        return \"continue_improvement\" # 改善ループ継続 (generate_proposal_nodeへ)\n",
    "\n",
    "def route_after_final_approval(state: IterativeApprovalState):\n",
    "    # ユーザーの最終承認に基づいてプロセスを完了するか、再度検討を促す（ここでは簡略化）\n",
    "    # このルーターはfinal_approval_request_nodeの後のInterruptの後に呼び出される\n",
    "    final_approval_feedback = state.get(\"user_critique\", \"\").strip().lower() # user_critiqueを最終承認のフィードバックにも使う\n",
    "    is_human_approved = state.get(\"is_final_proposal_approved_by_human\", False)\n",
    "    print(f\"route_after_final_approval: 最終承認フィードバック「{final_approval_feedback}」, 人間承認フラグ: {is_human_approved}\")\n",
    "\n",
    "    if final_approval_feedback == \"approve\" and is_human_approved:\n",
    "        print(\"route_after_final_approval: 最終承認。プロセス完了へ。\")\n",
    "        return \"complete_process\" # プロセス完了へ\n",
    "    else:\n",
    "        # ここでは簡略化のため、非承認の場合も終了するが、実際は最初のプロセスに戻るなどの処理も考えられる\n",
    "        print(\"route_after_final_approval: 最終的に非承認または不正な状態。プロセス終了（要改善）。\")\n",
    "        return \"end_process_unapproved\" # プロセス終了 (ENDへ)\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(IterativeApprovalState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"generate_proposal\", generate_proposal_node)\n",
    "workflow.add_node(\"request_critique\", request_human_critique_node)\n",
    "workflow.add_node(\"request_final_approval\", request_final_approval_node)\n",
    "workflow.add_node(\"process_complete\", process_complete_node)\n",
    "\n",
    "# エントリポイント\n",
    "workflow.set_entry_point(\"generate_proposal\")\n",
    "\n",
    "# 改善ループのフロー\n",
    "workflow.add_edge(\"generate_proposal\", \"request_critique\") # 提案生成後、人間による評価依頼へ\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"request_critique\", # このノードの後にInterruptが入り、再開時にこのルーターが呼ばれる\n",
    "    route_after_critique,\n",
    "    {\n",
    "        \"continue_improvement\": \"generate_proposal\", # 改善を続ける場合\n",
    "        \"proceed_to_final_approval\": \"request_final_approval\" # 最終承認に進む場合\n",
    "    }\n",
    ")\n",
    "\n",
    "# 最終承認フロー\n",
    "workflow.add_conditional_edges(\n",
    "    \"request_final_approval\", # このノードの後にInterruptが入り、再開時にこのルーターが呼ばれる\n",
    "    route_after_final_approval,\n",
    "    {\n",
    "        \"complete_process\": \"process_complete\",\n",
    "        \"end_process_unapproved\": END \n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"process_complete\", END)\n",
    "\n",
    "# グラフのコンパイル (複数のInterruptポイントを指定)\n",
    "app = workflow.compile(interrupt_after=[\"request_critique\", \"request_final_approval\"])\n",
    "\n",
    "# --- グラフの可視化 (オプション) ---\n",
    "try:\n",
    "    img_bytes = app.get_graph().draw_png()\n",
    "    display(Image(img_bytes))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}\")\n",
    "\n",
    "# --- グラフ実行シミュレーション ---\n",
    "print(\"\\n--- 複合制御フローテスト開始 ---\")\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "initial_user_request = \"新しいエコフレンドリーなコーヒーカップのデザイン案を作成してください。ターゲットは若年層で、持ち運びやすさが重要です。\"\n",
    "\n",
    "# ステップ1: 最初の提案生成 -> 人間による評価依頼 (Interrupt)\n",
    "print(\"\\n--- 改善ループ ステップ1: 最初の提案と評価依頼 ---\")\n",
    "current_state = app.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=initial_user_request)],\n",
    "        \"original_request\": initial_user_request,\n",
    "        \"iteration_count\": 0,\n",
    "        \"max_iterations\": 3,\n",
    "        \"is_proposal_accepted_by_llm\": False,\n",
    "        \"is_final_proposal_approved_by_human\": None # 初期化\n",
    "    },\n",
    "    config\n",
    ")\n",
    "print(f\"AIからの提案(1回目):\\n{current_state['current_proposal']}\")\n",
    "assert current_state[\"messages\"][-1].type == \"ai\" # AIからのメッセージがあるはず\n",
    "assert app.get_state(config).next == (\"request_critique\",) # request_critiqueの後で止まっているはず\n",
    "\n",
    "# ステップ2: 人間が改善点を入力 -> LLMが改善案を生成 -> 再度評価依頼 (Interrupt)\n",
    "print(\"\\n--- 改善ループ ステップ2: ユーザーが改善点を入力し、LLMが改善 ---\")\n",
    "app.update_state(config, {\"user_critique\": \"もっと具体的な素材の提案と、ユニークな形状のアイデアを加えてください。\"})\n",
    "current_state = app.invoke(None, config) # configを渡して再開\n",
    "print(f\"AIからの提案(2回目):\\n{current_state['current_proposal']}\")\n",
    "assert app.get_state(config).next == (\"request_critique\",)\n",
    "\n",
    "# ステップ3: 人間が「完璧」と評価 -> 改善ループ終了 -> 最終承認依頼 (Interrupt)\n",
    "print(\"\\n--- 改善ループ ステップ3: ユーザーが「完璧」と評価し、最終承認へ ---\")\n",
    "app.update_state(config, {\"user_critique\": \"完璧\"})\n",
    "current_state = app.invoke(None, config)\n",
    "print(f\"最終承認前の提案:\\n{current_state['current_proposal']}\")\n",
    "assert app.get_state(config).next == (\"request_final_approval\",) # request_final_approvalの後で止まっているはず\n",
    "\n",
    "# ステップ4: 人間が最終承認 -> プロセス完了 (END)\n",
    "print(\"\\n--- 最終承認ステップ: ユーザーが最終承認 ---\")\n",
    "app.update_state(config, {\"user_critique\": \"approve\", \"is_final_proposal_approved_by_human\": True})\n",
    "final_state = app.invoke(None, config)\n",
    "print(f\"最終状態のメッセージ: {final_state['messages'][-1].content}\")\n",
    "assert app.get_state(config).next == () # グラフが終了している\n",
    "print(\"\\n--- 複合制御フローテスト完了 ---\")\n",
    "\n",
    "# (オプション) 最大イテレーションで改善ループが終了するケース\n",
    "print(\"\\n--- 複合制御フローテスト (最大イテレーションケース) ---\")\n",
    "thread_id_max_iter = str(uuid.uuid4())\n",
    "config_max_iter = {\"configurable\": {\"thread_id\": thread_id_max_iter}}\n",
    "current_state_max_iter = app.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=initial_user_request)],\n",
    "        \"original_request\": initial_user_request,\n",
    "        \"iteration_count\": 0,\n",
    "        \"max_iterations\": 1, # 最大1イテレーションに設定\n",
    "        \"is_proposal_accepted_by_llm\": False,\n",
    "        \"is_final_proposal_approved_by_human\": None\n",
    "    },\n",
    "    config_max_iter\n",
    ")\n",
    "print(f\"AIからの提案(1回目):\\n{current_state_max_iter['current_proposal']}\")\n",
    "app.update_state(config_max_iter, {\"user_critique\": \"もっとカラフルにできないか？\"}) # 完璧とは言わない\n",
    "current_state_max_iter = app.invoke(None, config_max_iter)\n",
    "print(f\"最大イテレーション後の提案:\\n{current_state_max_iter['current_proposal']}\") \n",
    "# この時点で iteration_count が max_iterations に達しているので、次は final_approval に進むはず\n",
    "assert app.get_state(config_max_iter).next == (\"request_final_approval\",)\n",
    "print(\"最大イテレーションで改善ループが終了し、最終承認に進むことを確認。\")\n",
    "app.update_state(config_max_iter, {\"user_critique\": \"approve\", \"is_final_proposal_approved_by_human\": True})\n",
    "final_state_max_iter = app.invoke(None, config_max_iter)\n",
    "assert app.get_state(config_max_iter).next == () \n",
    "print(\"最大イテレーションケースも正常に完了。\")\n",
    "``````\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
