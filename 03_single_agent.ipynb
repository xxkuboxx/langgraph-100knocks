{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3章: シングルエージェントとツール活用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "以下のセルを順番に実行して、演習に必要な環境をセットアップします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "\n",
    "このセルは、LangGraphおよび関連するLangChainライブラリ、特にツール利用に必要な `langchain_community` や、検索ツール用の `duckduckgo-search` などをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ライブラリのインストール ===\n",
    "!pip install -qU langchain langgraph langchain_openai langchain_community duckduckgo-search\n",
    "\n",
    "# --- その他の推奨ライブラリ ---\n",
    "!pip install -qU python-dotenv pygraphviz pydotplus graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI APIキーの設定\n",
    "\n",
    "このノートブックでは、エージェントの中核となるLLMとしてOpenAIのモデルを使用します。\n",
    "事前にOpenAIのAPIキーを取得し、環境変数 `OPENAI_API_KEY` に設定するか、Google Colabの場合はColabのシークレットマネージャーに `OPENAI_API_KEY` という名前でキーを登録してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    if \"OPENAI_API_KEY\" in userdata.get_all():\n",
    "        os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "    print(\"OpenAI APIキーをColabシークレットからロードしました。\")\n",
    "except ImportError:\n",
    "    if os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"OpenAI APIキーを.envファイルからロードしました。\")\n",
    "    else:\n",
    "        print(\"OpenAI APIキーが見つかりません。環境変数に設定するか、Colabシークレットに登録してください。\")\n",
    "\n",
    "# LLMとToolの準備\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
    "\n",
    "# OpenAIモデルを初期化 (ツール呼び出しに対応したモデルが望ましい)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "print(f\"LLM ({llm.model_name}) と検索ツール ({search_tool.name}) の準備ができました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題001: 基本的なツールの実装と ToolNode の使用\n",
    "\n",
    "### 課題\n",
    "LangGraphエージェントが外部の機能を利用するためには、「ツール」を定義し、それをグラフ内で呼び出す必要があります。この問題では、簡単な算術演算を行うツール（例: 二つの数値を足し算する）を作成し、`ToolNode` を使ってグラフに組み込み、LLMからの指示でそのツールを実行させる方法を学びます。\n",
    "\n",
    "*   **学習内容:** `@tool`デコレータを使ったツールの定義方法、`ToolNode`の基本的な使い方、LLMにツールを使わせるためのプロンプトの工夫（限定的）、そしてツール実行結果を状態に反映させる方法を理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "\n",
    "# --- ツールの定義 ---\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"二つの整数 a と b を足し算します。\"\"\"\n",
    "    print(f\"Tool 'add_numbers' called with a={a}, b={b}\")\n",
    "    return a + b\n",
    "\n",
    "tools = [add_numbers]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class BasicToolAgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    # 他に必要な状態があれば追加 (例: tool_call_idなど)\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def agent_node(state: BasicToolAgentState, config):\n",
    "    # LLMにツール利用を促す (tool_choiceはまだ使わない)\n",
    "    # bind_tools を使ってLLMにツール情報を渡す\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# --- ルーター関数 (ツール呼び出しがあるか判断) ---\n",
    "def should_call_tool_router(state: BasicToolAgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"call_tool_node\" # ToolNodeへ\n",
    "    return END # ツール呼び出しがなければ終了\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(BasicToolAgentState)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tool_executor\", tool_node) # ToolNodeを追加\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_call_tool_router,\n",
    "    {\n",
    "        \"call_tool_node\": \"tool_executor\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tool_executor\", \"agent\") # ツール実行後、再度agentノードへ戻り結果を処理\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- 実行 --- \n",
    "print(\"--- 基本ツールのテスト (5 + 8 の計算) ---\")\n",
    "initial_messages = [HumanMessage(content=\"5たす8を計算してください。\")]\n",
    "inputs = {\"messages\": initial_messages}\n",
    "for event in graph.stream(inputs, {\"recursion_limit\": 5}):\n",
    "    print(event)\n",
    "\n",
    "final_state = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "print(\"\\n最終的なAIの応答:\")\n",
    "for msg in final_state[\"messages\"]:\n",
    "    if isinstance(msg, AIMessage) and not msg.tool_calls:\n",
    "        print(msg.content)\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langgraph.prebuilt import ToolNode # ToolNodeをインポート\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- ツールの定義 ---\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"二つの整数 a と b を足し算し、その結果を返します。\"\"\"\n",
    "    print(f\"Tool 'add_numbers' is called with a={a}, b={b}\")\n",
    "    result = a + b\n",
    "    print(f\"  -> Result: {result}\")\n",
    "    return result\n",
    "\n",
    "tools_list = [add_numbers] # ツールのリスト\n",
    "tool_executor_node = ToolNode(tools_list) # ToolNodeのインスタンス化\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class BasicToolAgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    # ツール呼び出しIDや結果を明示的に状態に持ちたい場合はここに追加\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def agent_decision_node(state: BasicToolAgentState, config):\n",
    "    print(\"\\nagent_decision_node: LLMに判断を仰ぎます...\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    print(f\"  -> 現在のメッセージ履歴: {current_messages}\")\n",
    "    \n",
    "    # LLMにツール情報をバインドして、ツール利用を判断させる\n",
    "    llm_with_tools = llm.bind_tools(tools_list)\n",
    "    ai_response = llm_with_tools.invoke(current_messages)\n",
    "    print(f\"  -> LLMからの応答: {ai_response}\")\n",
    "    \n",
    "    return {\"messages\": [ai_response]} # LLMの応答(AIMessage)を履歴に追加\n",
    "\n",
    "# --- ルーター関数 (ツール呼び出しがあるか判断) ---\n",
    "def should_call_tool_router(state: BasicToolAgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    print(f\"\\nshould_call_tool_router: 最後のメッセージを評価 -> {last_message}\")\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        print(\"  -> ツール呼び出し検知。tool_executor_nodeへ\")\n",
    "        return \"call_tool_node\" # ToolNodeへ遷移するエッジ名\n",
    "    print(\"  -> ツール呼び出しなし。ENDへ\")\n",
    "    return END # ツール呼び出しがなければ終了\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(BasicToolAgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", agent_decision_node)\n",
    "workflow.add_node(\"tool_executor\", tool_executor_node) # ToolNodeをグラフに追加\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\", # agentノードの後に分岐\n",
    "    should_call_tool_router, # ルーター関数\n",
    "    {\n",
    "        \"call_tool_node\": \"tool_executor\", # ルーターが \"call_tool_node\" を返したら tool_executor へ\n",
    "        END: END  # ルーターが END を返したら終了\n",
    "    }\n",
    ")\n",
    "\n",
    "# ToolNodeの実行後、再度agentノードに戻ってツール実行結果を処理させる\n",
    "workflow.add_edge(\"tool_executor\", \"agent\") \n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- 実行と結果確認 ---\n",
    "print(\"\\n--- 基本ツールのテスト (5 + 8 の計算) ---\")\n",
    "initial_messages_add = [HumanMessage(content=\"5たす8を計算して、結果を教えてください。\")]\n",
    "inputs_add = {\"messages\": initial_messages_add}\n",
    "\n",
    "print(\"\\nストリーム出力 (加算):\")\n",
    "for i, event in enumerate(graph.stream(inputs_add, {\"recursion_limit\": 5})):\n",
    "    print(f\"Event {i+1}: {event}\")\n",
    "\n",
    "final_state_add = graph.invoke(inputs_add, {\"recursion_limit\": 5})\n",
    "print(\"\\n最終的なAIの応答 (加算):\")\n",
    "found_final_answer = False\n",
    "for msg in reversed(final_state_add[\"messages\"]):\n",
    "    if isinstance(msg, AIMessage) and not msg.tool_calls:\n",
    "        print(f\"  -> {msg.content}\")\n",
    "        assert \"13\" in msg.content # 5+8=13\n",
    "        found_final_answer = True\n",
    "        break\n",
    "assert found_final_answer, \"最終的なAIの応答が見つかりませんでした。\"\n",
    "print(\"アサーション成功！\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題002: ReAct 風エージェント - LLM によるツール使用判断と応答選択\n",
    "\n",
    "### 課題\n",
    "より自律的なエージェントは、ユーザーの質問に応じて、ツールを使うべきか、それとも直接応答すべきかをLLM自身が判断します。この問題では、検索ツール（DuckDuckGoSearchRun）を用意し、LLMが「検索が必要な質問」か「直接回答できる質問」かを判断し、必要ならツールを実行、そうでなければ直接応答する、というReAct (Reasoning and Acting) の基本的な考え方を取り入れたエージェントを構築します。\n",
    "\n",
    "*   **学習内容:** LLMの応答にツール呼び出し(tool_calls)が含まれているかどうかで処理を分岐する方法、ツール実行結果(ToolMessage)をLLMにフィードバックして最終応答を生成させる流れを理解します。`bind_tools` を使ってLLMに利用可能なツールを認識させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "\n",
    "# search_tool は準備セルで初期化済み (DuckDuckGoSearchRun)\n",
    "react_tools = [search_tool]\n",
    "react_tool_node = ToolNode(react_tools)\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ReActAgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def react_agent_node(state: ReActAgentState, config):\n",
    "    llm_with_tools = llm.bind_tools(react_tools)\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# --- ルーター関数 ---\n",
    "def react_router(state: ReActAgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"call_tool\" # ツール実行へ\n",
    "    return END # 直接応答またはツール実行後の最終応答として終了\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(ReActAgentState)\n",
    "workflow.add_node(\"agent\", react_agent_node)\n",
    "workflow.add_node(\"tool_executor\", react_tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    react_router,\n",
    "    {\"call_tool\": \"tool_executor\", END: END}\n",
    ")\n",
    "workflow.add_edge(\"tool_executor\", \"agent\") # ツール実行後、再度agentへ\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- 実行 --- \n",
    "print(\"--- ReAct風エージェントテスト (検索が必要な質問) ---\")\n",
    "inputs_search = {\"messages\": [HumanMessage(content=\"今日の東京の天気は？\")]}\n",
    "for event in graph.stream(inputs_search, {\"recursion_limit\": 5}): print(event)\n",
    "\n",
    "print(\"\\n--- ReAct風エージェントテスト (直接回答できる質問) ---\")\n",
    "inputs_direct = {\"messages\": [HumanMessage(content=\"こんにちは！元気ですか？\")]}\n",
    "for event in graph.stream(inputs_direct, {\"recursion_limit\": 5}): print(event)\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# search_tool (DuckDuckGoSearchRun) は準備セルで初期化済み\n",
    "react_tools_list = [search_tool]\n",
    "react_tool_executor_node = ToolNode(react_tools_list)\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ReActAgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def react_agent_node(state: ReActAgentState, config):\n",
    "    print(\"\\nreact_agent_node: LLMが次のアクションを決定します...\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    print(f\"  -> 現在のメッセージ: {current_messages[-1]}\") # 最新のメッセージを表示\n",
    "    \n",
    "    # LLMにツールをバインド\n",
    "    llm_with_react_tools = llm.bind_tools(react_tools_list)\n",
    "    ai_response = llm_with_react_tools.invoke(current_messages)\n",
    "    print(f\"  -> LLMの応答: {ai_response}\")\n",
    "    \n",
    "    return {\"messages\": [ai_response]}\n",
    "\n",
    "# --- ルーター関数 (ツール呼び出しがあるか、または終了か) ---\n",
    "def react_conditional_router(state: ReActAgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    print(f\"\\nreact_conditional_router: 最後のメッセージを評価 -> {last_message.pretty_repr()[:500]}...\") # 長いToolMessageを考慮\n",
    "    \n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        print(\"  -> ツール呼び出し検知。tool_executorへ\")\n",
    "        return \"call_tool_executor\" # ToolNodeへ\n",
    "    \n",
    "    # ツール呼び出しがなく、AIMessageであれば、それが最終応答の可能性が高い\n",
    "    # (ToolMessageの後に再度agentに来た場合も、最終的にtool_callsなしのAIMessageになるはず)\n",
    "    if isinstance(last_message, AIMessage):\n",
    "        print(\"  -> ツール呼び出しなしのAIMessage。ENDへ\")\n",
    "        return END \n",
    "    \n",
    "    # 通常は上記2つのどちらかで分岐するはずだが、念のためフォールバック\n",
    "    print(\"  -> 予期せぬ状態。暫定的にENDへ\")\n",
    "    return END\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(ReActAgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", react_agent_node)\n",
    "workflow.add_node(\"tool_executor\", react_tool_executor_node)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    react_conditional_router,\n",
    "    {\n",
    "        \"call_tool_executor\": \"tool_executor\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# ツール実行後、再度agentノードに戻り、ツール結果を考慮して次の判断をさせる\n",
    "workflow.add_edge(\"tool_executor\", \"agent\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- 実行と結果確認 ---\n",
    "print(\"\\n--- ReAct風エージェントテスト (検索が必要な質問: 今日の東京の天気は？) ---\")\n",
    "inputs_search_q = {\"messages\": [SystemMessage(content=\"ユーザーの質問に答えてください。必要なら検索ツールを使って情報を調べてください。\"), HumanMessage(content=\"今日の東京の天気は？\")]}\n",
    "\n",
    "print(\"\\nストリーム出力 (検索質問):\")\n",
    "for i, event in enumerate(graph.stream(inputs_search_q, {\"recursion_limit\": 5})):\n",
    "    print(f\"Event {i+1}: {event}\")\n",
    "\n",
    "final_state_search = graph.invoke(inputs_search_q, {\"recursion_limit\": 5})\n",
    "print(\"\\n最終的なAIの応答 (検索質問):\")\n",
    "search_answer_found = False\n",
    "for msg in reversed(final_state_search[\"messages\"]):\n",
    "    if isinstance(msg, AIMessage) and not msg.tool_calls:\n",
    "        print(f\"  -> {msg.content}\")\n",
    "        search_answer_found = True\n",
    "        break\n",
    "assert search_answer_found, \"検索質問に対する最終応答が見つかりませんでした。\"\n",
    "\n",
    "print(\"\\n--- ReAct風エージェントテスト (直接回答できる質問: こんにちは！調子はどう？) ---\")\n",
    "inputs_direct_q = {\"messages\": [SystemMessage(content=\"ユーザーの質問にフレンドリーに答えてください。\"), HumanMessage(content=\"こんにちは！調子はどう？\")]}\n",
    "\n",
    "print(\"\\nストリーム出力 (直接質問):\")\n",
    "for i, event in enumerate(graph.stream(inputs_direct_q, {\"recursion_limit\": 5})):\n",
    "    print(f\"Event {i+1}: {event}\")\n",
    "\n",
    "final_state_direct = graph.invoke(inputs_direct_q, {\"recursion_limit\": 5})\n",
    "print(\"\\n最終的なAIの応答 (直接質問):\")\n",
    "direct_answer_found = False\n",
    "for msg in reversed(final_state_direct[\"messages\"]):\n",
    "    if isinstance(msg, AIMessage) and not msg.tool_calls:\n",
    "        print(f\"  -> {msg.content}\")\n",
    "        direct_answer_found = True\n",
    "        break\n",
    "assert direct_answer_found, \"直接質問に対する最終応答が見つかりませんでした。\"\n",
    "print(\"\\nアサーション成功！\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題003: 複数ツールからの選択\n",
    "\n",
    "### 課題\n",
    "エージェントが複数のツールを利用できる場合、状況に応じて最適なツールを選択する能力が求められます。この問題では、簡単な算術演算ツール（例: `add_numbers`）と文字列操作ツール（例: `reverse_string`）の二つを用意し、LLMがユーザーの指示内容を解釈して、どちらのツール（またはどちらも使わない）を実行すべきか判断するエージェントを構築します。\n",
    "\n",
    "*   **学習内容:** 複数のツールをリストとしてLLMにバインドする方法、LLMの応答に含まれる`tool_calls`リストから呼び出すべきツールとその引数を特定する方法、そして選択されたツールのみを実行するロジックを理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# --- ツールの定義 ---\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"整数aとbを加算します。\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def reverse_string(text: str) -> str:\n",
    "    \"\"\"文字列textを逆順にします。\"\"\"\n",
    "    return text[::-1]\n",
    "\n",
    "multi_tools = [add, reverse_string]\n",
    "multi_tool_node = ToolNode(multi_tools)\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class MultiToolAgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def multi_tool_agent_node(state: MultiToolAgentState, config):\n",
    "    llm_with_multi_tools = llm.bind_tools(multi_tools)\n",
    "    response = llm_with_multi_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# --- ルーター関数 ---\n",
    "def multi_tool_router(state: MultiToolAgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"execute_tool\"\n",
    "    return END\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(MultiToolAgentState)\n",
    "workflow.add_node(\"agent\", multi_tool_agent_node)\n",
    "workflow.add_node(\"tool_executor\", multi_tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    multi_tool_router,\n",
    "    {\"execute_tool\": \"tool_executor\", END: END}\n",
    ")\n",
    "workflow.add_edge(\"tool_executor\", \"agent\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- 実行 --- \n",
    "print(\"--- 複数ツール選択テスト (加算) ---\")\n",
    "inputs_add_test = {\"messages\": [HumanMessage(content=\"123 と 456 を足してください。\")]}\n",
    "for event in graph.stream(inputs_add_test, {\"recursion_limit\": 5}): print(event)\n",
    "\n",
    "print(\"\\n--- 複数ツール選択テスト (文字列逆順) ---\")\n",
    "inputs_reverse_test = {\"messages\": [HumanMessage(content=\"'hello world' を逆にして。\")]}\n",
    "for event in graph.stream(inputs_reverse_test, {\"recursion_limit\": 5}): print(event)\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- ツールの定義 ---\n",
    "@tool\n",
    "def add_numbers_tool(a: int, b: int) -> int:\n",
    "    \"\"\"二つの整数 a と b を足し算し、その結果を返します。\"\"\"\n",
    "    print(f\"Tool 'add_numbers_tool' called with a={a}, b={b}\")\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def reverse_string_tool(text: str) -> str:\n",
    "    \"\"\"与えられた文字列 text を逆順にして返します。\"\"\"\n",
    "    print(f\"Tool 'reverse_string_tool' called with text='{text}'\")\n",
    "    return text[::-1]\n",
    "\n",
    "available_tools = [add_numbers_tool, reverse_string_tool]\n",
    "selective_tool_node = ToolNode(available_tools)\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class MultiToolAgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def multi_tool_agent_node(state: MultiToolAgentState, config):\n",
    "    print(\"\\nmulti_tool_agent_node: LLMがツール選択または直接応答を判断します...\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    print(f\"  -> 現在のメッセージ (最新): {current_messages[-1]}\")\n",
    "    \n",
    "    # LLMに複数のツールをバインド\n",
    "    llm_with_multiple_tools = llm.bind_tools(available_tools)\n",
    "    ai_response = llm_with_multiple_tools.invoke(current_messages)\n",
    "    print(f\"  -> LLMの応答: {ai_response}\")\n",
    "    \n",
    "    return {\"messages\": [ai_response]}\n",
    "\n",
    "# --- ルーター関数 ---\n",
    "def route_tool_or_end(state: MultiToolAgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    print(f\"\\nroute_tool_or_end: 最後のメッセージを評価 -> {last_message.pretty_repr()[:300]}...\")\n",
    "    \n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        print(\"  -> ツール呼び出し検知。tool_executorへ\")\n",
    "        return \"call_selected_tool\"\n",
    "    print(\"  -> ツール呼び出しなし。ENDへ\")\n",
    "    return END\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(MultiToolAgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", multi_tool_agent_node)\n",
    "workflow.add_node(\"tool_executor\", selective_tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    route_tool_or_end,\n",
    "    {\n",
    "        \"call_selected_tool\": \"tool_executor\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tool_executor\", \"agent\") # ツール実行後、再度agentへ\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- 実行と結果確認 ---\n",
    "system_prompt = SystemMessage(content=\"あなたはユーザーのリクエストに応じて、利用可能なツールを適切に選択して実行するか、直接応答するAIアシスタントです。\")\n",
    "\n",
    "print(\"\\n--- 複数ツールからの選択テスト (加算タスク: 123 + 789) ---\")\n",
    "inputs_add_task = {\"messages\": [system_prompt, HumanMessage(content=\"123 と 789 を足した結果を教えてください。\")]}\n",
    "print(\"\\nストリーム出力 (加算タスク):\")\n",
    "for i, event in enumerate(graph.stream(inputs_add_task, {\"recursion_limit\": 5})):\n",
    "    print(f\"Event {i+1}: {event}\")\n",
    "final_state_add_task = graph.invoke(inputs_add_task, {\"recursion_limit\": 5})\n",
    "add_task_answered = False\n",
    "for msg in reversed(final_state_add_task[\"messages\"]):\n",
    "    if isinstance(msg, AIMessage) and not msg.tool_calls:\n",
    "        print(f\"  -> 最終応答 (加算): {msg.content}\")\n",
    "        assert \"912\" in msg.content # 123 + 789 = 912\n",
    "        add_task_answered = True\n",
    "        break\n",
    "assert add_task_answered\n",
    "\n",
    "print(\"\\n--- 複数ツールからの選択テスト (文字列逆順タスク: 'LangGraph') ---\")\n",
    "inputs_reverse_task = {\"messages\": [system_prompt, HumanMessage(content=\"'LangGraph' という文字列を逆順にしてください。\")]}\n",
    "print(\"\\nストリーム出力 (文字列逆順タスク):\")\n",
    "for i, event in enumerate(graph.stream(inputs_reverse_task, {\"recursion_limit\": 5})):\n",
    "    print(f\"Event {i+1}: {event}\")\n",
    "final_state_reverse_task = graph.invoke(inputs_reverse_task, {\"recursion_limit\": 5})\n",
    "reverse_task_answered = False\n",
    "for msg in reversed(final_state_reverse_task[\"messages\"]):\n",
    "    if isinstance(msg, AIMessage) and not msg.tool_calls:\n",
    "        print(f\"  -> 最終応答 (逆順): {msg.content}\")\n",
    "        assert \"hparGgnaL\" in msg.content # LangGraph の逆\n",
    "        reverse_task_answered = True\n",
    "        break\n",
    "assert reverse_task_answered\n",
    "\n",
    "print(\"\\n--- 複数ツールからの選択テスト (ツール不要な質問: こんにちは) ---\")\n",
    "inputs_no_tool_task = {\"messages\": [system_prompt, HumanMessage(content=\"こんにちは、元気ですか？\")]}\n",
    "print(\"\\nストリーム出力 (ツール不要タスク):\")\n",
    "for i, event in enumerate(graph.stream(inputs_no_tool_task, {\"recursion_limit\": 5})):\n",
    "    print(f\"Event {i+1}: {event}\")\n",
    "final_state_no_tool_task = graph.invoke(inputs_no_tool_task, {\"recursion_limit\": 5})\n",
    "no_tool_task_answered = False\n",
    "for msg in reversed(final_state_no_tool_task[\"messages\"]):\n",
    "    if isinstance(msg, AIMessage) and not msg.tool_calls:\n",
    "        print(f\"  -> 最終応答 (ツール不要): {msg.content}\")\n",
    "        no_tool_task_answered = True\n",
    "        break\n",
    "assert no_tool_task_answered\n",
    "print(\"\\n全てのアサーション成功！\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題004: ツール呼び出しの強制 (Forced Tool Calling)\n",
    "\n",
    "### 課題\n",
    "特定の状況下では、LLMに特定のツールを必ず使用させたい場合があります（例: ユーザーが明示的にツールの使用を指示した場合、あるいは特定の形式の情報を得るためにツールが必須の場合）。この問題では、LLMの`tool_choice`パラメータ（またはそれに類する機能、例: `ChatOpenAI`の`tool_choice`引数や`bind_tools(tool_choice=...)`）を使い、特定のツール（ここでは検索ツール）の呼び出しを強制する方法を学びます。\n",
    "\n",
    "*   **学習内容:** LLMの`tool_choice`機能を使って、特定のツールを強制的に呼び出させる方法を理解します。これにより、エージェントの行動をより細かく制御できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# --- ツールの定義 (検索ツールは準備セルで定義済み) ---\n",
    "# search_tool (DuckDuckGoSearchRun)\n",
    "forced_tools = [search_tool]\n",
    "forced_tool_node = ToolNode(forced_tools)\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ForcedToolAgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def forced_tool_agent_node(state: ForcedToolAgentState, config):\n",
    "    # tool_choice を使って特定のツール (ここでは search_tool) の使用を強制\n",
    "    # llm.bind_tools の tool_choice 引数にツール名を指定\n",
    "    llm_force_search = llm.bind_tools(forced_tools, tool_choice=search_tool.name)\n",
    "    response = llm_force_search.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# --- ルーター関数 (問題002と同様) ---\n",
    "def forced_tool_router(state: ForcedToolAgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"force_call_tool\"\n",
    "    return END\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(ForcedToolAgentState)\n",
    "workflow.add_node(\"agent_force_tool\", forced_tool_agent_node)\n",
    "workflow.add_node(\"tool_executor_forced\", forced_tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"agent_force_tool\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent_force_tool\",\n",
    "    forced_tool_router,\n",
    "    {\"force_call_tool\": \"tool_executor_forced\", END: END}\n",
    ")\n",
    "workflow.add_edge(\"tool_executor_forced\", \"agent_force_tool\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- 実行 --- \n",
    "print(\"--- 強制ツール呼び出しテスト (通常なら直接答えそうな質問でも検索ツールを使わせる) ---\")\n",
    "inputs_force = {\"messages\": [HumanMessage(content=\"こんにちは。LangGraphについて知っていることを教えて。\")]}\n",
    "for event in graph.stream(inputs_force, {\"recursion_limit\": 5}): print(event)\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from langchain_core.tools import tool # @tool デコレータ用\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- ツールの定義 ---\n",
    "# この問題では、準備セルで定義済みの search_tool (DuckDuckGoSearchRun) を使用します。\n",
    "# もし別のツールを強制したい場合は、ここで定義またはインポートします。\n",
    "# 例として、ダミーのカスタムツールも定義しておきます。\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"現在の時刻を文字列として返します。\"\"\"\n",
    "    import datetime\n",
    "    now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"Tool 'get_current_time' called, returning: {now}\")\n",
    "    return now\n",
    "\n",
    "tools_for_forcing = [search_tool, get_current_time] # LLMに知らせるツールのリスト\n",
    "forced_tool_executor_node = ToolNode(tools_for_forcing) # ToolNodeは全ツールを扱えるように\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ForcedToolAgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def forced_tool_agent_node(state: ForcedToolAgentState, config):\n",
    "    print(\"\\nforced_tool_agent_node: LLMに特定のツール使用を強制します...\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    print(f\"  -> 現在のメッセージ (最新): {current_messages[-1]}\")\n",
    "    \n",
    "    # `tool_choice`を使って特定のツール (ここでは search_tool.name) の使用を強制\n",
    "    # ChatOpenAI の場合は、invokeメソッドの tool_choice 引数でも指定可能\n",
    "    # llm_force_search = llm.bind_tools(tools_for_forcing, tool_choice=search_tool.name)\n",
    "    # または、invoke時に渡す方法:\n",
    "    # ai_response = llm.invoke(current_messages, tool_choice={\"type\": \"function\", \"function\": {\"name\": search_tool.name}})\n",
    "    # より汎用的なのは bind_tools での指定\n",
    "    llm_force_specific_tool = llm.bind_tools(tools_for_forcing, tool_choice=search_tool.name)\n",
    "    \n",
    "    ai_response = llm_force_specific_tool.invoke(current_messages)\n",
    "    print(f\"  -> LLMの応答 (ツール強制後): {ai_response}\")\n",
    "    \n",
    "    # 強制されたツール呼び出しが含まれているか確認 (デバッグ用)\n",
    "    if ai_response.tool_calls:\n",
    "        for tc in ai_response.tool_calls:\n",
    "            print(f\"    - Tool call id: {tc['id']}, name: {tc['name']}, args: {tc['args']}\")\n",
    "            assert tc['name'] == search_tool.name, f\"強制したはずの {search_tool.name} ではなく {tc['name']} が呼び出されました。\"\n",
    "    else:\n",
    "        print(\"    - 警告: ツール呼び出しが強制されませんでした。LLMまたはプロンプトの確認が必要です。\")\n",
    "        # この場合、テストは失敗する可能性が高い\n",
    "\n",
    "    return {\"messages\": [ai_response]}\n",
    "\n",
    "# --- ルーター関数 (ツール呼び出しがあるか、または終了か) ---\n",
    "def route_forced_tool_or_end(state: ForcedToolAgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    print(f\"\\nroute_forced_tool_or_end: 最後のメッセージを評価 -> {last_message.pretty_repr()[:300]}...\")\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        print(\"  -> ツール呼び出し検知。tool_executorへ\")\n",
    "        return \"execute_forced_tool\"\n",
    "    print(\"  -> ツール呼び出しなし。ENDへ (通常、強制後はここには来ないはず)\")\n",
    "    return END\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(ForcedToolAgentState)\n",
    "workflow.add_node(\"agent_force_tool_call\", forced_tool_agent_node)\n",
    "workflow.add_node(\"tool_executor_for_forced\", forced_tool_executor_node)\n",
    "\n",
    "workflow.set_entry_point(\"agent_force_tool_call\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent_force_tool_call\",\n",
    "    route_forced_tool_or_end,\n",
    "    {\n",
    "        \"execute_forced_tool\": \"tool_executor_for_forced\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tool_executor_for_forced\", \"agent_force_tool_call\") # ツール実行後、再度agentへ\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- 実行と結果確認 ---\n",
    "print(\"\\n--- 強制ツール呼び出しテスト --- \")\n",
    "query_for_forced_search = \"LangGraphの最新バージョンについて教えてください。\"\n",
    "print(f\"質問: '{query_for_forced_search}' (通常は検索が必要だが、ここでは検索ツール使用を強制)\")\n",
    "\n",
    "inputs_force_search = {\"messages\": [\n",
    "    SystemMessage(content=\"ユーザーの質問に答えるために、必ず指定されたツールを使ってください。\"),\n",
    "    HumanMessage(content=query_for_forced_search)\n",
    "]}\n",
    "\n",
    "print(\"\\nストリーム出力 (強制検索):\")\n",
    "tool_called_in_stream = False\n",
    "for i, event in enumerate(graph.stream(inputs_force_search, {\"recursion_limit\": 5})):\n",
    "    print(f\"Event {i+1}: {event}\")\n",
    "    # agentノードの出力でtool_callsがあるか確認\n",
    "    if 'agent_force_tool_call' in event:\n",
    "        agent_output_messages = event['agent_force_tool_call'].get('messages', [])\n",
    "        if agent_output_messages and hasattr(agent_output_messages[-1], 'tool_calls') and agent_output_messages[-1].tool_calls:\n",
    "            for tc in agent_output_messages[-1].tool_calls:\n",
    "                if tc['name'] == search_tool.name:\n",
    "                    tool_called_in_stream = True\n",
    "                    print(f\"  (ストリーム内で {search_tool.name} の呼び出しを確認)\")\n",
    "                    break\n",
    "assert tool_called_in_stream, f\"{search_tool.name} がストリーム内で呼び出されませんでした。\"\n",
    "\n",
    "final_state_forced = graph.invoke(inputs_force_search, {\"recursion_limit\": 5})\n",
    "print(\"\\n最終的なAIの応答 (強制検索後):\")\n",
    "forced_search_answered = False\n",
    "for msg in reversed(final_state_forced[\"messages\"]):\n",
    "    if isinstance(msg, AIMessage) and not msg.tool_calls:\n",
    "        print(f\"  -> {msg.content}\")\n",
    "        forced_search_answered = True\n",
    "        break\n",
    "assert forced_search_answered, \"強制検索後の最終応答が見つかりませんでした。\"\n",
    "print(\"\\nアサーション成功！\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題005: 複数ツールの並列実行\n",
    "\n",
    "### 課題\n",
    "LLMが一つの指示で複数のツールを同時に呼び出したい場合があります（例: 「今日の天気と最新ニュースを教えて」）。LangGraphと対応LLM（例: GPT-4o, Geminiの新しいバージョンなど）は、このような複数ツールの並列呼び出しをサポートします。この問題では、LLMが一度に複数のツール呼び出し（tool_callsに複数の要素）を生成し、それらが並列に実行され、結果がまとめてLLMに返されるフローを構築します。\n",
    "\n",
    "*   **学習内容:** LLMが生成する`tool_calls`リストに複数のツール呼び出しが含まれる場合の処理方法、`ToolNode`がこれらをどのように並列実行するか（または逐次実行するかはToolNodeの実装や設定によるが、LangGraphは概念的に並列性をサポート）、そして複数のツール結果(ToolMessage)をどのようにLLMにフィードバックするかを理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "\n",
    "# --- ツールの定義 (search_tool, get_current_time は問題004で定義/準備済み) ---\n",
    "parallel_tools = [search_tool, get_current_time]\n",
    "parallel_tool_node = ToolNode(parallel_tools) # ToolNodeはリスト内の全ツールを扱える\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ParallelToolAgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def parallel_tool_agent_node(state: ParallelToolAgentState, config):\n",
    "    # LLMにツール群をバインド。LLMが複数のツールコールを返すことを期待。\n",
    "    llm_with_parallel_tools = llm.bind_tools(parallel_tools)\n",
    "    response = llm_with_parallel_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# --- ルーター関数 (問題002, 004と同様) ---\n",
    "def parallel_tool_router(state: ParallelToolAgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"call_parallel_tools\"\n",
    "    return END\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(ParallelToolAgentState)\n",
    "workflow.add_node(\"agent_parallel\", parallel_tool_agent_node)\n",
    "workflow.add_node(\"tool_executor_parallel\", parallel_tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"agent_parallel\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent_parallel\",\n",
    "    parallel_tool_router,\n",
    "    {\"call_parallel_tools\": \"tool_executor_parallel\", END: END}\n",
    ")\n",
    "workflow.add_edge(\"tool_executor_parallel\", \"agent_parallel\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- 実行 --- \n",
    "print(\"--- 複数ツール並列実行テスト --- \")\n",
    "query_parallel = \"今日の東京の天気と、現在の時刻を教えてください。\"\n",
    "inputs_parallel = {\"messages\": [HumanMessage(content=query_parallel)]}\n",
    "\n",
    "for event in graph.stream(inputs_parallel, {\"recursion_limit\": 5}):\n",
    "    print(event)\n",
    "    # AIMessageのtool_callsに複数の呼び出しが含まれているか確認\n",
    "    if 'agent_parallel' in event and event['agent_parallel']['messages'][-1].tool_calls:\n",
    "        if len(event['agent_parallel']['messages'][-1].tool_calls) > 1:\n",
    "            print(\"  -> 検知: LLMが複数のツール呼び出しを生成しました！\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- ツールの定義 (search_tool, get_current_time は問題004で定義/準備済み) ---\n",
    "parallel_tools_list = [search_tool, get_current_time] # 並列実行させたいツールのリスト\n",
    "parallel_tool_executor_node = ToolNode(parallel_tools_list)\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ParallelToolAgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def parallel_tool_agent_node(state: ParallelToolAgentState, config):\n",
    "    print(\"\\nparallel_tool_agent_node: LLMが複数のツール使用または直接応答を判断します...\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    print(f\"  -> 現在のメッセージ (最新): {current_messages[-1]}\")\n",
    "    \n",
    "    llm_with_parallel_capability = llm.bind_tools(parallel_tools_list)\n",
    "    ai_response = llm_with_parallel_capability.invoke(current_messages)\n",
    "    print(f\"  -> LLMの応答: {ai_response}\")\n",
    "    \n",
    "    # LLMが複数のツールコールを返したか確認 (デバッグ用)\n",
    "    if hasattr(ai_response, 'tool_calls') and ai_response.tool_calls and len(ai_response.tool_calls) > 1:\n",
    "        print(f\"    -> 検知: LLMが {len(ai_response.tool_calls)} 個のツール呼び出しを並列で提案しました。\")\n",
    "        for tc in ai_response.tool_calls:\n",
    "            print(f\"      - Tool call id: {tc['id']}, name: {tc['name']}, args: {tc['args']}\")\n",
    "            \n",
    "    return {\"messages\": [ai_response]}\n",
    "\n",
    "# --- ルーター関数 (ツール呼び出しがあるか、または終了か) ---\n",
    "def route_parallel_tools_or_end(state: ParallelToolAgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    print(f\"\\nroute_parallel_tools_or_end: 最後のメッセージを評価 -> {last_message.pretty_repr()[:400]}...\")\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        print(\"  -> ツール呼び出し検知。tool_executorへ\")\n",
    "        return \"execute_parallel_tools\"\n",
    "    print(\"  -> ツール呼び出しなし。ENDへ\")\n",
    "    return END\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(ParallelToolAgentState)\n",
    "\n",
    "workflow.add_node(\"agent_for_parallel_calls\", parallel_tool_agent_node)\n",
    "workflow.add_node(\"tool_executor_for_parallel\", parallel_tool_executor_node)\n",
    "\n",
    "workflow.set_entry_point(\"agent_for_parallel_calls\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent_for_parallel_calls\",\n",
    "    route_parallel_tools_or_end,\n",
    "    {\n",
    "        \"execute_parallel_tools\": \"tool_executor_for_parallel\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tool_executor_for_parallel\", \"agent_for_parallel_calls\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- 実行と結果確認 ---\n",
    "print(\"\\n--- 複数ツール並列実行テスト --- \")\n",
    "query_for_parallel_execution = \"今日の東京の天気と、現在の正確な時刻を教えてください。\"\n",
    "print(f\"質問: '{query_for_parallel_execution}'\")\n",
    "\n",
    "inputs_for_parallel = {\"messages\": [\n",
    "    SystemMessage(content=\"ユーザーの質問に答えるために、利用可能なツールを最大限活用してください。複数の情報源が一度に必要なら、それらを同時に要求できます。\"),\n",
    "    HumanMessage(content=query_for_parallel_execution)\n",
    "]}\n",
    "\n",
    "print(\"\\nストリーム出力 (並列ツール呼び出し):\")\n",
    "num_tool_calls_generated = 0\n",
    "tool_messages_received = []\n",
    "\n",
    "for i, event in enumerate(graph.stream(inputs_for_parallel, {\"recursion_limit\": 5})):\n",
    "    print(f\"Event {i+1}: {event}\")\n",
    "    # agentノードの出力で複数のtool_callsがあるか確認\n",
    "    if 'agent_for_parallel_calls' in event:\n",
    "        agent_output_messages = event['agent_for_parallel_calls'].get('messages', [])\n",
    "        if agent_output_messages and hasattr(agent_output_messages[-1], 'tool_calls') and agent_output_messages[-1].tool_calls:\n",
    "            num_tool_calls_generated = len(agent_output_messages[-1].tool_calls)\n",
    "            if num_tool_calls_generated > 1:\n",
    "                print(f\"  (ストリーム内で {num_tool_calls_generated} 個のツール呼び出しをLLMが生成したのを確認)\")\n",
    "    # tool_executorノードの出力で複数のToolMessageがあるか確認\n",
    "    if 'tool_executor_for_parallel' in event:\n",
    "        tool_executor_messages = event['tool_executor_for_parallel'].get('messages', [])\n",
    "        tool_messages_received.extend([m for m in tool_executor_messages if isinstance(m, ToolMessage)])\n",
    "        if len(tool_messages_received) > 1:\n",
    "             print(f\"  (ストリーム内で {len(tool_messages_received)} 個のツール結果メッセージを受信したのを確認)\")\n",
    "\n",
    "assert num_tool_calls_generated >= 1, \"LLMがツール呼び出しを生成しませんでした。\" # 理想は >=2 だがモデルによる\n",
    "# モデルによっては厳密に2つにならない場合もあるため、ここでは1つ以上ツールが呼ばれたかで判定\n",
    "print(f\"LLMによって生成されたツールコール数: {num_tool_calls_generated}\")\n",
    "if num_tool_calls_generated < 2:\n",
    "    print(\"警告: LLMが期待通り複数のツールコールを生成しませんでした。モデルやプロンプトの調整が必要かもしれません。\")\n",
    "\n",
    "final_state_parallel = graph.invoke(inputs_for_parallel, {\"recursion_limit\": 5})\n",
    "print(\"\\n最終的なAIの応答 (並列ツール呼び出し後):\")\n",
    "parallel_answered = False\n",
    "for msg in reversed(final_state_parallel[\"messages\"]):\n",
    "    if isinstance(msg, AIMessage) and not msg.tool_calls:\n",
    "        print(f\"  -> {msg.content}\")\n",
    "        parallel_answered = True\n",
    "        break\n",
    "assert parallel_answered, \"並列ツール呼び出し後の最終応答が見つかりませんでした。\"\n",
    "print(\"\\nアサーション成功（少なくとも1つのツール呼び出しと最終応答を確認）。\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題006: シンプルな Plan-and-Execute エージェントの構築\n",
    "\n",
    "### 課題\n",
    "より複雑なタスクでは、まず計画（Plan）を立て、その計画に従ってステップを実行（Execute）するアプローチが有効です。この問題では、非常にシンプルなPlan-and-Execute型のエージェントを構築します。具体的には、以下の2つの主要ノードを持つグラフを作成します。\n",
    "1.  **Plannerノード:** ユーザーの要求を受け取り、それを達成するためのステップのリスト（計画）を生成します（LLMを使用）。\n",
    "2.  **Executorノード:** 計画の各ステップを順番に実行します（ここではダミーの実行とし、LLMは使わずにステップ内容をログ出力する程度）。全てのステップが完了したら終了します。\n",
    "\n",
    "*   **学習内容:** 状態に「計画リスト」と「現在のステップ番号」を保持し、条件分岐を使って計画のステップを一つずつ実行していくループ構造を構築する方法を学びます。LLMを計画生成と実行（ここでは簡易的）の異なる役割で使う概念を理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class PlanExecuteState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    user_request: str\n",
    "    plan: Optional[List[str]] # ステップのリスト\n",
    "    current_step_index: int\n",
    "    execution_log: List[str] # 各ステップの実行ログ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def planner_node(state: PlanExecuteState):\n",
    "    request = state[\"user_request\"]\n",
    "    print(f\"planner_node: Request '{request}' の計画を生成中...\")\n",
    "    # 実際にはLLMで計画生成。ここではダミープラン。\n",
    "    # plan_str = llm.invoke(f\"「{request}」を達成するためのステップを箇条書きで3つ提案してください。\").content\n",
    "    # generated_plan = [s.strip() for s in plan_str.split('\\n') if s.strip() and s.startswith('- ')]\n",
    "    # generated_plan = [s[2:] for s in generated_plan] # '- ' を除去\n",
    "    generated_plan = [f\"ステップ1: {request}の準備\", f\"ステップ2: {request}の実行\", f\"ステップ3: {request}の確認\"]\n",
    "    return {\"plan\": generated_plan, \"current_step_index\": 0, \"execution_log\": [], \"messages\": [AIMessage(content=f\"計画生成完了: {generated_plan}\")]}\n",
    "\n",
    "def executor_node(state: PlanExecuteState):\n",
    "    plan = state[\"plan\"]\n",
    "    idx = state[\"current_step_index\"]\n",
    "    step_description = plan[idx]\n",
    "    log_entry = f\"実行中 (ステップ {idx + 1}/{len(plan)}): {step_description}\"\n",
    "    print(f\"executor_node: {log_entry}\")\n",
    "    current_log = state.get(\"execution_log\", [])\n",
    "    return {\"current_step_index\": idx + 1, \"execution_log\": current_log + [log_entry], \"messages\": [AIMessage(content=log_entry)]}\n",
    "\n",
    "# --- ルーター関数 ---\n",
    "def plan_router(state: PlanExecuteState):\n",
    "    if state[\"plan\"] and state[\"current_step_index\"] < len(state[\"plan\"]):\n",
    "        return \"execute_next_step\"\n",
    "    return END # 全ステップ完了\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(PlanExecuteState)\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"executor\", executor_node)\n",
    "\n",
    "workflow.set_entry_point(\"planner\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"planner\", # プランナーの後、最初の実行ステップへ（またはプランがなければ終了）\n",
    "    plan_router, # プランがあるかチェック\n",
    "    {\"execute_next_step\": \"executor\", END: END}\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"executor\", # 各ステップ実行後、次のステップへ（または全ステップ完了なら終了）\n",
    "    plan_router, \n",
    "    {\"execute_next_step\": \"executor\", END: END} # ループ\n",
    ")\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- 実行 --- \n",
    "request = \"美味しいコーヒーを淹れる\"\n",
    "inputs = {\"messages\": [HumanMessage(content=request)], \"user_request\": request}\n",
    "for event in graph.stream(inputs, {\"recursion_limit\": 10}): print(event)\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class PlanExecuteState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages] # 主にデバッグや最終報告用\n",
    "    user_request: str                           # ユーザーからの元のリクエスト\n",
    "    plan: Optional[List[str]]                   # LLMが生成したステップのリスト\n",
    "    current_step_index: int                     # 現在実行中の計画ステップのインデックス\n",
    "    execution_log: List[str]                    # 各ステップの実行結果（またはログ）\n",
    "    final_summary: Optional[str]                # 全ステップ実行後の最終サマリー\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def initialize_plan_state_node(state: PlanExecuteState):\n",
    "    user_req = state[\"messages\"][-1].content\n",
    "    print(f\"initialize_plan_state_node: ユーザーリクエスト '{user_req}' を設定。\")\n",
    "    return {\n",
    "        \"user_request\": user_req,\n",
    "        \"plan\": None,\n",
    "        \"current_step_index\": 0,\n",
    "        \"execution_log\": [],\n",
    "        \"final_summary\": None\n",
    "    }\n",
    "\n",
    "def planner_node(state: PlanExecuteState):\n",
    "    request = state[\"user_request\"]\n",
    "    print(f\"planner_node: リクエスト '{request}' に対する計画を生成中...\")\n",
    "    \n",
    "    # LLMを使って計画を生成\n",
    "    planner_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"あなたは優秀なプランナーです。ユーザーの複雑なリクエストを、実行可能なステップに分解してください。各ステップは簡潔に記述し、番号付きリストではなく、各行が1ステップとなるようにしてください。\"),\n",
    "        HumanMessage(content=\"リクエスト: {user_request}\\n\\n計画ステップ:\")\n",
    "    ])\n",
    "    planner_chain = planner_prompt | llm\n",
    "    response = planner_chain.invoke({\"user_request\": request})\n",
    "    generated_plan_str = response.content.strip()\n",
    "    \n",
    "    # 生成された計画文字列をリストに変換\n",
    "    # (例: \"ステップ1\\nステップ2\" -> [\"ステップ1\", \"ステップ2\"])\n",
    "    plan_steps = [step.strip() for step in generated_plan_str.split('\\n') if step.strip()]\n",
    "    \n",
    "    if not plan_steps: # もしLLMが空の計画を返したら、ダミープランを設定\n",
    "        print(\"  -> LLMが空の計画を返したため、ダミープランを使用します。\")\n",
    "        plan_steps = [f\"'{request}'の準備作業\", f\"'{request}'の主要作業\", f\"'{request}'の完了確認\"]\n",
    "        \n",
    "    print(f\"  -> 生成された計画: {plan_steps}\")\n",
    "    return {\"plan\": plan_steps, \"messages\": [AIMessage(content=f\"生成された計画: {plan_steps}\")]}\n",
    "\n",
    "def executor_node(state: PlanExecuteState):\n",
    "    plan = state.get(\"plan\")\n",
    "    if not plan: # プランがなければ実行できない\n",
    "        print(\"executor_node: 実行すべき計画がありません。\")\n",
    "        return {\"execution_log\": state.get(\"execution_log\", []) + [\"エラー: 計画なし\"], \"messages\": [AIMessage(content=\"エラー: 計画がありませんでした。\")]}\n",
    "        \n",
    "    idx = state.get(\"current_step_index\", 0)\n",
    "    if idx >= len(plan):\n",
    "        print(\"executor_node: 全ての計画ステップが完了済みです。\")\n",
    "        return {}\n",
    "\n",
    "    step_description = plan[idx]\n",
    "    log_entry = f\"実行完了 (ステップ {idx + 1}/{len(plan)}): {step_description}\"\n",
    "    print(f\"executor_node: {log_entry}\")\n",
    "    \n",
    "    # ここで実際にステップに対応するアクションを実行する (例: ツール呼び出し、別のLLM呼び出しなど)\n",
    "    # この問題では、ログに記録するのみとする\n",
    "    time.sleep(0.5) # ダミーの処理時間\n",
    "    \n",
    "    updated_log = state.get(\"execution_log\", []) + [f\"[成功] {step_description}\"]\n",
    "    return {\"current_step_index\": idx + 1, \"execution_log\": updated_log, \"messages\": [AIMessage(content=f\"ステップ実行ログ: {log_entry}\")]}\n",
    "\n",
    "def summarize_execution_node(state: PlanExecuteState):\n",
    "    log = state.get(\"execution_log\", [])\n",
    "    summary = f\"計画「{state.get('user_request')}」の全 {len(log)} ステップが完了しました。\\n実行ログ:\\n\" + \"\\n\".join(log)\n",
    "    print(f\"summarize_execution_node: {summary}\")\n",
    "    return {\"final_summary\": summary, \"messages\": [AIMessage(content=summary)]}\n",
    "\n",
    "# --- ルーター関数 ---\n",
    "def route_after_planning(state: PlanExecuteState):\n",
    "    print(f\"\\nroute_after_planning: 生成された計画 -> {state.get('plan')}\")\n",
    "    if state.get(\"plan\") and len(state.get(\"plan\", [])) > 0:\n",
    "        print(\"  -> 計画あり。executorへ\")\n",
    "        return \"start_execution\"\n",
    "    print(\"  -> 計画なしまたは空。ENDへ\")\n",
    "    return END # プランがなければ終了\n",
    "\n",
    "def route_after_step_execution(state: PlanExecuteState):\n",
    "    current_idx = state.get(\"current_step_index\", 0)\n",
    "    total_steps = len(state.get(\"plan\", []))\n",
    "    print(f\"\\nroute_after_step_execution: 現在ステップ {current_idx}/{total_steps}\")\n",
    "    if current_idx < total_steps:\n",
    "        print(\"  -> 次のステップあり。executorへループ\")\n",
    "        return \"execute_next_step\"\n",
    "    print(\"  -> 全ステップ完了。summarizerへ\")\n",
    "    return \"all_steps_completed\"\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(PlanExecuteState)\n",
    "\n",
    "workflow.add_node(\"initializer\", initialize_plan_state_node)\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"executor\", executor_node)\n",
    "workflow.add_node(\"summarizer\", summarize_execution_node)\n",
    "\n",
    "workflow.set_entry_point(\"initializer\")\n",
    "workflow.add_edge(\"initializer\", \"planner\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"planner\",\n",
    "    route_after_planning,\n",
    "    {\"start_execution\": \"executor\", END: END}\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"executor\",\n",
    "    route_after_step_execution,\n",
    "    {\"execute_next_step\": \"executor\", \"all_steps_completed\": \"summarizer\"} # ループ\n",
    ")\n",
    "workflow.add_edge(\"summarizer\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- 実行と結果確認 ---\n",
    "user_task_request = \"美味しいラーメンを作る\"\n",
    "print(f\"\\n--- Plan-and-Executeテスト (リクエスト: '{user_task_request}') ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=user_task_request)]}\n",
    "\n",
    "print(\"\\nストリーム出力:\")\n",
    "for i, event in enumerate(graph.stream(inputs, {\"recursion_limit\": 15})): # ステップ数に応じて調整\n",
    "    print(f\"Event {i+1}: {event}\")\n",
    "\n",
    "final_state_plan_exec = graph.invoke(inputs, {\"recursion_limit\": 15})\n",
    "print(\"\\n最終状態の確認:\")\n",
    "print(f\"  ユーザーリクエスト: {final_state_plan_exec['user_request']}\")\n",
    "print(f\"  生成された計画: {final_state_plan_exec['plan']}\")\n",
    "print(f\"  実行ログの行数: {len(final_state_plan_exec['execution_log'])}\")\n",
    "print(f\"  最終サマリー: {final_state_plan_exec['final_summary']}\")\n",
    "assert final_state_plan_exec['plan'] is not None and len(final_state_plan_exec['plan']) > 0\n",
    "assert len(final_state_plan_exec['execution_log']) == len(final_state_plan_exec['plan'])\n",
    "assert final_state_plan_exec['final_summary'] is not None\n",
    "print(\"\\nアサーション成功！\")\n",
    "``````\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
