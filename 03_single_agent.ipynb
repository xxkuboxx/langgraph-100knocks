{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3章: ツールを使うシングルエージェント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "以下のセルを順番に実行して、演習に必要な環境をセットアップします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "\n",
    "このセルは、LangGraphおよび関連するLangChainライブラリをインストールします。実行には数分かかる場合があります。\n",
    "ご利用になるLLMプロバイダーに応じて、コメントアウトを解除して必要なライブラリをインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ライブラリのインストール ===\n",
    "# ご利用になるLLMプロバイダーに応じて、以下のコメントを解除して実行してください。\n",
    "# !%pip install -qU langchain langgraph langchain_openai langchain_google_vertexai langchain_google_genai langchain_anthropic langchain_aws boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMプロバイダーの選択\n",
    "\n",
    "このセルでは、使用するLLMプロバイダーを選択します。\n",
    "`LLM_PROVIDER` 変数に、利用したいプロバイダー名を設定してください。\n",
    "選択可能なプロバイダー: `\"openai\"`, `\"azure\"`, `\"google\"` (Vertex AI), `\"google_genai\"` (Gemini API), `\"anthropic\"`, `\"bedrock\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMプロバイダーの選択 ===\n",
    "# 利用したいLLMプロバイダーを以下の変数で指定してください。\n",
    "# \"openai\", \"azure\", \"google\" (Vertex AI), \"google_genai\" (Gemini API), \"anthropic\", \"bedrock\" のいずれかを選択できます。\n",
    "LLM_PROVIDER = \"openai\"  # 例: OpenAI を利用する場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIキー/環境変数の設定\n",
    "\n",
    "以下のセルを実行する前に、選択したLLMプロバイダーに応じたAPIキーまたは環境変数を設定する必要があります。\n",
    "\n",
    "**一般的な手順:**\n",
    "1.  `.env.sample` ファイルをコピーして `.env` ファイルを作成します。\n",
    "2.  `.env` ファイルを開き、選択したLLMプロバイダーに対応するAPIキーや必要な情報を記述します。\n",
    "    *   **OpenAI:** `OPENAI_API_KEY`\n",
    "    *   **Azure OpenAI:** `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `OPENAI_API_VERSION`, `AZURE_OPENAI_DEPLOYMENT_NAME`\n",
    "    *   **Google (Vertex AI):** `GOOGLE_CLOUD_PROJECT_ID`, `GOOGLE_CLOUD_LOCATION` (Colab環境外で実行する場合、`GOOGLE_APPLICATION_CREDENTIALS` 環境変数の設定も必要になることがあります)\n",
    "    *   **Google (Gemini API):** `GOOGLE_API_KEY`\n",
    "    *   **Anthropic:** `ANTHROPIC_API_KEY`\n",
    "    *   **AWS Bedrock:** `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION_NAME` (IAMロールを使用する場合は、これらのキー設定は不要な場合がありますが、リージョン名は必須です)\n",
    "3.  ファイルを保存します。\n",
    "\n",
    "**Google Colab を使用している場合:**\n",
    "上記の `.env` ファイルを使用する代わりに、Colabのシークレットマネージャーに必要なキーを登録してください。\n",
    "例えば、OpenAIを使用する場合は `OPENAI_API_KEY` という名前でシークレットを登録します。\n",
    "Vertex AI を利用する場合は、Colab上での認証 (`google.colab.auth.authenticate_user()`) が実行されます。\n",
    "\n",
    "このセルは、設定された情報に基づいて環境変数をロードし、LLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === APIキー/環境変数の設定 ===\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envファイルから環境変数を読み込む (存在する場合)\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# --- OpenAI ---\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY and IS_COLAB:\n",
    "        OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"OpenAI APIキーが設定されていません。環境変数 OPENAI_API_KEY を設定するか、Colab環境の場合はシークレットに OPENAI_API_KEY を設定してください。\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# --- Azure OpenAI ---\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not AZURE_OPENAI_API_KEY: AZURE_OPENAI_API_KEY = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not AZURE_OPENAI_ENDPOINT: AZURE_OPENAI_ENDPOINT = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        if not OPENAI_API_VERSION: OPENAI_API_VERSION = userdata.get(\"OPENAI_API_VERSION\") # 例: \"2023-07-01-preview\"\n",
    "        if not AZURE_OPENAI_DEPLOYMENT_NAME: AZURE_OPENAI_DEPLOYMENT_NAME = userdata.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if not AZURE_OPENAI_API_KEY: raise ValueError(\"Azure OpenAI APIキー (AZURE_OPENAI_API_KEY) が設定されていません。\")\n",
    "    if not AZURE_OPENAI_ENDPOINT: raise ValueError(\"Azure OpenAI エンドポイント (AZURE_OPENAI_ENDPOINT) が設定されていません。\")\n",
    "    if not OPENAI_API_VERSION: OPENAI_API_VERSION = \"2023-07-01-preview\" # デフォルトを設定することも可能\n",
    "    if not AZURE_OPENAI_DEPLOYMENT_NAME: raise ValueError(\"Azure OpenAI デプロイメント名 (AZURE_OPENAI_DEPLOYMENT_NAME) が設定されていません。\")\n",
    "\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "    os.environ[\"OPENAI_API_VERSION\"] = OPENAI_API_VERSION\n",
    "\n",
    "# --- Google Cloud Vertex AI (Gemini) ---\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT_ID\") # .env 用に修正\n",
    "    LOCATION = os.environ.get(\"GOOGLE_CLOUD_LOCATION\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not PROJECT_ID: PROJECT_ID = userdata.get(\"GOOGLE_CLOUD_PROJECT_ID\")\n",
    "        if not LOCATION: LOCATION = userdata.get(\"GOOGLE_CLOUD_LOCATION\") # 例: \"us-central1\"\n",
    "        from google.colab import auth as google_auth\n",
    "        google_auth.authenticate_user() # Vertex AI を使う場合は Colab での認証を推奨\n",
    "    else: # Colab外の場合、.envから読み込んだ値で環境変数を設定\n",
    "        if PROJECT_ID: os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID # Vertex AI SDKが参照する標準的な環境変数名\n",
    "        if LOCATION: os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "\n",
    "    if not PROJECT_ID: raise ValueError(\"Google Cloud Project ID が設定されていません。環境変数 GOOGLE_CLOUD_PROJECT_ID を設定するか、Colab環境の場合はシークレットに GOOGLE_CLOUD_PROJECT_ID を設定してください。\")\n",
    "    if not LOCATION: LOCATION = \"us-central1\" # デフォルトロケーション\n",
    "\n",
    "# --- Google Gemini API (langchain-google-genai) ---\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY and IS_COLAB:\n",
    "        GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"Google APIキーが設定されていません。環境変数 GOOGLE_API_KEY を設定するか、Colab環境の場合はシークレットに GOOGLE_API_KEY を設定してください。\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# --- Anthropic (Claude) ---\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY and IS_COLAB:\n",
    "        ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        raise ValueError(\"Anthropic APIキーが設定されていません。環境変数 ANTHROPIC_API_KEY を設定するか、Colab環境の場合はシークレットに ANTHROPIC_API_KEY を設定してください。\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "# --- Amazon Bedrock (Claude) ---\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    AWS_REGION_NAME = os.environ.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if IS_COLAB: \n",
    "        if not AWS_ACCESS_KEY_ID: AWS_ACCESS_KEY_ID = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
    "        if not AWS_SECRET_ACCESS_KEY: AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        if not AWS_REGION_NAME: AWS_REGION_NAME = userdata.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if not AWS_REGION_NAME:\n",
    "         raise ValueError(\"AWSリージョン名 (AWS_REGION_NAME) が設定されていません。Bedrock利用にはリージョン指定が必要です。\")\n",
    "\n",
    "    # 環境変数に設定 (boto3がこれらを自動で読み込む)\n",
    "    if AWS_ACCESS_KEY_ID: os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "    if AWS_SECRET_ACCESS_KEY: os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION_NAME # boto3が参照する標準的なリージョン環境変数名\n",
    "    os.environ[\"AWS_REGION\"] = AWS_REGION_NAME # いくつかのライブラリはこちらを参照することもある\n",
    "\n",
    "print(f\"APIキー/環境変数の設定完了 (プロバイダー: {LLM_PROVIDER})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMクライアントの初期化\n",
    "\n",
    "このセルは、上で選択・設定したLLMプロバイダーに基づいて、対応するLLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMクライアントの動的初期化 ===\n",
    "llm = None\n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), # 環境変数から取得\n",
    "        openai_api_version=os.environ.get(\"OPENAI_API_VERSION\"), # 環境変数から取得\n",
    "        temperature=0,\n",
    "    )\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    from langchain_google_vertexai import ChatVertexAI\n",
    "    # PROJECT_ID, LOCATION は前のセルで環境変数に設定済みか、Colabの場合は直接利用\n",
    "    llm = ChatVertexAI(model_name=\"gemini-1.5-flash-001\", temperature=0, project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"), location=os.environ.get(\"GOOGLE_CLOUD_LOCATION\"))\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0, convert_system_message_to_human=True)\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    from langchain_anthropic import ChatAnthropic\n",
    "    llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    from langchain_aws import ChatBedrock # langchain_community.chat_models から langchain_aws に変更の可能性あり\n",
    "    # AWS_REGION_NAME は前のセルで環境変数 AWS_DEFAULT_REGION に設定済み\n",
    "    llm = ChatBedrock( # BedrockChat ではなく ChatBedrock が一般的\n",
    "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        # region_name=os.environ.get(\"AWS_DEFAULT_REGION\"), # 通常、boto3が環境変数から自動で読み込む\n",
    "        model_kwargs={\"temperature\": 0},\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Unsupported LLM_PROVIDER: {LLM_PROVIDER}. \"\n",
    "        \"Please choose from 'openai', 'azure', 'google', 'google_genai', 'anthropic', or 'bedrock'.\"\n",
    "    )\n",
    "\n",
    "print(f\"LLM Provider: {LLM_PROVIDER}\")\n",
    "if llm:\n",
    "    print(f\"LLM Client Type: {type(llm)}\")\n",
    "    # モデル名取得の試行を汎用的に\n",
    "    model_attr = getattr(llm, 'model', None) or \\ \n",
    "                 getattr(llm, 'model_name', None) or \\ \n",
    "                 getattr(llm, 'model_id', None) or \\ \n",
    "                 (hasattr(llm, 'llm') and getattr(llm.llm, 'model', None)) # 一部のLLMクライアントのネスト構造に対応\n",
    "    if hasattr(llm, 'azure_deployment') and not model_attr: # Azure特有の属性\n",
    "        model_attr = llm.azure_deployment\n",
    "        \n",
    "    if model_attr:\n",
    "        print(f\"LLM Model: {model_attr}\")\n",
    "    else:\n",
    "        print(\"LLM Model: (Could not determine model name from client attributes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この章では、ToolNodeの活用やPlan-and-Execute型エージェントの構築など、ツール連携を含むシングルエージェントの作成方法を学びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今後の実装に向けた空のコードセル"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
