NOTEBOOK_CONTENT = """\
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章: 発展技術と運用\\n",
    "\\n",
    "## 準備\\n",
    "\\n",
    "以下のセルを順番に実行して、演習に必要な環境をセットアップします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\\n",
    "\\n",
    "このセルは、LangGraphおよび関連するLangChainライブラリ、加えてこの章で必要となる可能性のある追加ライブラリ（例: `langsmith`、データベース操作ライブラリなど）をインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ライブラリのインストール ===\\n",
    "!pip install -qU langchain langgraph langchain_openai langchain_google_vertexai langchain_google_genai langchain_anthropic langchain_aws boto3 python-dotenv pygraphviz langchain_community tavily-python langsmith sqlalchemy psycopg2-binary aiosqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMプロバイダーおよびLangSmithの設定\\n",
    "\\n",
    "これまでの章と同様にLLMプロバイダーを選択・設定します。\\n",
    "加えて、この章ではLangSmithとの連携を扱う問題が含まれます。LangSmithを利用する場合は、以下の環境変数を設定してください。\\n",
    "- `LANGCHAIN_API_KEY`: LangSmithのAPIキー。\\n",
    "- `LANGCHAIN_TRACING_V2`: `true`に設定。\\n",
    "- `LANGCHAIN_ENDPOINT`: 通常は `https://api.smith.langchain.com`。\\n",
    "- `LANGCHAIN_PROJECT`: LangSmith上のプロジェクト名（例: `langgraph-100knocks-advanced`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMプロバイダーの選択とAPIキー/環境変数の設定 ===\\n",
    "import os\\n",
    "from dotenv import load_dotenv\\n",
    "import warnings\\n",
    "\\n",
    "load_dotenv()\\n",
    "\\n",
    "try:\\n",
    "    from google.colab import userdata\\n",
    "    IS_COLAB = True\\n",
    "except ImportError:\\n",
    "    IS_COLAB = False\\n",
    "\\n",
    "LLM_PROVIDER = \\"openai\\"  # \\"openai\\", \\"azure\\", \\"google\\", \\"google_genai\\", \\"anthropic\\", \\"bedrock\\"\\n",
    "\\n",
    "# LangSmith設定 (必要に応じて値を設定またはColab Secrets/環境変数から読み込む)\\n",
    "os.environ[\\"LANGCHAIN_TRACING_V2\\"] = os.environ.get(\\"LANGCHAIN_TRACING_V2\\", \\"false\\") # デフォルトはfalse\\n",
    "if os.environ[\\"LANGCHAIN_TRACING_V2\\"].lower() == \\"true\\":\\n",
    "    LANGCHAIN_API_KEY = os.environ.get(\\"LANGCHAIN_API_KEY\\")\\n",
    "    LANGCHAIN_ENDPOINT = os.environ.get(\\"LANGCHAIN_ENDPOINT\\", \\"https://api.smith.langchain.com\\")\\n",
    "    LANGCHAIN_PROJECT = os.environ.get(\\"LANGCHAIN_PROJECT\\", \\"langgraph-100knocks-adv\\") # デフォルトプロジェクト名\\n",
    "    if IS_COLAB:\\n",
    "        if not LANGCHAIN_API_KEY: LANGCHAIN_API_KEY = userdata.get(\\"LANGCHAIN_API_KEY\\")\\n",
    "        # ENDPOINTとPROJECTもColab Secretsから取得可能にしてもよい\\n",
    "    if not LANGCHAIN_API_KEY:\\n",
    "        warnings.warn(\\"LangSmithのAPIキー (LANGCHAIN_API_KEY) が設定されていません。LangSmith連携は無効になります。\\")\\n",
    "        os.environ[\\"LANGCHAIN_TRACING_V2\\"] = \\"false\\"\\n",
    "    else:\\n",
    "        os.environ[\\"LANGCHAIN_API_KEY\\"] = LANGCHAIN_API_KEY\\n",
    "        os.environ[\\"LANGCHAIN_ENDPOINT\\"] = LANGCHAIN_ENDPOINT\\n",
    "        os.environ[\\"LANGCHAIN_PROJECT\\"] = LANGCHAIN_PROJECT\\n",
    "        print(f\\"LangSmith連携が有効です。プロジェクト: {LANGCHAIN_PROJECT}\\")\\n",
    "else:\\n",
    "    print(\\"LangSmith連携は無効です。\\")\\n",
    "\\n",
    "# (04_multi_agent.ipynb と同じAPIキー設定ロジックをここに貼り付け)\\n",
    "# --- OpenAI ---\\n",
    "if LLM_PROVIDER == \\"openai\\":\\n",
    "    OPENAI_API_KEY = os.environ.get(\\"OPENAI_API_KEY\\")\\n",
    "    if not OPENAI_API_KEY and IS_COLAB:\\n",
    "        OPENAI_API_KEY = userdata.get(\\"OPENAI_API_KEY\\")\\n",
    "    if not OPENAI_API_KEY:\\n",
    "        warnings.warn(\\"OpenAI APIキーが設定されていません。ダミーのLLMを使用します。\\")\\n",
    "        os.environ[\\"OPENAI_API_KEY\\"] = \\"dummy_key_openai\\"\\n",
    "    else:\\n",
    "        os.environ[\\"OPENAI_API_KEY\\"] = OPENAI_API_KEY\\n",
    "# ... (Azure, Google, Anthropic, Bedrock の設定。04_multi_agent.ipynbと同様)\\n",
    "elif LLM_PROVIDER == \\"azure\\":\\n",
    "    AZURE_OPENAI_API_KEY = os.environ.get(\\"AZURE_OPENAI_API_KEY\\")\\n",
    "    AZURE_OPENAI_ENDPOINT = os.environ.get(\\"AZURE_OPENAI_ENDPOINT\\")\\n",
    "    OPENAI_API_VERSION = os.environ.get(\\"OPENAI_API_VERSION\\")\\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get(\\"AZURE_OPENAI_DEPLOYMENT_NAME\\")\\n",
    "    if IS_COLAB:\\n",
    "        if not AZURE_OPENAI_API_KEY: AZURE_OPENAI_API_KEY = userdata.get(\\"AZURE_OPENAI_API_KEY\\")\\n",
    "        if not AZURE_OPENAI_ENDPOINT: AZURE_OPENAI_ENDPOINT = userdata.get(\\"AZURE_OPENAI_ENDPOINT\\")\\n",
    "        if not OPENAI_API_VERSION: OPENAI_API_VERSION = userdata.get(\\"OPENAI_API_VERSION\\")\\n",
    "        if not AZURE_OPENAI_DEPLOYMENT_NAME: AZURE_OPENAI_DEPLOYMENT_NAME = userdata.get(\\"AZURE_OPENAI_DEPLOYMENT_NAME\\")\\n",
    "    if not (AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT and OPENAI_API_VERSION and AZURE_OPENAI_DEPLOYMENT_NAME):\\n",
    "        warnings.warn(\\"Azure OpenAI の必要な情報が不足しています。ダミーのLLMを使用します。\\")\\n",
    "        LLM_PROVIDER = \\"openai\\" \\n",
    "        os.environ[\\"OPENAI_API_KEY\\"] = \\"dummy_key_azure\\"\\n",
    "    else:\\n",
    "        os.environ[\\"AZURE_OPENAI_API_KEY\\"] = AZURE_OPENAI_API_KEY\\n",
    "        os.environ[\\"AZURE_OPENAI_ENDPOINT\\"] = AZURE_OPENAI_ENDPOINT\\n",
    "        os.environ[\\"OPENAI_API_VERSION\\"] = OPENAI_API_VERSION\\n",
    "elif LLM_PROVIDER == \\"google\\":\\n",
    "    PROJECT_ID = os.environ.get(\\"GOOGLE_CLOUD_PROJECT_ID\\")\\n",
    "    LOCATION = os.environ.get(\\"GOOGLE_CLOUD_LOCATION\\")\\n",
    "    if IS_COLAB:\\n",
    "        if not PROJECT_ID: PROJECT_ID = userdata.get(\\"GOOGLE_CLOUD_PROJECT_ID\\")\\n",
    "        if not LOCATION: LOCATION = userdata.get(\\"GOOGLE_CLOUD_LOCATION\\")\\n",
    "        try:\\n",
    "            from google.colab import auth as google_auth\\n",
    "            google_auth.authenticate_user()\\n",
    "        except ImportError:\\n",
    "            warnings.warn(\\"Google Colab環境での認証に失敗しました。\\")\\n",
    "    if not (PROJECT_ID and LOCATION):\\n",
    "        warnings.warn(\\"Google Vertex AI のProject IDまたはLocationが設定されていません。ダミーのLLMを使用します。\\")\\n",
    "        LLM_PROVIDER = \\"openai\\"\\n",
    "        os.environ[\\"OPENAI_API_KEY\\"] = \\"dummy_key_google_vertex\\"\\n",
    "    else:\\n",
    "        os.environ[\\"GOOGLE_CLOUD_PROJECT\\"] = PROJECT_ID\\n",
    "        os.environ[\\"GOOGLE_CLOUD_LOCATION\\"] = LOCATION\\n",
    "elif LLM_PROVIDER == \\"google_genai\\":\\n",
    "    GOOGLE_API_KEY = os.environ.get(\\"GOOGLE_API_KEY\\")\\n",
    "    if not GOOGLE_API_KEY and IS_COLAB:\\n",
    "        GOOGLE_API_KEY = userdata.get(\\"GOOGLE_API_KEY\\")\\n",
    "    if not GOOGLE_API_KEY:\\n",
    "        warnings.warn(\\"Google APIキーが設定されていません。ダミーのLLMを使用します。\\")\\n",
    "        LLM_PROVIDER = \\"openai\\"\\n",
    "        os.environ[\\"OPENAI_API_KEY\\"] = \\"dummy_key_google_genai\\"\\n",
    "    else:\\n",
    "        os.environ[\\"GOOGLE_API_KEY\\"] = GOOGLE_API_KEY\\n",
    "elif LLM_PROVIDER == \\"anthropic\\":\\n",
    "    ANTHROPIC_API_KEY = os.environ.get(\\"ANTHROPIC_API_KEY\\")\\n",
    "    if not ANTHROPIC_API_KEY and IS_COLAB:\\n",
    "        ANTHROPIC_API_KEY = userdata.get(\\"ANTHROPIC_API_KEY\\")\\n",
    "    if not ANTHROPIC_API_KEY:\\n",
    "        warnings.warn(\\"Anthropic APIキーが設定されていません。ダミーのLLMを使用します。\\")\\n",
    "        LLM_PROVIDER = \\"openai\\"\\n",
    "        os.environ[\\"OPENAI_API_KEY\\"] = \\"dummy_key_anthropic\\"\\n",
    "    else:\\n",
    "        os.environ[\\"ANTHROPIC_API_KEY\\"] = ANTHROPIC_API_KEY\\n",
    "elif LLM_PROVIDER == \\"bedrock\\":\\n",
    "    AWS_REGION_NAME = os.environ.get(\\"AWS_REGION_NAME\\")\\n",
    "    if IS_COLAB:\\n",
    "        if not AWS_REGION_NAME: AWS_REGION_NAME = userdata.get(\\"AWS_REGION_NAME\\")\\n",
    "    if not AWS_REGION_NAME:\\n",
    "        warnings.warn(\\"AWSリージョン名 (AWS_REGION_NAME) が設定されていません。ダミーのLLMを使用します。\\")\\n",
    "        LLM_PROVIDER = \\"openai\\"\\n",
    "        os.environ[\\"OPENAI_API_KEY\\"] = \\"dummy_key_bedrock\\"\\n",
    "    else:\\n",
    "        os.environ[\\"AWS_DEFAULT_REGION\\"] = AWS_REGION_NAME\\n",
    "\\n",
    "print(f\\"APIキー/環境変数の設定完了 (プロバイダー: {LLM_PROVIDER})\\")\\n",
    "\\n",
    "# === LLMクライアントの初期化 ===\\n",
    "llm = None\\n",
    "try:\\n",
    "    if LLM_PROVIDER == \\"openai\\":\\n",
    "        from langchain_openai import ChatOpenAI\\n",
    "        llm = ChatOpenAI(model=\\"gpt-4o-mini\\", temperature=0)\\n",
    "    elif LLM_PROVIDER == \\"azure\\":\\n",
    "        from langchain_openai import AzureChatOpenAI\\n",
    "        llm = AzureChatOpenAI(\\n",
    "            azure_deployment=os.environ.get(\\"AZURE_OPENAI_DEPLOYMENT_NAME\\"),\\n",
    "            openai_api_version=os.environ.get(\\"OPENAI_API_VERSION\\"),\\n",
    "            temperature=0,\\n",
    "        )\\n",
    "    elif LLM_PROVIDER == \\"google\\":\\n",
    "        from langchain_google_vertexai import ChatVertexAI\\n",
    "        llm = ChatVertexAI(model_name=\\"gemini-1.5-flash-001\\", temperature=0, project=os.environ.get(\\"GOOGLE_CLOUD_PROJECT\\"), location=os.environ.get(\\"GOOGLE_CLOUD_LOCATION\\"))\\n",
    "    elif LLM_PROVIDER == \\"google_genai\\":\\n",
    "        from langchain_google_genai import ChatGoogleGenerativeAI\\n",
    "        llm = ChatGoogleGenerativeAI(model=\\"gemini-1.5-flash-001\\", temperature=0)\\n",
    "    elif LLM_PROVIDER == \\"anthropic\\":\\n",
    "        from langchain_anthropic import ChatAnthropic\\n",
    "        llm = ChatAnthropic(model=\\"claude-3-haiku-20240307\\", temperature=0)\\n",
    "    elif LLM_PROVIDER == \\"bedrock\\":\\n",
    "        from langchain_aws import ChatBedrock\\n",
    "        llm = ChatBedrock(\\n",
    "            model_id=\\"anthropic.claude-3-haiku-20240307-v1:0\\",\\n",
    "            model_kwargs={\\"temperature\\": 0},\\n",
    "        )\\n",
    "except Exception as e:\\n",
    "    warnings.warn(f\\"選択されたLLMプロバイダー ({LLM_PROVIDER}) のクライアント初期化に失敗しました: {e}。代わりにFakeLLMを使用します。\\")\\n",
    "    LLM_PROVIDER = \\"fake\\"\\n",
    "\\n",
    "if LLM_PROVIDER == \\"fake\\" or llm is None:\\n",
    "    from langchain_core.language_models.fake_models import FakeMessagesListChatModel\\n",
    "    from langchain_core.messages import AIMessage\\n",
    "    class CustomFakeChatModel(FakeMessagesListChatModel):\\n",
    "        async def _agenerate(self, messages, stop=None, run_manager=None, **kwargs):\\n",
    "            # 非同期版も同期版と同じように動作させる (FakeLLMなので)\\n",
    "            return self._generate(messages, stop=stop, run_manager=run_manager, **kwargs)\\n",
    "        def _generate(self, messages, stop=None, run_manager=None, **kwargs):\\n",
    "            # FakeMessagesListChatModelは通常、応答のリストを順番に消費する\\n",
    "            # ここでは、常にリストの最初の応答を返すようにして、ストリーミングのテストをしやすくする\\n",
    "            # 実際のストリーミングLLMはトークンを逐次生成する\\n",
    "            from langchain_core.outputs import ChatGeneration, ChatResult\\n",
    "            response_message = AIMessage(content=self.responses[0]) \\n",
    "            return ChatResult(generations=[ChatGeneration(message=response_message)])\\n",
    "\\n",
    "    llm = CustomFakeChatModel(responses=[\\n",
    "        \\"これはFakeLLMからのストリーミング風応答の断片1です。\\",\\n",
    "        \\"これはFakeLLMからのストリーミング風応答の断片2です。続き。\\",\\n",
    "        \\"そしてこれが最後の部分です。FakeLLMより。\\"\\n",
    "    ])\\n",
    "    print(\\"CustomFakeChatModelクライアントが初期化されました。\\")\\n",
    "\\n",
    "if llm:\\n",
    "    print(f\\"LLMクライアント ({type(llm).__name__}) の初期化が完了しました。プロバイダー: {LLM_PROVIDER}\\")\\n",
    "else:\\n",
    "    warnings.warn(\\"LLMクライアントが初期化されませんでした。演習が正しく動作しない可能性があります。\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題001: グラフ実行のストリーミング出力（状態のリアルタイム取得）\\n",
    "\\n",
    "LangGraphのグラフ実行は、`invoke()`で最終結果を一度に受け取るだけでなく、`stream()`メソッドを使うことで、各ノードの実行が完了するたびにその時点でのグラフの状態（またはその差分）をリアルタイムで受け取ることができます。これにより、処理の進行状況をユーザーに逐次表示したり、特定の状態変化に応じて追加の処理をトリガーしたりすることが可能になります。\\n",
    "\\n",
    "*   **学習内容:**\\n",
    "    *   `graph.stream()`メソッドの基本的な使い方。\\n",
    "    *   ストリームから返されるイベントの構造（どのノードが完了し、その結果どのような状態変化があったか）。\\n",
    "    *   ストリーミングされた各状態（または状態差分）を処理し、進行状況を表示する方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001 - グラフ構築\\n",
    "from typing import TypedDict, Annotated, List, Optional\\n",
    "import time\\n",
    "from langgraph.graph import StateGraph, END\\n",
    "from langgraph.graph.message import add_messages\\n",
    "from langchain_core.messages import HumanMessage, AIMessage # HumanMessageは直接使わないが、概念として (解答例より)\\n",
    "from uuid import uuid4 # 解答例より\\n",
    "\\n",
    "# --- 1. 状態定義 ---\\n",
    "class StreamingState(TypedDict):\\n",
    "    messages: Annotated[List[AIMessage], add_messages] # AIMessageのみを想定 (解答例より)\\n",
    "    current_step_name: Optional[str]\\n",
    "    step_outputs: List[str]\\n",
    "\\n",
    "# --- 2. ノード定義 ---\\n",
    "def first_step_node(state: StreamingState) -> dict:\\n",
    "    print(\\"\\n[Step 1: 開始処理]\\")\\n",
    "    time.sleep(0.2) # 処理時間を模倣 (解答例より時間変更)\\n",
    "    output = \\"最初のステップが完了しました。データAを生成。\\" # 解答例よりメッセージ変更\\n",
    "    print(f\\"  -> 出力: {output}\\")\\n",
    "    return {\\"current_step_name\\": \\"Step1_Done\\", \\"step_outputs\\": state.get(\\"step_outputs\\", []) + [output]} # 解答例より state.get 追加\\n",
    "\\n",
    "def second_step_node(state: StreamingState) -> dict:\\n",
    "    print(\\"\\n[Step 2: 中間処理]\\")\\n",
    "    time.sleep(0.3) # 解答例より時間変更\\n",
    "    prev_output = state[\\"step_outputs\\"][-1] if state.get(\\"step_outputs\\") else \\"(前の出力なし)\\" # 解答例より state.get 追加\\n",
    "    output = f\\"2番目のステップ完了。データBを生成 (前の出力: 「{prev_output}」)。\\" # 解答例よりメッセージ変更\\n",
    "    print(f\\"  -> 出力: {output}\\")\\n",
    "    return {\\"current_step_name\\": \\"Step2_Done\\", \\"step_outputs\\": state.get(\\"step_outputs\\", []) + [output]} # 解答例より state.get 追加\\n",
    "\\n",
    "def final_step_node(state: StreamingState) -> dict:\\n",
    "    print(\\"\\n[Step 3: 最終処理]\\")\\n",
    "    time.sleep(0.1) # 解答例より時間変更\\n",
    "    output = \\"全てのステップが完了し、最終結果「XYZ」が得られました。\\" # 解答例よりメッセージ変更\\n",
    "    print(f\\"  -> 出力: {output}\\")\\n",
    "    return {\\n",
    "        \\"current_step_name\\": \\"Final_Done\\", \\n",
    "        \\"step_outputs\\": state.get(\\"step_outputs\\", []) + [output], \\n",
    "        \\"messages\\": [AIMessage(content=output)] # 最終結果をmessagesにも記録 (解答例より)\\n",
    "    }\\n",
    "\\n",
    "# --- 3. グラフ構築 ---\\n",
    "workflow_q1_ch5 = StateGraph(StreamingState)\\n",
    "workflow_q1_ch5.add_node(\\"step1\\", first_step_node)\\n",
    "workflow_q1_ch5.add_node(\\"step2\\", second_step_node)\\n",
    "workflow_q1_ch5.add_node(\\"final_step\\", final_step_node)\\n",
    "\\n",
    "workflow_q1_ch5.set_entry_point(\\"step1\\")\\n",
    "workflow_q1_ch5.add_edge(\\"step1\\", \\"step2\\") # 解答例より\\n",
    "workflow_q1_ch5.add_edge(\\"step2\\", \\"final_step\\") # 解答例より\\n",
    "workflow_q1_ch5.add_edge(\\"final_step\\", END) # 解答例より\\n",
    "\\n",
    "graph_q1_ch5 = workflow_q1_ch5.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001 - グラフ可視化\\n",
    "from IPython.display import Image, display # 解答例より\\n",
    "\\n",
    "try:\\n",
    "    display(Image(graph_q1_ch5.get_graph().draw_png()))\\n",
    "except Exception as e:\\n",
    "    print(f\\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001 - グラフ実行\\n",
    "print(\\"--- グラフ実行のストリーミング開始 ---\\")\\n",
    "initial_input_q1 = {\\n",
    "    \\"messages\\": [], \\n",
    "    \\"step_outputs\\": [] \\n",
    "}\\n",
    "thread_id_q1_ch5 = f\\"streaming-test-{uuid4()[:4]}\\" # 解答例より\\n",
    "config_q1_ch5 = {\\"configurable\\": {\\"thread_id\\": thread_id_q1_ch5}} # 解答例より\\n",
    "\\n",
    "for i, event_chunk in enumerate(graph_q1_ch5.stream(initial_input_q1, config=config_q1_ch5, recursion_limit=10)): # graph_q1_ch5 に修正、config追加 (解答例より)\\n",
    "    print(f\\"\\n--- イベント {i+1} --- \\")\\n",
    "    for node_name, state_update_dict in event_chunk.items(): # 解答例より変数名変更\\n",
    "        print(f\\"  ノード「{node_name}」が完了しました。\\")\\n",
    "        if isinstance(state_update_dict, dict): # 解答例より\\n",
    "            print(f\\"  状態更新の内容:\\")\\n",
    "            for key, value in state_update_dict.items(): # 解答例より\\n",
    "                print(f\\"    {key}: {value}\\")\\n",
    "        else: \\n",
    "            print(f\\"  イベントデータ (ENDなど): {state_update_dict}\\") # 解答例より\\n",
    "print(\\"\\n--- ストリーミング終了 --- \\")\\n",
    "\\n",
    "final_state_q1 = graph_q1_ch5.get_state(config=config_q1_ch5) # 解答例より config 追加\\n",
    "if final_state_q1:\\n",
    "    print(\\"\\n最終的なグラフの状態:\\")\\n",
    "    print(f\\"  最終ステップ名: {final_state_q1.values.get('current_step_name')}\\")\\n",
    "    print(f\\"  全ステップ出力: {final_state_q1.values.get('step_outputs')}\\")\\n",
    "    final_messages = final_state_q1.values.get('messages', []) # 解答例より\\n",
    "    print(f\\"  最終メッセージ: {final_messages[-1].content if final_messages else 'N/A'}\\") # 解答例より\\n",
    "else:\\n",
    "    print(\\"最終状態が取得できませんでした。\\") # 解答例より"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答001</summary>\\n",
    "\\n",
    "``````python\\n",
    "from typing import TypedDict, Annotated, List, Optional\\n",
    "import time\\n",
    "from langgraph.graph import StateGraph, END\\n",
    "from langgraph.graph.message import add_messages\\n",
    "from langchain_core.messages import HumanMessage, AIMessage # HumanMessageは直接使わないが、概念として\\n",
    "from IPython.display import Image, display\\n",
    "from uuid import uuid4\\n",
    "\\n",
    "# --- 1. 状態定義 ---\\n",
    "class StreamingState(TypedDict):\\n",
    "    messages: Annotated[List[AIMessage], add_messages] # AIMessageのみを想定\\n",
    "    current_step_name: Optional[str]\\n",
    "    step_outputs: List[str]\\n",
    "\\n",
    "# --- 2. ノード定義 ---\\n",
    "def first_step_node(state: StreamingState) -> dict:\\n",
    "    print(\\"\\n[Step 1: 開始処理]\\")\\n",
    "    time.sleep(0.2) # 処理時間を模倣\\n",
    "    output = \\"最初のステップが完了しました。データAを生成。\\"\\n",
    "    print(f\\"  -> 出力: {output}\\")\\n",
    "    return {\\"current_step_name\\": \\"Step1_Done\\", \\"step_outputs\\": state.get(\\"step_outputs\\", []) + [output]}\\n",
    "\\n",
    "def second_step_node(state: StreamingState) -> dict:\\n",
    "    print(\\"\\n[Step 2: 中間処理]\\")\\n",
    "    time.sleep(0.3)\\n",
    "    prev_output = state[\\"step_outputs\\"][-1] if state.get(\\"step_outputs\\") else \\"(前の出力なし)\\"\\n",
    "    output = f\\"2番目のステップ完了。データBを生成 (前の出力: 「{prev_output}」)。\\"\\n",
    "    print(f\\"  -> 出力: {output}\\")\\n",
    "    return {\\"current_step_name\\": \\"Step2_Done\\", \\"step_outputs\\": state.get(\\"step_outputs\\", []) + [output]}\\n",
    "\\n",
    "def final_step_node(state: StreamingState) -> dict:\\n",
    "    print(\\"\\n[Step 3: 最終処理]\\")\\n",
    "    time.sleep(0.1)\\n",
    "    output = \\"全てのステップが完了し、最終結果「XYZ」が得られました。\\"\\n",
    "    print(f\\"  -> 出力: {output}\\")\\n",
    "    return {\\n",
    "        \\"current_step_name\\": \\"Final_Done\\", \\n",
    "        \\"step_outputs\\": state.get(\\"step_outputs\\", []) + [output], \\n",
    "        \\"messages\\": [AIMessage(content=output)] # 最終結果をmessagesにも記録\\n",
    "    }\\n",
    "\\n",
    "# --- 3. グラフ構築 ---\\n",
    "workflow_q1_ch5 = StateGraph(StreamingState)\\n",
    "workflow_q1_ch5.add_node(\\"step1\\", first_step_node)\\n",
    "workflow_q1_ch5.add_node(\\"step2\\", second_step_node)\\n",
    "workflow_q1_ch5.add_node(\\"final_step\\", final_step_node)\\n",
    "\\n",
    "workflow_q1_ch5.set_entry_point(\\"step1\\")\\n",
    "workflow_q1_ch5.add_edge(\\"step1\\", \\"step2\\")\\n",
    "workflow_q1_ch5.add_edge(\\"step2\\", \\"final_step\\")\\n",
    "workflow_q1_ch5.add_edge(\\"final_step\\", END)\\n",
    "\\n",
    "graph_q1_ch5 = workflow_q1_ch5.compile()\\n",
    "try: display(Image(graph_q1_ch5.get_graph().draw_png()))\\n",
    "except Exception as e: print(f\\"グラフ描画失敗: {e}\\")\\n",
    "\\n",
    "# --- 4. ストリーミング実行と状態表示 ---\\n",
    "print(\\"\\n--- グラフ実行のストリーミング開始 ---\\")\\n",
    "initial_input_q1 = {\\n",
    "    \\"messages\\": [], \\n",
    "    \\"step_outputs\\": [] \\n",
    "}\\n",
    "thread_id_q1_ch5 = f\\"streaming-test-{uuid4()[:4]}\\"\\n",
    "config_q1_ch5 = {\\"configurable\\": {\\"thread_id\\": thread_id_q1_ch5}}\\n",
    "\\n",
    "for i, event_chunk in enumerate(graph_q1_ch5.stream(initial_input_q1, config=config_q1_ch5, recursion_limit=10)):\\n",
    "    print(f\\"\\n--- イベント {i+1} --- \\")\\n",
    "    for node_name, state_update_dict in event_chunk.items():\\n",
    "        print(f\\"  ノード「{node_name}」が完了しました。\\")\\n",
    "        if isinstance(state_update_dict, dict):\\n",
    "            print(f\\"  状態更新の内容:\\")\\n",
    "            for key, value in state_update_dict.items():\\n",
    "                print(f\\"    {key}: {value}\\")\\n",
    "        else: \\n",
    "            print(f\\"  イベントデータ (ENDなど): {state_update_dict}\\")\\n",
    "print(\\"\\n--- ストリーミング終了 --- \\")\\n",
    "\\n",
    "final_state_q1 = graph_q1_ch5.get_state(config=config_q1_ch5)\\n",
    "if final_state_q1:\\n",
    "    print(\\"\\n最終的なグラフの状態:\\")\\n",
    "    print(f\\"  最終ステップ名: {final_state_q1.values.get('current_step_name')}\\")\\n",
    "    print(f\\"  全ステップ出力: {final_state_q1.values.get('step_outputs')}\\")\\n",
    "    final_messages = final_state_q1.values.get('messages', [])\\n",
    "    print(f\\"  最終メッセージ: {final_messages[-1].content if final_messages else 'N/A'}\\")\\n",
    "else:\\n",
    "    print(\\"最終状態が取得できませんでした。\\")\\n",
    "``````\\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説001</summary>\\n",
    "\\n",
    "#### この問題のポイント\\n",
    "\\n",
    "*   **`graph.stream()`メソッド:** グラフを実行し、各ノードの処理が完了するたびにイベントを生成するジェネレータを返します。`invoke()`が最終結果のみを返すのに対し、`stream()`は処理の途中経過をリアルタイムに追跡できます。\\n",
    "*   **イベントチャンクの構造:** `stream()`から返される各`event_chunk`は、通常、以下のような辞書形式です:\\n",
    "    ```python\\n",
    "    {\\n",
    "        \\"node_name\\": { # 処理が完了したノードの名前\\n",
    "            \\"state_key1\\": updated_value1, # そのノードによって更新された状態キーと値\\n",
    "            \\"state_key2\\": updated_value2,\\n",
    "            ...\\n",
    "        }\\n",
    "    }\\n",
    "    ```\\n",
    "    *   `node_name` は、そのイベントチャンクを生成したノード（処理が完了したノード）の名前です。\\n",
    "    *   `state_update_dict` は、そのノードの実行によって更新された状態のキーと値のペアを含む辞書です。これは完全な状態ではなく、そのステップでの差分（パッチ）であることが多いです。\\n",
    "    *   グラフの実行が `END` に到達すると、特別なキー（例: `__end__` または最後に実行されたノード名）と共に、その時点での完全な最終状態が返されることがあります（これはLangGraphのバージョンや設定によって挙動が異なる場合があります）。\\n",
    "*   **状態の更新とリスト:** `step_outputs` のようなリスト型の状態キーを更新する場合、ノードは既存のリストに新しい要素を追加した新しいリストを返す必要があります（例: `state.get(\\"step_outputs\\", []) + [new_output]`）。`add_messages`アノテーションが付いた`messages`キーは特別で、新しいメッセージを返すだけで自動的にリストに追加されます。\\n",
    "*   **`config`と`recursion_limit`:** `stream()`や`invoke()`を呼び出す際には、必要に応じて`config`（特にスレッドIDの管理のため）や`recursion_limit`（ループや深いグラフの場合）を指定します。\\n",
    "*   **利用シナリオ:**\\n",
    "    *   長時間かかる処理の進捗をユーザーインターフェースに表示する。\\n",
    "    *   特定のノードが完了した時点で、外部システムに通知を送るなどの副作用を実行する。\\n",
    "    *   デバッグ時に、各ステップで状態がどのように変化していくかを詳細に追跡する。\\n",
    "\\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題002: LLM からのストリーミング応答のリアルタイム処理\\n",
    "\\n",
    "問題001ではグラフ全体の実行状態のストリーミングを見ましたが、LLMが応答を生成する際には、その応答自体もトークン単位でストリーミングされることが一般的です。この問題では、グラフ内のLLMノードが生成する応答（テキスト）をストリーミングで受け取り、それをリアルタイムで（例: 標準出力に逐次）表示する方法を学びます。\\n",
    "\\n",
    "*   **学習内容:**\\n",
    "    *   LangChainのLLMオブジェクトが持つストリーミング機能（例: `llm.stream()` や `chain.stream()`）。\\n",
    "    *   LLMノード内でこのストリーミング機能を利用し、生成されるトークンやテキストチャンクを処理する方法。\\n",
    "    *   グラフ全体のストリーム（`graph.stream()`）と、LLM応答のストリームをどのように組み合わせるか、または区別して扱うかの基本的な考え方。\\n",
    "    *   （注意）LangGraphの `graph.stream()` は主にノードレベルの状態変化をストリーミングし、LLMのトークンレベルのストリーミングを直接透過的に扱うわけではありません。LLMのトークンストリーミングはノード内部でハンドリングし、その結果（部分的な応答や最終応答）を状態に反映させる形になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002 - グラフ構築\\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage # HumanMessage, AIMessage をインポート (解答例より)\\n",
    "from typing import TypedDict, Annotated, List, Optional # (既にインポート済みだが明示)\\n",
    "import time # (既にインポート済みだが明示)\\n",
    "from langgraph.graph import StateGraph, END # (既にインポート済みだが明示)\\n",
    "from langgraph.graph.message import add_messages # (既にインポート済みだが明示)\\n",
    "from uuid import uuid4 # (既にインポート済みだが明示)\\n",
    "\\n",
    "# --- 1. 状態定義 (問題001のStreamingStateを再利用可能) ---\\n",
    "# class StreamingState(TypedDict): ... (解答例よりコメントアウト)\\n",
    "\\n",
    "# --- 2. LLM応答をストリーミングするノード定義 ---\\n",
    "def streaming_llm_node(state: StreamingState) -> dict:\\n",
    "    user_prompt = \\"LangGraphのストリーミング機能（graph.stream()とllm.stream()）について、それぞれの役割と違いを初心者にも分かりやすく説明してください。\\" # 解答例よりプロンプト変更\\n",
    "    print(f\\"\\n[ストリーミングLLMノード] プロンプト: {user_prompt}\\")\\n",
    "    \\n",
    "    accumulated_response = \\"\\"\\n",
    "    print(\\"\\n  LLM応答 (リアルタイムストリーミング中):\\") # 解答例より\\n",
    "    print(\\"  \\") # 見やすくするための空行 (解答例より)\\n",
    "    \\n",
    "    try:\\n",
    "        if hasattr(llm, 'stream') and LLM_PROVIDER != \\"fake\\":\\n",
    "            for chunk in llm.stream([HumanMessage(content=user_prompt)]): # llm.stream() を使用 (解答例より)\\n",
    "                content_piece = chunk.content if hasattr(chunk, 'content') else str(chunk)\\n",
    "                print(content_piece, end=\\"\\", flush=True)\\n",
    "                accumulated_response += content_piece\\n",
    "        elif LLM_PROVIDER == \\"fake\\" and hasattr(llm, 'responses') and isinstance(llm.responses, list): # 解答例より isinstance チェック追加\\n",
    "            for response_part in llm.responses: # 解答例より\\n",
    "                print(response_part, end=\\" \\", flush=True) # 解答例より\\n",
    "                accumulated_response += response_part + \\" \\" # 解答例より\\n",
    "                time.sleep(0.1) # ストリーミングを模倣するための遅延 (解答例より)\\n",
    "            accumulated_response = accumulated_response.strip() # 解答例より\\n",
    "        else: \\n",
    "            print(\\"  WARN: llm.stream() が利用できないか、FakeLLMのstream実装が対応していません。invokeで取得します。\\") # 解答例より\\n",
    "            response_obj = llm.invoke([HumanMessage(content=user_prompt)])\\n",
    "            accumulated_response = response_obj.content\\n",
    "            print(accumulated_response)\\n",
    "\\n",
    "    except Exception as e:\\n",
    "        print(f\\"\\n  LLMストリーミングエラー: {e}\\")\\n",
    "        import traceback # 解答例より\\n",
    "        traceback.print_exc() # 解答例より\\n",
    "        accumulated_response = \\"(LLMからの応答取得に失敗しました)\\"\\n",
    "    \\n",
    "    print(\\"\\n  ストリーミング完了。\\")\\n",
    "    return {\\n",
    "        \\"messages\\": [AIMessage(content=accumulated_response)],\\n",
    "        \\"current_step_name\\": \\"LLM_Stream_Completed\\", # 解答例より\\n",
    "        \\"step_outputs\\": state.get(\\"step_outputs\\", []) + [f\\"LLM応答(要約): {accumulated_response[:70]}...\\"] # 解答例より\\n",
    "    }\\n",
    "\\n",
    "# --- 3. グラフ構築 (ストリーミングLLMノードのみのシンプルなグラフ) ---\\n",
    "workflow_q2_ch5 = StateGraph(StreamingState)\\n",
    "workflow_q2_ch5.add_node(\\"llm_streaming_responder\\", streaming_llm_node)\\n",
    "workflow_q2_ch5.set_entry_point(\\"llm_streaming_responder\\")\\n",
    "workflow_q2_ch5.add_edge(\\"llm_streaming_responder\\", END)\\n",
    "graph_q2_ch5 = workflow_q2_ch5.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002 - グラフ可視化\\n",
    "from IPython.display import Image, display # 解答例より\\n",
    "\\n",
    "try:\\n",
    "    display(Image(graph_q2_ch5.get_graph().draw_png()))\\n",
    "except Exception as e:\\n",
    "    print(f\\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002 - グラフ実行\\n",
    "print(\\"\\n--- LLM応答ストリーミングテスト開始 ---\\")\\n",
    "initial_input_q2 = {\\"messages\\": [], \\"step_outputs\\": []}\\n",
    "thread_id_q2_ch5 = f\\"llm-streaming-test-{uuid4()[:4]}\\"\\n",
    "config_q2_ch5 = {\\"configurable\\": {\\"thread_id\\": thread_id_q2_ch5}}\\n",
    "\\n",
    "final_state_q2 = graph_q2_ch5.invoke(initial_input_q2, config=config_q2_ch5)\\n",
    "\\n",
    "print(\\"\\n\\n--- グラフ実行完了後の最終状態 --- \\") # 解答例より \\n 追加\\n",
    "if final_state_q2:\\n",
    "    print(f\\"  最終ステップ名: {final_state_q2.get('current_step_name')}\\")\\n",
    "    final_messages_q2 = final_state_q2.get('messages', [])\\n",
    "    if final_messages_q2: # 解答例より\\n",
    "      print(f\\"  LLMからの最終応答 (状態より):\\") # 解答例より\\n",
    "      print(f\\"    {final_messages_q2[-1].content}\\") # 解答例より\\n",
    "    else: # 解答例より\\n",
    "      print(\\"  LLMからの最終応答が状態に見つかりません。\\") # 解答例より\\n",
    "else:\\n",
    "    print(\\"最終状態が取得できませんでした。\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答002</summary>\\n",
    "\\n",
    "``````python\\n",
    "from typing import TypedDict, Annotated, List, Optional\\n",
    "import time\\n",
    "from langgraph.graph import StateGraph, END\\n",
    "from langgraph.graph.message import add_messages\\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\\n",
    "from IPython.display import Image, display # graphviz用に追加\\n",
    "from uuid import uuid4\\n",
    "\\n",
    "# --- 1. 状態定義 (問題001のStreamingStateを再利用) ---\\n",
    "class StreamingState(TypedDict):\\n",
    "    messages: Annotated[List[AIMessage], add_messages]\\n",
    "    current_step_name: Optional[str]\\n",
    "    step_outputs: List[str]\\n",
    "\\n",
    "# --- 2. LLM応答をストリーミングするノード定義 ---\\n",
    "def streaming_llm_node(state: StreamingState) -> dict:\\n",
    "    user_prompt = \\"LangGraphのストリーミング機能（graph.stream()とllm.stream()）について、それぞれの役割と違いを初心者にも分かりやすく説明してください。\\"\\n",
    "    print(f\\"\\n[ストリーミングLLMノード] プロンプト: {user_prompt}\\")\\n",
    "    \\n",
    "    accumulated_response = \\"\\"\\n",
    "    print(\\"\\n  LLM応答 (リアルタイムストリーミング中):\\")\\n",
    "    print(\\"  \\") # 見やすくするための空行\\n",
    "    \\n",
    "    try:\\n",
    "        # llm.stream() は LangChain の LLM/ChatModel が持つメソッド\\n",
    "        # FakeLLM の場合は、準備セルで定義した CustomFakeChatModel の _generate が呼ばれる\\n",
    "        # そのため、FakeLLMでは厳密なトークンごとのストリーミングにはならないが、\\n",
    "        # 応答をチャンクに分けて逐次処理するイメージは掴める。\\n",
    "        if hasattr(llm, 'stream') and LLM_PROVIDER != \\"fake\\":\\n",
    "            for chunk in llm.stream([HumanMessage(content=user_prompt)]):\\n",
    "                content_piece = chunk.content if hasattr(chunk, 'content') else str(chunk)\\n",
    "                print(content_piece, end=\\"\\", flush=True)\\n",
    "                accumulated_response += content_piece\\n",
    "        elif LLM_PROVIDER == \\"fake\\" and hasattr(llm, 'responses') and isinstance(llm.responses, list):\\n",
    "            # CustomFakeChatModelの場合、responsesリストの各要素をチャンクとして扱う\\n",
    "            for response_part in llm.responses: \\n",
    "                print(response_part, end=\\" \\", flush=True)\\n",
    "                accumulated_response += response_part + \\" \\"\\n",
    "                time.sleep(0.1) # ストリーミングを模倣するための遅延\\n",
    "            accumulated_response = accumulated_response.strip()\\n",
    "        else: # stream非対応または未知のFakeLLM\\n",
    "            print(\\"  WARN: llm.stream() が利用できないか、FakeLLMのstream実装が対応していません。invokeで取得します。\\")\\n",
    "            response_obj = llm.invoke([HumanMessage(content=user_prompt)])\\n",
    "            accumulated_response = response_obj.content\\n",
    "            print(accumulated_response)\\n",
    "\\n",
    "    except Exception as e:\\n",
    "        print(f\\"\\n  LLMストリーミングエラー: {e}\\")\\n",
    "        import traceback\\n",
    "        traceback.print_exc()\\n",
    "        accumulated_response = \\"(LLMからの応答取得に失敗しました)\\"\\n",
    "    \\n",
    "    print(\\"\\n  ストリーミング完了。\\")\\n",
    "    return {\\n",
    "        \\"messages\\": [AIMessage(content=accumulated_response)],\\n",
    "        \\"current_step_name\\": \\"LLM_Stream_Completed\\",\\n",
    "        \\"step_outputs\\": state.get(\\"step_outputs\\", []) + [f\\"LLM応答(要約): {accumulated_response[:70]}...\\"]\\n",
    "    }\\n",
    "\\n",
    "# --- 3. グラフ構築 ---\\n",
    "workflow_q2_ch5 = StateGraph(StreamingState)\\n",
    "workflow_q2_ch5.add_node(\\"llm_streaming_responder\\", streaming_llm_node)\\n",
    "workflow_q2_ch5.set_entry_point(\\"llm_streaming_responder\\")\\n",
    "workflow_q2_ch5.add_edge(\\"llm_streaming_responder\\", END)\\n",
    "graph_q2_ch5 = workflow_q2_ch5.compile()\\n",
    "try: display(Image(graph_q2_ch5.get_graph().draw_png()))\\n",
    "except Exception as e: print(f\\"グラフ描画失敗: {e}\\")\\n",
    "\\n",
    "# --- 4. グラフ実行 ---\\n",
    "print(\\"\\n--- LLM応答ストリーミングテスト開始 ---\\")\\n",
    "initial_input_q2 = {\\"messages\\": [], \\"step_outputs\\": []}\\n",
    "thread_id_q2_ch5 = f\\"llm-streaming-test-{uuid4()[:4]}\\"\\n",
    "config_q2_ch5 = {\\"configurable\\": {\\"thread_id\\": thread_id_q2_ch5}}\\n",
    "\\n",
    "# invokeで実行し、ノード内のprint出力でストリーミングを確認\\n",
    "final_state_q2 = graph_q2_ch5.invoke(initial_input_q2, config=config_q2_ch5)\\n",
    "\\n",
    "print(\\"\\n\\n--- グラフ実行完了後の最終状態 --- \\")\\n",
    "if final_state_q2:\\n",
    "    print(f\\"  最終ステップ名: {final_state_q2.get('current_step_name')}\\")\\n",
    "    final_messages_q2 = final_state_q2.get('messages', [])\\n",
    "    if final_messages_q2:\\n",
    "      print(f\\"  LLMからの最終応答 (状態より):\\")\\n",
    "      print(f\\"    {final_messages_q2[-1].content}\\")\\n",
    "    else:\\n",
    "      print(\\"  LLMからの最終応答が状態に見つかりません。\\")\\n",
    "else:\\n",
    "    print(\\"最終状態が取得できませんでした。\\")\\n",
    "``````\\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説002</summary>\\n",
    "\\n",
    "#### この問題のポイント\\n",
    "\\n",
    "*   **LLMのストリーミング応答:** 多くのLangChainのLLM/ChatModelオブジェクトは、`stream()`というメソッドを持っています。これを呼び出すと、LLMが生成する応答をテキストチャンク（多くの場合、トークンまたは単語の断片）のストリームとして受け取ることができます。\\n",
    "    *   `streaming_llm_node`内では、`for chunk in llm.stream(...):` のようにしてこれらのチャンクを反復処理し、`print(chunk.content, end=\\"\\", flush=True)` で標準出力に逐次表示しています。`end=\\"\\"` と `flush=True` が、改行せずにリアルタイムで表示するためのポイントです。\\n",
    "    *   各チャンクの内容は `accumulated_response` に蓄積され、ストリーミング完了後に完全な応答としてグラフの状態に保存されます。\\n",
    "*   **グラフ全体のストリーミングとの違い:**\\n",
    "    *   `graph.stream()`（問題001で扱ったもの）: LangGraphグラフ全体の実行フローをストリーミングします。各ノードの実行完了とその結果（状態の差分）がイベントとして得られます。これは「どのノードが何をしたか」というマクロなレベルのストリームです。\\n",
    "    *   `llm.stream()`: 特定のLLM呼び出しの応答生成プロセスをストリーミングします。LLMがテキストを生成する際のトークンごと、またはそれに近い単位でのミクロなレベルのストリームです。\\n",
    "*   **ノード内でのLLMストリーミング処理:** LangGraphのノード（この例では `streaming_llm_node`）の中で `llm.stream()` を使ってLLMの応答を処理します。ノードの戻り値としては、通常、ストリーミングで得られた完全な応答や、それに関する情報（例: 要約）を状態に含めます。LangGraph自体がLLMのトークンレベルのストリーミングを直接外部に中継するわけではなく、あくまでノードがその処理を行い、結果を状態として返す、という流れになります。\\n",
    "*   **FakeLLMでの模倣:** 準備セルで定義した `CustomFakeChatModel` は、`stream()` メソッドを持たない `FakeMessagesListChatModel` をベースにしているため、厳密なストリーミングは行いません。解答例の `streaming_llm_node` 内のフォールバックロジックでは、`FakeLLM` の `responses` リストの各要素を時間差で表示することで、ストリーミングの「雰囲気」を模倣しています。実際のストリーミング対応LLMでは、より自然な逐次表示が期待できます。\\n",
    "*   **ユーザー体験の向上:** LLMの応答生成に時間がかかる場合、ストリーミング表示を行うことで、ユーザーは処理が進行していることをリアルタイムに確認でき、体感的な待ち時間を短縮する効果があります。チャットボットなどで非常に有効なテクニックです。\\n",
    "\\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題003: グラフ状態の永続化と再開 (MemorySaver)\\n",
    "\\n",
    "長時間実行されるグラフや、途中でユーザーの介入を必要とするグラフ（例: 問題005 Human-in-the-Loop）では、グラフの現在の状態を保存（永続化）し、後でその状態から実行を再開できる機能が重要になります。LangGraphは`checkpointer`という仕組みを提供しており、`MemorySaver`はその最も基本的なインメモリのチェックポインターです。\\n",
    "\\n",
    "*   **学習内容:**\\n",
    "    *   `langgraph.checkpoint.memory.MemorySaver` の基本的な使い方。\\n",
    "    *   グラフコンパイル時に `checkpointer` を指定する方法。\\n",
    "    *   グラフ実行時 (`invoke`, `stream`) に `configurable` パラメータでスレッドID (`thread_id`) を指定し、状態の保存と復元を可能にする方法。\\n",
    "    *   `graph.get_state(config)` を使って特定のスレッドIDの最新状態を取得する方法。\\n",
    "    *   中断可能なノード (`interrupt_before` または `interrupt_after`) と組み合わせることで、中断と再開のワークフローを実現する方法（第2章問題005の発展）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003 - グラフ構築\\n",
    "from langgraph.checkpoint.memory import MemorySaver\\n",
    "from uuid import uuid4\\n",
    "from langgraph.graph import Interrupt # 解答例より Interrupt をインポート\\n",
    "from typing import TypedDict, Annotated, List, Optional # (既にインポート済みだが明示)\\n",
    "import time # (既にインポート済みだが明示)\\n",
    "from langgraph.graph import StateGraph, END # (既にインポート済みだが明示)\\n",
    "from langgraph.graph.message import add_messages # (既にインポート済みだが明示)\\n",
    "from langchain_core.messages import AIMessage # (既にインポート済みだが明示)\\n",
    "\\n",
    "# --- 1. 状態定義 (問題001のStreamingStateを再利用) ---\n",
    "# class StreamingState(TypedDict): ... (解答例よりコメントアウト)\\n",
    "\\n",
    "# --- 2. ノード定義 (問題001のものを一部変更して利用) ---\\n",
    "def persistent_step_node(state: StreamingState, step_name: str, sleep_time: float, message_prefix: str) -> dict:\\n",
    "    print(f\\"\\n[{step_name}] 実行中...\\")\\n",
    "    time.sleep(sleep_time)\\n",
    "    output = f\\"{message_prefix}「{state.get('current_step_name', '初期')}」からの出力。\\"\\n",
    "    if state.get(\\"step_outputs\\") and state[\\"step_outputs\\"]: # 解答例より None チェック追加\\n",
    "        output = f\\"{message_prefix}「{state['step_outputs'][-1][:20]}...」を受けて処理。\\"\\n",
    "    print(f\\"  -> {step_name} 出力: {output}\\")\\n",
    "    current_outputs = state.get(\\"step_outputs\\", []) # 解答例より\\n",
    "    return {\\"current_step_name\\": step_name, \\"step_outputs\\": current_outputs + [output]}\\n",
    "\\n",
    "def node_alpha(state: StreamingState): return persistent_step_node(state, \\"NodeAlpha\\", 0.1, \\"アルファ処理: \\") # 解答例より時間変更\\n",
    "def node_beta(state: StreamingState):  return persistent_step_node(state, \\"NodeBeta\\", 0.1, \\"ベータ処理: \\") # 解答例より時間変更\\n",
    "\\n",
    "def node_gamma_interrupt(state: StreamingState): \\n",
    "    print(\\"\\n[NodeGamma (中断ポイント)] に到達。このノードの処理は中断前に実行されます。\\") # 解答例より\\n",
    "    return {\\"current_step_name\\": \\"NodeGamma_Reached_And_Processed\\"} # 解答例より\\n",
    "\\n",
    "def node_delta_after_resume(state: StreamingState): \\n",
    "    print(\\"\\n[NodeDelta (再開後)] 実行中...\\")\\n",
    "    last_message_content = \\"(再開時メッセージなし)\\" # 解答例より\\n",
    "    if state.get(\\"messages\\") and state[\\"messages\\"]: # 解答例より\\n",
    "        last_message_content = state[\\"messages\\"][-1].content\\n",
    "        \\n",
    "    output = f\\"ガンマから再開。直前のmessagesの最後の内容は「{last_message_content}」。デルタ処理完了。\\" # 解答例より\\n",
    "    print(f\\"  -> NodeDelta 出力: {output}\\")\\n",
    "    current_outputs = state.get(\\"step_outputs\\", []) # 解答例より\\n",
    "    return {\\"current_step_name\\": \\"NodeDelta_Done\\", \\"step_outputs\\": current_outputs + [output], \\"messages\\": [AIMessage(content=output)]}\\n",
    "\\n",
    "# --- 3. グラフ構築とMemorySaverの設定 ---\\n",
    "memory_saver = MemorySaver() # MemorySaverインスタンスを作成 (解答例より)\\n",
    "\\n",
    "workflow_q3_ch5 = StateGraph(StreamingState)\\n",
    "workflow_q3_ch5.add_node(\\"alpha\\", node_alpha)\\n",
    "workflow_q3_ch5.add_node(\\"beta\\", node_beta)\\n",
    "workflow_q3_ch5.add_node(\\"gamma_interrupt_point\\", node_gamma_interrupt)\\n",
    "workflow_q3_ch5.add_node(\\"delta_resumed\\", node_delta_after_resume)\\n",
    "\\n",
    "workflow_q3_ch5.set_entry_point(\\"alpha\\")\\n",
    "workflow_q3_ch5.add_edge(\\"alpha\\", \\"beta\\")\\n",
    "workflow_q3_ch5.add_edge(\\"beta\\", \\"gamma_interrupt_point\\")\\n",
    "workflow_q3_ch5.add_edge(\\"gamma_interrupt_point\\", \\"delta_resumed\\") \\n",
    "workflow_q3_ch5.add_edge(\\"delta_resumed\\", END)\\n",
    "\\n",
    "graph_q3_ch5 = workflow_q3_ch5.compile(\\n",
    "    checkpointer=memory_saver, # 作成したMemorySaverインスタンスを指定 (解答例より)\\n",
    "    interrupt_before=[\\"gamma_interrupt_point\\"] \\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003 - グラフ可視化\\n",
    "from IPython.display import Image, display # 解答例より\\n",
    "\\n",
    "try:\\n",
    "    display(Image(graph_q3_ch5.get_graph().draw_png()))\\n",
    "except Exception as e:\\n",
    "    print(f\\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003 - グラフ実行\\n",
    "thread_id_q3 = str(uuid4()) \\n",
    "config_q3 = {\\"configurable\\": {\\"thread_id\\": thread_id_q3}}\\n",
    "\\n",
    "print(f\\"--- 状態の永続化と再開テスト (Thread ID: {thread_id_q3}) ---\\")\\n",
    "print(\\"\\n1. グラフを初期状態で実行 (gamma_interrupt_pointの手前で中断するはず):\\")\\n",
    "initial_input_q3 = {\\"messages\\": [], \\"step_outputs\\": [], \\"current_step_name\\": \\"Initial\\"} # 解答例より\\n",
    "\\n",
    "try: # 解答例より try-except ブロック追加\\n",
    "    graph_q3_ch5.invoke(initial_input_q3, config_q3, {\\"recursion_limit\\": 10})\\n",
    "except Interrupt as e: # 解答例より\\n",
    "    print(f\\"  >>> グラフ実行が Interrupt により計画通り中断されました。中断情報: {e.args}\\")\\n",
    "    pass \\n",
    "except Exception as e: # 解答例より\\n",
    "    print(f\\"  予期せぬエラーでグラフ実行が停止: {e}\\")\\n",
    "\\n",
    "print(\\"\\n2. 中断時のグラフの状態を取得:\\")\\n",
    "interrupted_state = graph_q3_ch5.get_state(config_q3)\\n",
    "if interrupted_state:\\n",
    "    print(f\\"  現在のステップ名 (中断時): {interrupted_state.values.get('current_step_name')}\\") # 解答例より\\n",
    "    print(f\\"  これまでのステップ出力: {interrupted_state.values.get('step_outputs')}\\")\\n",
    "    print(f\\"  次の実行予定ノード: {interrupted_state.next}\\") \\n",
    "else:\\n",
    "    print(\\"  中断状態が取得できませんでした。\\")\\n",
    "\\n",
    "print(\\"\\n3. グラフに新しい情報（メッセージ）を注入して再開:\\")\\n",
    "resume_input_q3 = {\\"messages\\": [AIMessage(content=\\"ユーザーが処理の再開を指示しました。\\")]} # 解答例より\\n",
    "final_state_after_resume = None # 解答例より\\n",
    "try: # 解答例より\\n",
    "    for event in graph_q3_ch5.stream(resume_input_q3, config_q3, {\\"recursion_limit\\": 10}): \\n",
    "        print(f\\"  Event (再開後): {event}\\")\\n",
    "        if END in event: # 解答例より\\n",
    "            final_state_after_resume = event[END] # 解答例より\\n",
    "except Exception as e: # 解答例より\\n",
    "    print(f\\"  再開後の実行でエラー: {e}\\")\\n",
    "\\n",
    "print(\\"\\n4. 再開後の最終状態を取得:\\")\\n",
    "if not final_state_after_resume: # 解答例より\\n",
    "    final_state_obj_q3 = graph_q3_ch5.get_state(config_q3)\\n",
    "    if final_state_obj_q3: final_state_after_resume = final_state_obj_q3.values\\n",
    "\\n",
    "if final_state_after_resume: # 解答例より\\n",
    "    print(f\\"  最終ステップ名: {final_state_after_resume.get('current_step_name')}\\")\\n",
    "    print(f\\"  全ステップ出力: {final_state_after_resume.get('step_outputs')}\\")\\n",
    "    final_messages_q3 = final_state_after_resume.get('messages', [])\\n",
    "    print(f\\"  最終メッセージ: {final_messages_q3[-1].content if final_messages_q3 else 'N/A'}\\")\\n",
    "else:\\n",
    "    print(\\"  最終状態が取得できませんでした。\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答003</summary>\\n",
    "\\n",
    "``````python\\n",
    "from typing import TypedDict, Annotated, List, Optional\\n",
    "import time\\n",
    "from langgraph.graph import StateGraph, END, Interrupt # Interruptをインポート\\n",
    "from langgraph.graph.message import add_messages\\n",
    "from langchain_core.messages import HumanMessage, AIMessage \\n",
    "from IPython.display import Image, display\\n",
    "from uuid import uuid4\\n",
    "from langgraph.checkpoint.memory import MemorySaver\\n",
    "\\n",
    "# --- 1. 状態定義 (StreamingStateを再利用) ---\\n",
    "class StreamingState(TypedDict):\\n",
    "    messages: Annotated[List[AIMessage], add_messages]\\n",
    "    current_step_name: Optional[str]\\n",
    "    step_outputs: List[str]\\n",
    "\\n",
    "# --- 2. ノード定義 ---\\n",
    "def persistent_step_node(state: StreamingState, step_name: str, sleep_time: float, message_prefix: str) -> dict:\\n",
    "    print(f\\"\\n[{step_name}] 実行中...\\")\\n",
    "    time.sleep(sleep_time)\\n",
    "    output = f\\"{message_prefix}「{state.get('current_step_name', '初期')}」からの出力。\\"\\n",
    "    if state.get(\\"step_outputs\\") and state[\\"step_outputs\\"]:\\n",
    "        output = f\\"{message_prefix}「{state['step_outputs'][-1][:20]}...」を受けて処理。\\"\\n",
    "    print(f\\"  -> {step_name} 出力: {output}\\")\\n",
    "    current_outputs = state.get(\\"step_outputs\\", [])\\n",
    "    return {\\"current_step_name\\": step_name, \\"step_outputs\\": current_outputs + [output]}\\n",
    "\\n",
    "def node_alpha(state: StreamingState): return persistent_step_node(state, \\"NodeAlpha\\", 0.1, \\"アルファ処理: \\")\\n",
    "def node_beta(state: StreamingState):  return persistent_step_node(state, \\"NodeBeta\\", 0.1, \\"ベータ処理: \\")\\n",
    "\\n",
    "def node_gamma_interrupt(state: StreamingState): \\n",
    "    print(\\"\\n[NodeGamma (中断ポイント)] に到達。このノードの処理は中断前に実行されます。\\")\\n",
    "    # interrupt_before で指定されたノードは、その処理が実行される前に中断が発生する。\\n",
    "    # もしこのノードの処理内容が中断後の状態に影響を与える場合、その挙動を理解しておく必要がある。\\n",
    "    # ここでは、中断が発生することをログで示すに留める。\\n",
    "    # raise Interrupt() をノード内で使うと、その場で即座に中断できる。\\n",
    "    return {\\"current_step_name\\": \\"NodeGamma_Reached_And_Processed\\"} \\n",
    "\\n",
    "def node_delta_after_resume(state: StreamingState): \\n",
    "    print(\\"\\n[NodeDelta (再開後)] 実行中...\\")\\n",
    "    # messagesキーはAIMessageのリストと定義されているので、HumanMessageは直接追加できない。\\n",
    "    # 再開時の入力は、stateを直接更新する形で与えられる。\\n",
    "    # ここでは、再開時にmessagesに何か追加されたかを確認する。\\n",
    "    last_message_content = \\"(再開時メッセージなし)\\"\\n",
    "    if state.get(\\"messages\\") and state[\\"messages\\"]:\\n",
    "        last_message_content = state[\\"messages\\"][-1].content\\n",
    "        \\n",
    "    output = f\\"ガンマから再開。直前のmessagesの最後の内容は「{last_message_content}」。デルタ処理完了。\\"\\n",
    "    print(f\\"  -> NodeDelta 出力: {output}\\")\\n",
    "    current_outputs = state.get(\\"step_outputs\\", [])\\n",
    "    return {\\"current_step_name\\": \\"NodeDelta_Done\\", \\"step_outputs\\": current_outputs + [output], \\"messages\\": [AIMessage(content=output)]}\\n",
    "\\n",
    "# --- 3. グラフ構築とMemorySaverの設定 ---\\n",
    "memory_saver = MemorySaver() \\n",
    "\\n",
    "workflow_q3_ch5 = StateGraph(StreamingState)\\n",
    "workflow_q3_ch5.add_node(\\"alpha\\", node_alpha)\\n",
    "workflow_q3_ch5.add_node(\\"beta\\", node_beta)\\n",
    "workflow_q3_ch5.add_node(\\"gamma_interrupt_point\\", node_gamma_interrupt)\\n",
    "workflow_q3_ch5.add_node(\\"delta_resumed\\", node_delta_after_resume)\\n",
    "\\n",
    "workflow_q3_ch5.set_entry_point(\\"alpha\\")\\n",
    "workflow_q3_ch5.add_edge(\\"alpha\\", \\"beta\\")\\n",
    "workflow_q3_ch5.add_edge(\\"beta\\", \\"gamma_interrupt_point\\")\\n",
    "workflow_q3_ch5.add_edge(\\"gamma_interrupt_point\\", \\"delta_resumed\\")\\n",
    "workflow_q3_ch5.add_edge(\\"delta_resumed\\", END)\\n",
    "\\n",
    "graph_q3_ch5 = workflow_q3_ch5.compile(\\n",
    "    checkpointer=memory_saver,\\n",
    "    interrupt_before=[\\"gamma_interrupt_point\\"] \\n",
    ")\\n",
    "try: display(Image(graph_q3_ch5.get_graph().draw_png()))\\n",
    "except Exception as e: print(f\\"グラフ描画失敗: {e}\\")\\n",
    "\\n",
    "# --- 4. 実行、中断、状態確認、再開 ---\\n",
    "thread_id_q3 = str(uuid4()) \\n",
    "config_q3 = {\\"configurable\\": {\\"thread_id\\": thread_id_q3}}\\n",
    "\\n",
    "print(f\\"\\n--- 状態の永続化と再開テスト (Thread ID: {thread_id_q3}) ---\\")\\n",
    "print(\\"\\n1. グラフを初期状態で実行 (gamma_interrupt_pointの手前で中断するはず):\\")\\n",
    "initial_input_q3 = {\\"messages\\": [], \\"step_outputs\\": [], \\"current_step_name\\": \\"Initial\\"}\\n",
    "\\n",
    "# invoke() を使うと、中断が発生した時点で Interrupt 例外が送出される\\n",
    "interrupted_run = None\\n",
    "try:\\n",
    "    graph_q3_ch5.invoke(initial_input_q3, config_q3, {\\"recursion_limit\\": 10})\\n",
    "except Interrupt as e:\\n",
    "    print(f\\"  >>> グラフ実行が Interrupt により計画通り中断されました。中断情報: {e.args}\\")\\n",
    "    # 中断時の実行ID（run_id）などを取得できる場合がある\\n",
    "    # e.args[0] に run_id が含まれることがある (LangGraphのバージョンによる)\\n",
    "    # interrupted_run = e.args[0] if e.args else None \\n",
    "    pass # 意図した中断なので例外処理はここまで\\n",
    "except Exception as e:\\n",
    "    print(f\\"  予期せぬエラーでグラフ実行が停止: {e}\\")\\n",
    "\\n",
    "print(\\"\\n2. 中断時のグラフの状態を取得:\\")\\n",
    "interrupted_state = graph_q3_ch5.get_state(config_q3)\\n",
    "if interrupted_state:\\n",
    "    print(f\\"  現在のステップ名 (中断時): {interrupted_state.values.get('current_step_name')}\\")\\n",
    "    print(f\\"  これまでのステップ出力: {interrupted_state.values.get('step_outputs')}\\")\\n",
    "    print(f\\"  次の実行予定ノード: {interrupted_state.next}\\") \\n",
    "else:\\n",
    "    print(\\"  中断状態が取得できませんでした。checkpointerが正しく機能しているか確認してください。\\")\\n",
    "\\n",
    "print(\\"\\n3. グラフに新しい情報（メッセージ）を注入して再開:\\")\\n",
    "resume_input_q3 = {\\"messages\\": [AIMessage(content=\\"ユーザーが処理の再開を指示しました。\\")]}\\n",
    "final_state_after_resume = None\\n",
    "try:\\n",
    "    # 中断したグラフに新しい状態(差分)を渡して再開\\n",
    "    # streamで再開処理の各ステップを見ることもできる\\n",
    "    for event in graph_q3_ch5.stream(resume_input_q3, config_q3, {\\"recursion_limit\\": 10}): \\n",
    "        print(f\\"  Event (再開後): {event}\\")\\n",
    "        if END in event:\\n",
    "            final_state_after_resume = event[END]\\n",
    "except Exception as e:\\n",
    "    print(f\\"  再開後の実行でエラー: {e}\\")\\n",
    "\\n",
    "print(\\"\\n4. 再開後の最終状態を取得:\\")\\n",
    "if not final_state_after_resume: # streamのENDでキャッチできなかった場合\\n",
    "    final_state_obj_q3 = graph_q3_ch5.get_state(config_q3)\\n",
    "    if final_state_obj_q3: final_state_after_resume = final_state_obj_q3.values\\n",
    "\\n",
    "if final_state_after_resume:\\n",
    "    print(f\\"  最終ステップ名: {final_state_after_resume.get('current_step_name')}\\")\\n",
    "    print(f\\"  全ステップ出力: {final_state_after_resume.get('step_outputs')}\\")\\n",
    "    final_messages_q3 = final_state_after_resume.get('messages', [])\\n",
    "    print(f\\"  最終メッセージ: {final_messages_q3[-1].content if final_messages_q3 else 'N/A'}\\")\\n",
    "else:\\n",
    "    print(\\"  最終状態が取得できませんでした。\\")\\n",
    "``````\\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説003</summary>\\n",
    "\\n",
    "#### この問題のポイント\\n",
    "\\n",
    "*   **`MemorySaver`:** `langgraph.checkpoint.memory.MemorySaver()` は、グラフの状態をPythonのメモリ内に保存する最もシンプルなチェックポインターです。これにより、グラフの実行履歴（スナップショット）が保持され、中断や再開が可能になります。プロダクション環境では、`SqliteSaver` や `PostgresSaver` など、より永続的なストレージバックエンドを使うことが一般的です。\\n",
    "*   **`checkpointer`の指定:** `graph = workflow.compile(checkpointer=memory_saver, ...)` のように、グラフをコンパイルする際に `checkpointer` 引数に作成した `MemorySaver` インスタンスを渡します。\\n",
    "*   **`interrupt_before` (または `interrupt_after`):** `compile` メソッドの `interrupt_before` (または `interrupt_after`) 引数にノード名のリストを指定すると、それらのノードの実行前（または後）にグラフの実行が自動的に `Interrupt` 例外を発生させて一時停止します。この機能は `checkpointer` が設定されている場合にのみ有効です。\\n",
    "    *   この例では `interrupt_before=[\\"gamma_interrupt_point\\"]` としているので、`gamma_interrupt_point` ノードが実行される直前で中断します。\\n",
    "*   **`configurable`とスレッドID (`thread_id`):**\\n",
    "    *   グラフの実行 (`invoke` や `stream`) 時に、`config={\\"configurable\\": {\\"thread_id\\": \\"unique_id\\"}}` のように `configurable` パラメータを通じてユニークな `thread_id` を渡します。\\n",
    "    *   `checkpointer` はこの `thread_id` を使って、特定の実行シーケンス（会話やセッションに相当）の状態を識別し、保存・復元します。\\n",
    "    *   同じ `thread_id` を使って再度 `invoke` や `stream` を呼び出すと、中断した箇所から処理が再開されます。再開時に新しい入力（状態の差分）を渡すことで、中断後の処理に影響を与えることができます。\\n",
    "*   **`graph.get_state(config)`:** 特定の `thread_id` に関連付けられたグラフの最新の保存済み状態（スナップショット）を取得します。中断が発生した直後に呼び出すことで、中断時の状態を確認できます。\\n",
    "*   **中断と再開のフロー:**\\n",
    "    1.  最初の `graph.invoke(initial_input_q3, config_q3)` を呼び出すと、`gamma_interrupt_point` ノードの手前で `Interrupt` 例外が発生して実行が止まります。この時点までの状態は `memory_saver` に保存されています。\\n",
    "    2.  `graph.get_state(config_q3)` で中断時の状態を確認できます。`interrupted_state.next` には次に実行されるべきノード（この場合は `gamma_interrupt_point`）の名前が入っています。\\n",
    "    3.  次に、`graph.stream(resume_input_q3, config_q3)` (または `invoke`) を同じ `config_q3` で呼び出します。`resume_input_q3` には、再開後の処理に反映させたい状態の更新（例: 新しいメッセージ）を含めます。\\n",
    "    4.  グラフは中断した `gamma_interrupt_point` ノードから実行を再開し、`delta_resumed` ノード、そして `END` へと処理が進みます。\\n",
    "*   **注意点:** `interrupt_before` で指定されたノード（例: `gamma_interrupt_point`）は、中断が発生する時点ではまだ実行されていません。再開後にそのノードが最初に実行されます。もしノードの処理の途中で中断したい場合は、ノード関数内で明示的に `raise Interrupt()` を呼び出す必要があります。\\n",
    "\\n",
    "この永続化と再開のメカニズムは、ユーザーとの対話が途中で必要になるエージェントや、非常に長時間の処理を行うバッチプロセスなどで不可欠な機能です。\\n",
    "\\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題004: データベースによる永続化 (SqliteSaver)\\n",
    "\\n",
    "問題003ではインメモリの`MemorySaver`を使いましたが、実際のアプリケーションではデータベースなどのより永続的なストレージにグラフの状態を保存する必要があります。LangGraphは`SqliteSaver`や`PostgresSaver`など、いくつかのデータベースバックエンド用のチェックポインターを提供しています。この問題では、`SqliteSaver`を使ってグラフの状態をSQLiteデータベースファイルに保存し、そこから再開する方法を学びます。\\n",
    "\\n",
    "*   **学習内容:**\\n",
    "    *   `langgraph.checkpoint.sqlite.SqliteSaver` のセットアップ方法（接続文字列またはコネクションオブジェクトの指定）。\\n",
    "    *   SQLiteデータベースファイルがどのように作成され、グラフの状態がどのようにテーブルに格納されるかの概要理解。\\n",
    "    *   `MemorySaver`と同様に、`thread_id`を使って特定の実行状態を識別し、中断・再開を行う流れ。\\n",
    "    *   インメモリとの違い（永続性、複数プロセスからのアクセス可能性など）の意識。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004 - グラフ構築\\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\\n",
    "import sqlite3\\n",
    "import os # 解答例より\\n",
    "from langgraph.graph import Interrupt # 解答例より\\n",
    "from typing import TypedDict, Annotated, List, Optional # (既にインポート済みだが明示)\\n",
    "import time # (既にインポート済みだが明示)\\n",
    "from langgraph.graph import StateGraph, END # (既にインポート済みだが明示)\\n",
    "from langgraph.graph.message import add_messages # (既にインポート済みだが明示)\\n",
    "from langchain_core.messages import AIMessage # (既にインポート済みだが明示)\\n",
    "from uuid import uuid4 # (既にインポート済みだが明示)\\n",
    "\\n",
    "# --- 1. 状態定義 (StreamingStateを再利用) ---\n",
    "# class StreamingState(TypedDict): ... (解答例よりコメントアウト)\\n",
    "\\n",
    "# --- 2. ノード定義 (問題003のものを再利用) ---\\n",
    "# def node_alpha(state: StreamingState): ... (解答例では persistent_step_node を直接呼び出し)\\n",
    "# def node_beta(state: StreamingState): ... (解答例では persistent_step_node を直接呼び出し)\\n",
    "# def node_gamma_interrupt(state: StreamingState): ... (解答例より)\\n",
    "# def node_delta_after_resume(state: StreamingState): ... (解答例より)\\n",
    "\\n",
    "# --- 3. SqliteSaverの設定とグラフ構築 ---\\n",
    "db_file = \\"langgraph_checkpoint_q4.sqlite\\"\\n",
    "try: \\n",
    "    if os.path.exists(db_file):\\n",
    "        os.remove(db_file) \\n",
    "        print(f\\"既存のDBファイル「{db_file}」を削除しました。\\")\\n",
    "except OSError as e:\\n",
    "    print(f\\"DBファイル削除エラー: {e}。テストは続行しますが、古いデータが残っている可能性があります。\\")\\n",
    "\\n",
    "sqlite_saver = SqliteSaver.from_conn_string(f\\"sqlite:///{db_file}\\") # SQLiteの接続文字列を指定 (解答例より)\\n",
    "\\n",
    "workflow_q4_ch5 = StateGraph(StreamingState)\\n",
    "workflow_q4_ch5.add_node(\\"alpha\\", node_alpha) # node_alpha, node_beta は問題003のものを流用\\n",
    "workflow_q4_ch5.add_node(\\"beta\\", node_beta)\\n",
    "workflow_q4_ch5.add_node(\\"gamma_interrupt_point\\", node_gamma_interrupt)\\n",
    "workflow_q4_ch5.add_node(\\"delta_resumed\\", node_delta_after_resume)\\n",
    "workflow_q4_ch5.set_entry_point(\\"alpha\\")\\n",
    "workflow_q4_ch5.add_edge(\\"alpha\\", \\"beta\\")\\n",
    "workflow_q4_ch5.add_edge(\\"beta\\", \\"gamma_interrupt_point\\")\\n",
    "workflow_q4_ch5.add_edge(\\"gamma_interrupt_point\\", \\"delta_resumed\\")\\n",
    "workflow_q4_ch5.add_edge(\\"delta_resumed\\", END)\\n",
    "\\n",
    "graph_q4_ch5 = workflow_q4_ch5.compile(\\n",
    "    checkpointer=sqlite_saver, # 作成したSqliteSaverインスタンスを指定 (解答例より)\\n",
    "    interrupt_before=[\\"gamma_interrupt_point\\"]\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004 - グラフ可視化\\n",
    "from IPython.display import Image, display # 解答例より\\n",
    "\\n",
    "try:\\n",
    "    display(Image(graph_q4_ch5.get_graph().draw_png()))\\n",
    "except Exception as e:\\n",
    "    print(f\\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004 - グラフ実行\\n",
    "thread_id_q4 = str(uuid4())\\n",
    "config_q4 = {\\"configurable\\": {\\"thread_id\\": thread_id_q4}}\\n",
    "\\n",
    "print(f\\"\\n--- SqliteSaverテスト (DB: {db_file}, Thread ID: {thread_id_q4}) ---\\")\\n",
    "print(\\"\\n1. グラフを初期状態で実行 (中断予定):\\")\\n",
    "initial_input_q4 = {\\"messages\\": [], \\"step_outputs\\": [], \\"current_step_name\\": \\"Initial_SQLite\\"}\\n",
    "try:\\n",
    "    graph_q4_ch5.invoke(initial_input_q4, config_q4, {\\"recursion_limit\\": 10})\\n",
    "except Interrupt:\\n",
    "    print(\\"  >>> グラフ実行が計画通り中断しました (SQLite)。\\")\\n",
    "except Exception as e:\\n",
    "    print(f\\"  予期せぬエラーでグラフ実行が停止 (SQLite): {e}\\")\\n",
    "\\n",
    "print(\\"\\n2. 中断時のグラフの状態をDBから取得:\\")\\n",
    "interrupted_state_q4 = graph_q4_ch5.get_state(config_q4)\\n",
    "if interrupted_state_q4:\\n",
    "    print(f\\"  現在のステップ名 (中断時): {interrupted_state_q4.values.get('current_step_name')}\\")\\n",
    "    print(f\\"  次の実行予定ノード: {interrupted_state_q4.next}\\")\\n",
    "    print(f\\"  DBに保存されたステップ出力: {interrupted_state_q4.values.get('step_outputs')}\\") # 解答例より\\n",
    "else:\\n",
    "    print(\\"  中断状態がDBから取得できませんでした。\\")\\n",
    "\\n",
    "print(\\"\\n3. グラフに新しい情報を注入してDBから再開:\\")\\n",
    "resume_input_q4 = {\\"messages\\": [AIMessage(content=\\"SQLiteからの再開指示。NodeGammaの処理は中断前に完了しています。\\")]} # 解答例より\\n",
    "final_state_val_q4_stream = None # 解答例より\\n",
    "try:\\n",
    "    for event in graph_q4_ch5.stream(resume_input_q4, config_q4, {\\"recursion_limit\\": 10}):\\n",
    "        print(f\\"  Event (再開後 SQLite): {event}\\")\\n",
    "        if END in event: final_state_val_q4_stream = event[END] # 解答例より\\n",
    "except Exception as e:\\n",
    "    print(f\\"  再開後の実行でエラー (SQLite): {e}\\")\\n",
    "\\n",
    "print(\\"\\n4. 再開後の最終状態をDBから取得:\\")\\n",
    "final_state_val_q4_get = None # 解答例より\\n",
    "final_state_obj_q4 = graph_q4_ch5.get_state(config_q4) # 解答例より\\n",
    "if final_state_obj_q4: final_state_val_q4_get = final_state_obj_q4.values # 解答例より\\n",
    "\\n",
    "final_authoritative_state = final_state_val_q4_stream if final_state_val_q4_stream else final_state_val_q4_get # 解答例より\\n",
    "if final_authoritative_state: # 解答例より\\n",
    "    print(f\\"  最終ステップ名: {final_authoritative_state.get('current_step_name')}\\")\\n",
    "    final_messages_q4 = final_authoritative_state.get('messages', [])\\n",
    "    print(f\\"  最終メッセージ: {final_messages_q4[-1].content if final_messages_q4 else 'N/A'}\\")\\n",
    "    print(f\\"  DBに保存された全ステップ出力: {final_authoritative_state.get('step_outputs')}\\") # 解答例より\\n",
    "else:\\n",
    "    print(\\"  最終状態がDBから取得できませんでした。\\")\\n",
    "\\n",
    "print(f\\"\\nSQLiteデータベースファイル '{db_file}' が作成/更新されているはずです。\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答004</summary>\\n",
    "\\n",
    "``````python\\n",
    "from typing import TypedDict, Annotated, List, Optional\\n",
    "import time\\n",
    "import os # ファイル削除のため\\n",
    "import sqlite3 # DB確認のため(オプション)\\n",
    "from langgraph.graph import StateGraph, END, Interrupt\\n",
    "from langgraph.graph.message import add_messages\\n",
    "from langchain_core.messages import AIMessage\\n",
    "from IPython.display import Image, display\\n",
    "from uuid import uuid4\\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\\n",
    "\\n",
    "# --- 1. 状態定義 (StreamingStateを再利用) ---\\n",
    "class StreamingState(TypedDict):\\n",
    "    messages: Annotated[List[AIMessage], add_messages]\\n",
    "    current_step_name: Optional[str]\\n",
    "    step_outputs: List[str]\\n",
    "\\n",
    "# --- 2. ノード定義 (問題003のものを再利用) ---\\n",
    "def persistent_step_node(state: StreamingState, step_name: str, sleep_time: float, message_prefix: str) -> dict:\\n",
    "    print(f\\"\\n[{step_name}] 実行中...\\")\\n",
    "    time.sleep(sleep_time)\\n",
    "    output = f\\"{message_prefix}「{state.get('current_step_name', '初期')}」からの出力。\\"\\n",
    "    if state.get(\\"step_outputs\\") and state[\\"step_outputs\\"]:\\n",
    "        output = f\\"{message_prefix}「{state['step_outputs'][-1][:20]}...」を受けて処理。\\"\\n",
    "    print(f\\"  -> {step_name} 出力: {output}\\")\\n",
    "    current_outputs = state.get(\\"step_outputs\\", [])\\n",
    "    return {\\"current_step_name\\": step_name, \\"step_outputs\\": current_outputs + [output]}\\n",
    "\\n",
    "def node_alpha(state: StreamingState): return persistent_step_node(state, \\"NodeAlpha\\", 0.05, \\"アルファ(SQLite): \\")\\n",
    "def node_beta(state: StreamingState):  return persistent_step_node(state, \\"NodeBeta\\", 0.05, \\"ベータ(SQLite): \\")\\n",
    "def node_gamma_interrupt(state: StreamingState): \\n",
    "    print(\\"\\n[NodeGamma (中断ポイント)] に到達 (SQLite)。\\")\\n",
    "    return {\\"current_step_name\\": \\"NodeGamma_Reached_SQLite\\"} \\n",
    "def node_delta_after_resume(state: StreamingState): \\n",
    "    print(\\"\\n[NodeDelta (再開後 SQLite)] 実行中...\\")\\n",
    "    last_message_content = \\"(再開時メッセージなし)\\"\\n",
    "    if state.get(\\"messages\\") and state[\\"messages\\"]:\\n",
    "        last_message_content = state[\\"messages\\"][-1].content\\n",
    "    output = f\\"ガンマから再開(SQLite)。直前メッセージ「{last_message_content}」。デルタ処理完了。\\"\\n",
    "    print(f\\"  -> NodeDelta 出力: {output}\\")\\n",
    "    current_outputs = state.get(\\"step_outputs\\", [])\\n",
    "    return {\\"current_step_name\\": \\"NodeDelta_Done_SQLite\\", \\"step_outputs\\": current_outputs + [output], \\"messages\\": [AIMessage(content=output)]}\\n",
    "\\n",
    "# --- 3. SqliteSaverの設定とグラフ構築 ---\\n",
    "db_file = \\"langgraph_checkpoint_q4.sqlite\\"\\n",
    "try: \\n",
    "    if os.path.exists(db_file):\\n",
    "        os.remove(db_file) \\n",
    "        print(f\\"既存のDBファイル「{db_file}」を削除しました。\\")\\n",
    "except OSError as e:\\n",
    "    print(f\\"DBファイル削除エラー: {e}。テストは続行しますが、古いデータが残っている可能性があります。\\")\\n",
    "\\n",
    "sqlite_saver = SqliteSaver.from_conn_string(f\\"sqlite:///{db_file}\\") \\n",
    "\\n",
    "workflow_q4_ch5 = StateGraph(StreamingState)\\n",
    "workflow_q4_ch5.add_node(\\"alpha\\", node_alpha)\\n",
    "workflow_q4_ch5.add_node(\\"beta\\", node_beta)\\n",
    "workflow_q4_ch5.add_node(\\"gamma_interrupt_point\\", node_gamma_interrupt)\\n",
    "workflow_q4_ch5.add_node(\\"delta_resumed\\", node_delta_after_resume)\\n",
    "workflow_q4_ch5.set_entry_point(\\"alpha\\")\\n",
    "workflow_q4_ch5.add_edge(\\"alpha\\", \\"beta\\")\\n",
    "workflow_q4_ch5.add_edge(\\"beta\\", \\"gamma_interrupt_point\\")\\n",
    "workflow_q4_ch5.add_edge(\\"gamma_interrupt_point\\", \\"delta_resumed\\")\\n",
    "workflow_q4_ch5.add_edge(\\"delta_resumed\\", END)\\n",
    "\\n",
    "graph_q4_ch5 = workflow_q4_ch5.compile(\\n",
    "    checkpointer=sqlite_saver,\\n",
    "    interrupt_before=[\\"gamma_interrupt_point\\"]\\n",
    ")\\n",
    "try: display(Image(graph_q4_ch5.get_graph().draw_png()))\\n",
    "except Exception as e: print(f\\"グラフ描画失敗: {e}\\")\\n",
    "\\n",
    "# --- 4. 実行、中断、状態確認、再開 ---\\n",
    "thread_id_q4 = str(uuid4())\\n",
    "config_q4 = {\\"configurable\\": {\\"thread_id\\": thread_id_q4}}\\n",
    "\\n",
    "print(f\\"\\n--- SqliteSaverテスト (DB: {db_file}, Thread ID: {thread_id_q4}) ---\\")\\n",
    "print(\\"\\n1. グラフを初期状態で実行 (中断予定):\\")\\n",
    "initial_input_q4 = {\\"messages\\": [], \\"step_outputs\\": [], \\"current_step_name\\": \\"Initial_SQLite\\"}\\n",
    "try:\\n",
    "    graph_q4_ch5.invoke(initial_input_q4, config_q4, {\\"recursion_limit\\": 10})\\n",
    "except Interrupt:\\n",
    "    print(\\"  >>> グラフ実行が計画通り中断しました (SQLite)。\\")\\n",
    "except Exception as e:\\n",
    "    print(f\\"  予期せぬエラーでグラフ実行が停止 (SQLite): {e}\\")\\n",
    "\\n",
    "print(\\"\\n2. 中断時のグラフの状態をDBから取得:\\")\\n",
    "interrupted_state_q4 = graph_q4_ch5.get_state(config_q4)\\n",
    "if interrupted_state_q4:\\n",
    "    print(f\\"  現在のステップ名 (中断時): {interrupted_state_q4.values.get('current_step_name')}\\")\\n",
    "    print(f\\"  次の実行予定ノード: {interrupted_state_q4.next}\\")\\n",
    "    print(f\\"  DBに保存されたステップ出力: {interrupted_state_q4.values.get('step_outputs')}\\")\\n",
    "else:\\n",
    "    print(\\"  中断状態がDBから取得できませんでした。\\")\\n",
    "\\n",
    "print(\\"\\n3. グラフに新しい情報を注入してDBから再開:\\")\\n",
    "resume_input_q4 = {\\"messages\\": [AIMessage(content=\\"SQLiteからの再開指示。NodeGammaの処理は中断前に完了しています。\\")]}\\n",
    "final_state_val_q4_stream = None\\n",
    "try:\\n",
    "    for event in graph_q4_ch5.stream(resume_input_q4, config_q4, {\\"recursion_limit\\": 10}):\\n",
    "        print(f\\"  Event (再開後 SQLite): {event}\\")\\n",
    "        if END in event: final_state_val_q4_stream = event[END]\\n",
    "except Exception as e:\\n",
    "    print(f\\"  再開後の実行でエラー (SQLite): {e}\\")\\n",
    "\\n",
    "print(\\"\\n4. 再開後の最終状態をDBから取得:\\")\\n",
    "final_state_val_q4_get = None\\n",
    "final_state_obj_q4 = graph_q4_ch5.get_state(config_q4)\\n",
    "if final_state_obj_q4: final_state_val_q4_get = final_state_obj_q4.values\\n",
    "\\n",
    "final_authoritative_state = final_state_val_q4_stream if final_state_val_q4_stream else final_state_val_q4_get\\n",
    "if final_authoritative_state:\\n",
    "    print(f\\"  最終ステップ名: {final_authoritative_state.get('current_step_name')}\\")\\n",
    "    final_messages_q4 = final_authoritative_state.get('messages', [])\\n",
    "    print(f\\"  最終メッセージ: {final_messages_q4[-1].content if final_messages_q4 else 'N/A'}\\")\\n",
    "    print(f\\"  DBに保存された全ステップ出力: {final_authoritative_state.get('step_outputs')}\\")\\n",
    "else:\\n",
    "    print(\\"  最終状態がDBから取得できませんでした。\\")\\n",
    "\\n",
    "print(f\\"\\nSQLiteデータベースファイル '{db_file}' が作成/更新されているはずです。\\")\\n",
    "``````\\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説004</summary>\\n",
    "\\n",
    "#### この問題のポイント\\n",
    "\\n",
    "*   **`SqliteSaver`:** `langgraph.checkpoint.sqlite.SqliteSaver.from_conn_string(\\"sqlite:///your_db_file.sqlite\\")` のようにして、SQLiteデータベースファイルへの接続を指定し、チェックポインターインスタンスを作成します。指定したファイルが存在しない場合は自動的に作成されます。\\n",
    "*   **データベースへの状態保存:** `MemorySaver` と同様に、`checkpointer` として `SqliteSaver` を指定してグラフをコンパイルし、実行時に `thread_id` を含む `config` を渡すことで、各ステップ完了時のグラフの状態（スナップショット）がSQLiteデータベース内のテーブル（通常は `checkpoints` テーブルなど）にシリアライズされて保存されます。\\n",
    "*   **永続性:** `MemorySaver` との最大の違いは、状態がディスク上のファイルに永続的に保存される点です。これにより、Pythonスクリプトの実行が終了しても状態は残り、後日同じ `thread_id` を使ってグラフの実行を再開できます。また、異なるプロセスやマシンから（適切にDBアクセスを設定すれば）状態を共有することも理論上は可能です（ただし、同時書き込みなどには注意が必要）。\\n",
    "*   **中断と再開:** `interrupt_before` (または `interrupt_after`) と組み合わせた中断・再開のロジックは `MemorySaver` の場合と全く同じです。`get_state(config)` でDBから特定のスレッドIDの状態を取得し、`invoke(new_input, config)` や `stream(new_input, config)` でDBに保存された状態から処理を再開します。\\n",
    "*   **データベーススキーマ:** `SqliteSaver` (や他のDBベースのSaver) は、状態を保存するために特定のテーブル構造（スキーマ）をデータベース内に作成します。例えば、`checkpoints` テーブルには、`thread_id`、`checkpoint`（シリアライズされた状態データ）、タイムスタンプなどが格納されます。LangGraphのドキュメントやソースコードで詳細なスキーマを確認できます。\\n",
    "*   **他のDBバックエンド:** `PostgresSaver` など、他のリレーショナルデータベース用のチェックポインターも同様のコンセプトで動作しますが、接続文字列や必要なライブラリが異なります。\\n",
    "*   **クリーンアップ:** テストやデバッグのために、実行前に既存のSQLiteファイル (`os.remove(db_file)`) を削除しておくと、毎回クリーンな状態からテストを開始できます。\\n",
    "\\n",
    "データベースによる状態の永続化は、本番環境で信頼性の高い、中断・再開可能な長時間実行エージェントやワークフローを構築する上で非常に重要な機能です。\\n",
    "\\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題005: デバッグと可視化 - LangSmith との連携\\n",
    "\\n",
    "複雑なLangGraphグラフのデバッグや実行トレースの可視化には、LangSmith ( [https://smith.langchain.com/](https://smith.langchain.com/) ) が非常に有効です。LangSmithは、LLMアプリケーションの実行詳細（プロンプト、応答、ツール呼び出し、レイテンシ、エラーなど）を記録し、視覚的に追跡・分析するためのプラットフォームです。この問題では、準備セクションで設定したLangSmith連携を有効にしてグラフを実行し、LangSmithのUI上でその実行トレースを確認する方法を学びます。\\n",
    "\\n",
    "*   **学習内容:**\\n",
    "    *   LangSmith連携を有効にするための環境変数の設定（準備セクションで実施済み）。\\n",
    "    *   LangSmithが有効な状態でLangGraphグラフを実行すると、自動的にトレース情報がLangSmithプロジェクトに送信されることの確認。\\n",
    "    *   LangSmithのUIで、実行されたグラフの構造、各ノードの入出力、LLMの思考プロセス、ツール呼び出しの詳細などをどのように確認できるかの概要理解。\\n",
    "    *   （LangSmith UIへのアクセスと操作は演習範囲外とし、コード実行とその結果（トレースが送信されたことの確認）までを対象とします）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005 - グラフ構築\\n",
    "import uuid\\n",
    "from typing import TypedDict, Annotated, List, Optional # (既にインポート済みだが明示)\\n",
    "import time # (既にインポート済みだが明示)\\n",
    "import os # (既にインポート済みだが明示)\\n",
    "from langgraph.graph import StateGraph, END # (既にインポート済みだが明示)\\n",
    "from langgraph.graph.message import add_messages # (既にインポート済みだが明示)\\n",
    "from langchain_core.messages import AIMessage # (既にインポート済みだが明示)\\n",
    "\\n",
    "# --- 1. 状態定義 (既存のものを流用、または新規作成) ---\\n",
    "# ここでは、問題001の StreamingState とノードを再利用してシンプルなグラフで試します。\\n",
    "# class StreamingState(TypedDict): ... (定義済み) (解答例よりコメントアウト)\\n",
    "# def first_step_node(state: StreamingState): ... (定義済み) (解答例よりコメントアウト)\\n",
    "# def second_step_node(state: StreamingState): ... (定義済み) (解答例よりコメントアウト)\\n",
    "# def final_step_node(state: StreamingState): ... (定義済み) (解答例よりコメントアウト)\\n",
    "\\n",
    "# --- 2. グラフ構築 (問題001のグラフを再利用) ---\\n",
    "workflow_q5_ch5 = StateGraph(StreamingState) \\n",
    "workflow_q5_ch5.add_node(\\"step1_langsmith\\", first_step_node) \\n",
    "workflow_q5_ch5.add_node(\\"step2_langsmith\\", second_step_node)\\n",
    "workflow_q5_ch5.add_node(\\"final_step_langsmith\\", final_step_node)\\n",
    "\\n",
    "workflow_q5_ch5.set_entry_point(\\"step1_langsmith\\")\\n",
    "workflow_q5_ch5.add_edge(\\"step1_langsmith\\", \\"step2_langsmith\\")\\n",
    "workflow_q5_ch5.add_edge(\\"step2_langsmith\\", \\"final_step_langsmith\\")\\n",
    "workflow_q5_ch5.add_edge(\\"final_step_langsmith\\", END)\\n",
    "\\n",
    "graph_q5_ch5 = workflow_q5_ch5.compile() # CheckpointerなしでもLangSmithは機能する (解答例より)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005 - グラフ可視化\\n",
    "from IPython.display import Image, display # 解答例より\\n",
    "\\n",
    "try:\\n",
    "    display(Image(graph_q5_ch5.get_graph().draw_png()))\\n",
    "except Exception as e:\\n",
    "    print(f\\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005 - グラフ実行\\n",
    "print(\\"--- LangSmith連携テスト開始 ---\\")\\n",
    "\\n",
    "langsmith_enabled = os.environ.get(\\"LANGCHAIN_TRACING_V2\\", \\"false\\").lower() == \\"true\\" and os.environ.get(\\"LANGCHAIN_API_KEY\\") # 解答例より\\n",
    "\\n",
    "if langsmith_enabled: # 解答例より\\n",
    "    print(f\\"LangSmithトレースが有効です。プロジェクト名: {os.environ.get('LANGCHAIN_PROJECT', '(未設定またはデフォルト)')}\\") # 解答例より\\n",
    "    run_name_prefix = f\\"LangGraph100Knocks_Ch5Q5_{str(uuid.uuid4())[:4]}\\" # 解答例より\\n",
    "    \\n",
    "    invoke_config = {\\n",
    "        \\"run_name\\": f\\"{run_name_prefix}_Invoke\\", # LangSmith UIで表示される実行名 (解答例より)\\n",
    "        \\"metadata\\": {\\"test_scenario\\": \\"Chapter5_Question5_Invoke\\", \\"environment\\": \\"Jupyter\\"} # 解答例より\\n",
    "    }\\n",
    "    initial_input_q5 = {\\"messages\\": [], \\"step_outputs\\": [], \\"current_step_name\\": \\"LangSmith_Test_Initial_Invoke\\"} # 解答例より\\n",
    "    \\n",
    "    print(\\"\\n1. invoke() を使った実行 (LangSmithにトレースが送信されます):\\")\\n",
    "    try:\\n",
    "        final_result_invoke = graph_q5_ch5.invoke(initial_input_q5, config=invoke_config)\\n",
    "        print(\\"  invoke() 実行完了。LangSmithで上記実行名でトレースを確認してください。\\") # 解答例より\\n",
    "    except Exception as e:\\n",
    "        print(f\\"  invoke() 実行中にエラー: {e}\\")\\n",
    "\\n",
    "    stream_config = {\\n",
    "        \\"run_name\\": f\\"{run_name_prefix}_Stream\\", # 解答例より\\n",
    "        \\"metadata\\": {\\"test_scenario\\": \\"Chapter5_Question5_Stream\\"} # 解答例より\\n",
    "    }\\n",
    "    initial_input_stream_q5 = {\\"messages\\": [], \\"step_outputs\\": [], \\"current_step_name\\": \\"LangSmith_Test_Initial_Stream\\"} # 解答例より\\n",
    "    print(\\"\\n2. stream() を使った実行 (こちらもLangSmithにトレースされます):\\")\\n",
    "    try:\\n",
    "        for i, event_chunk in enumerate(graph_q5_ch5.stream(initial_input_stream_q5, config=stream_config, recursion_limit=10)): # 解答例より config 追加, recursion_limit は元の解答から\\n",
    "            if i == 0: print(\\"  ストリーミングイベント受信中...\\") # 解答例より\\n",
    "        print(\\"  stream() 実行完了。LangSmithで上記実行名でトレースを確認してください。\\") # 解答例より\\n",
    "    except Exception as e:\\n",
    "        print(f\\"  stream() 実行中にエラー: {e}\\")\\n",
    "        \\n",
    "    print(\\"\\nLangSmithのUIで、設定したプロジェクト内で上記の実行名（例: LangGraph100Knocks_Ch5Q5_xxxx_Invoke）を持つトレースが記録されているか確認してください。\\")\\n",
    "    print(f\\"LangSmithプロジェクトURLのヒント: {os.environ.get('LANGCHAIN_ENDPOINT', 'https://smith.langchain.com/')}/projects (ログイン後、該当プロジェクトを選択)\\") # 解答例より\\n",
    "\\n",
    "else:\\n",
    "    print(\\"LangSmith連携が無効（環境変数が未設定または不備）なため、この問題の主要なテストはスキップされます。\\") # 解答例より\\n",
    "    print(\\"グラフ自体はローカルで実行されますが、トレースはLangSmithに送信されません。\\")\\n",
    "    initial_input_q5_no_ls = {\\"messages\\": [], \\"step_outputs\\": [], \\"current_step_name\\": \\"NoLangSmith_TestRun\\"} # 解答例より\\n",
    "    result_no_ls = graph_q5_ch5.invoke(initial_input_q5_no_ls)\\n",
    "    print(f\\"LangSmith無効時のローカル実行結果 (最終ステップ名): {result_no_ls.get('current_step_name')}\\")\\n",
    "\\n",
    "print(\\"\\n--- LangSmith連携テスト終了 --- \\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答005</summary>\\n",
    "\\n",
    "``````python\\n",
    "from typing import TypedDict, Annotated, List, Optional\\n",
    "import time\\n",
    "import os\\n",
    "import uuid # uuidをインポート\\n",
    "from langgraph.graph import StateGraph, END\\n",
    "from langgraph.graph.message import add_messages\\n",
    "from langchain_core.messages import AIMessage\\n",
    "from IPython.display import Image, display\\n",
    "\\n",
    "# --- 1. 状態定義 (StreamingStateを再利用) ---\\n",
    "class StreamingState(TypedDict):\\n",
    "    messages: Annotated[List[AIMessage], add_messages]\\n",
    "    current_step_name: Optional[str]\\n",
    "    step_outputs: List[str]\\n",
    "\\n",
    "# --- 2. ノード定義 (問題001のものを再利用) ---\\n",
    "def first_step_node(state: StreamingState) -> dict:\\n",
    "    print(\\"\\n[Step 1: 開始処理 (LangSmith Test)]\\")\\n",
    "    time.sleep(0.05) \\n",
    "    output = \\"LS: 最初のステップ完了。\\"\\n",
    "    return {\\"current_step_name\\": \\"LS_Step1_Done\\", \\"step_outputs\\": state.get(\\"step_outputs\\", []) + [output]}\\n",
    "\\n",
    "def second_step_node(state: StreamingState) -> dict:\\n",
    "    print(\\"\\n[Step 2: 中間処理 (LangSmith Test)]\\")\\n",
    "    time.sleep(0.05)\\n",
    "    output = f\\"LS: 2番目のステップ完了。\\"\\n",
    "    return {\\"current_step_name\\": \\"LS_Step2_Done\\", \\"step_outputs\\": state.get(\\"step_outputs\\", []) + [output]}\\n",
    "\\n",
    "def final_step_node(state: StreamingState) -> dict:\\n",
    "    print(\\"\\n[Step 3: 最終処理 (LangSmith Test)]\\")\\n",
    "    time.sleep(0.05)\\n",
    "    output = \\"LS: 全ステップ完了。最終結果。\\"\\n",
    "    return {\\n",
    "        \\"current_step_name\\": \\"LS_Final_Done\\", \\n",
    "        \\"step_outputs\\": state.get(\\"step_outputs\\", []) + [output], \\n",
    "        \\"messages\\": [AIMessage(content=output)]\\n",
    "    }\\n",
    "\\n",
    "# --- 3. グラフ構築 ---\\n",
    "workflow_q5_ch5 = StateGraph(StreamingState)\\n",
    "workflow_q5_ch5.add_node(\\"step1_langsmith\\", first_step_node) \\n",
    "workflow_q5_ch5.add_node(\\"step2_langsmith\\", second_step_node)\\n",
    "workflow_q5_ch5.add_node(\\"final_step_langsmith\\", final_step_node)\\n",
    "\\n",
    "workflow_q5_ch5.set_entry_point(\\"step1_langsmith\\")\\n",
    "workflow_q5_ch5.add_edge(\\"step1_langsmith\\", \\"step2_langsmith\\")\\n",
    "workflow_q5_ch5.add_edge(\\"step2_langsmith\\", \\"final_step_langsmith\\")\\n",
    "workflow_q5_ch5.add_edge(\\"final_step_langsmith\\", END)\\n",
    "\\n",
    "graph_q5_ch5 = workflow_q5_ch5.compile() # CheckpointerなしでもLangSmithは機能する\\n",
    "try: display(Image(graph_q5_ch5.get_graph().draw_png()))\\n",
    "except Exception as e: print(f\\"グラフ描画失敗: {e}\\")\\n",
    "\\n",
    "# --- 4. LangSmith連携を有効にしてグラフを実行 ---\\n",
    "print(\\"\\n--- LangSmith連携テスト開始 ---\\")\\n",
    "\\n",
    "langsmith_enabled = os.environ.get(\\"LANGCHAIN_TRACING_V2\\", \\"false\\").lower() == \\"true\\" and os.environ.get(\\"LANGCHAIN_API_KEY\\")\\n",
    "\\n",
    "if langsmith_enabled:\\n",
    "    print(f\\"LangSmithトレースが有効です。プロジェクト名: {os.environ.get('LANGCHAIN_PROJECT', '(未設定またはデフォルト)')}\\")\\n",
    "    run_name_prefix = f\\"LangGraph100Knocks_Ch5Q5_{str(uuid.uuid4())[:4]}\\"\\n",
    "    \\n",
    "    invoke_config = {\\n",
    "        # checkpointerがない場合、thread_idはLangSmith側で自動生成されるか、実行ごとに異なるものが使われる\\n",
    "        # 特定の実行をグループ化したい場合は、checkpointerとthread_idを併用するのが良い\\n",
    "        \\"run_name\\": f\\"{run_name_prefix}_Invoke\\", # LangSmith UIで表示される実行名\\n",
    "        \\"metadata\\": {\\"test_scenario\\": \\"Chapter5_Question5_Invoke\\", \\"environment\\": \\"Jupyter\\"}\\n",
    "    }\\n",
    "    initial_input_q5 = {\\"messages\\": [], \\"step_outputs\\": [], \\"current_step_name\\": \\"LangSmith_Test_Initial_Invoke\\"}\\n",
    "    \\n",
    "    print(\\"\\n1. invoke() を使った実行 (LangSmithにトレースが送信されます):\\")\\n",
    "    try:\\n",
    "        final_result_invoke = graph_q5_ch5.invoke(initial_input_q5, config=invoke_config)\\n",
    "        print(\\"  invoke() 実行完了。LangSmithで上記実行名でトレースを確認してください。\\")\\n",
    "    except Exception as e:\\n",
    "        print(f\\"  invoke() 実行中にエラー: {e}\\")\\n",
    "\\n",
    "    stream_config = {\\n",
    "        \\"run_name\\": f\\"{run_name_prefix}_Stream\\",\\n",
    "        \\"metadata\\": {\\"test_scenario\\": \\"Chapter5_Question5_Stream\\"}\\n",
    "    }\\n",
    "    initial_input_stream_q5 = {\\"messages\\": [], \\"step_outputs\\": [], \\"current_step_name\\": \\"LangSmith_Test_Initial_Stream\\"}\\n",
    "    print(\\"\\n2. stream() を使った実行 (こちらもLangSmithにトレースされます):\\")\\n",
    "    try:\\n",
    "        for i, event_chunk in enumerate(graph_q5_ch5.stream(initial_input_stream_q5, config=stream_config, recursion_limit=10)):\\n",
    "            # イベント内容はここでは詳細表示しない（ノード内のprintで確認）\\n",
    "            if i == 0: print(\\"  ストリーミングイベント受信中...\\")\\n",
    "        print(\\"  stream() 実行完了。LangSmithで上記実行名でトレースを確認してください。\\")\\n",
    "    except Exception as e:\\n",
    "        print(f\\"  stream() 実行中にエラー: {e}\\")\\n",
    "        \\n",
    "    print(\\"\\nLangSmithのUIで、設定したプロジェクト内で上記の実行名（例: LangGraph100Knocks_Ch5Q5_xxxx_Invoke）を持つトレースが記録されているか確認してください。\\")\\n",
    "    print(f\\"LangSmithプロジェクトURLのヒント: {os.environ.get('LANGCHAIN_ENDPOINT', 'https://smith.langchain.com/')}/projects (ログイン後、該当プロジェクトを選択)\\")\\n",
    "\\n",
    "else:\\n",
    "    print(\\"LangSmith連携が無効（環境変数が未設定または不備）なため、この問題の主要なテストはスキップされます。\\")\\n",
    "    print(\\"グラフ自体はローカルで実行されますが、トレースはLangSmithに送信されません。\\")\\n",
    "    initial_input_q5_no_ls = {\\"messages\\": [], \\"step_outputs\\": [], \\"current_step_name\\": \\"NoLangSmith_TestRun\\"}\\n",
    "    result_no_ls = graph_q5_ch5.invoke(initial_input_q5_no_ls)\\n",
    "    print(f\\"LangSmith無効時のローカル実行結果 (最終ステップ名): {result_no_ls.get('current_step_name')}\\")\\n",
    "\\n",
    "print(\\"\\n--- LangSmith連携テスト終了 --- \\")\\n",
    "``````\\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説005</summary>\\n",
    "\\n",
    "#### この問題のポイント\\n",
    "\\n",
    "*   **LangSmith連携の有効化:** 準備セクションで説明した通り、以下の環境変数を正しく設定することで、LangChain/LangGraphの実行は自動的にLangSmithにトレースされるようになります。\\n",
    "    *   `LANGCHAIN_API_KEY`: LangSmithのウェブサイトで取得できるAPIキー。\\n",
    "    *   `LANGCHAIN_TRACING_V2=\\"true\\"`: LangSmithへのトレースを有効化します。\\n",
    "    *   `LANGCHAIN_ENDPOINT=\\"https://api.smith.langchain.com\\"`: 通常はこのエンドポイントです。\\n",
    "    *   `LANGCHAIN_PROJECT=\\"your-project-name\\"`: LangSmith上でトレースを整理するためのプロジェクト名を指定します。存在しない場合は自動的に作成されます。\\n",
    "*   **トレースの自動送信:** 上記の環境変数が設定されていれば、`graph.invoke()` や `graph.stream()` を実行するだけで、特別なコード変更なしに実行トレースがLangSmithに送信されます。\\n",
    "*   **実行名 (`run_name`) とメタデータ (`metadata`):** `invoke` や `stream` メソッドの `config` 引数を通じて、`run_name` や `metadata` を指定できます。\\n",
    "    *   `\\"run_name\\": \\"my_descriptive_run_name\\"`: LangSmithのUI上で個々の実行を識別しやすくするために、ユニークで分かりやすい名前を付けることが推奨されます。UUIDなどを組み合わせると良いでしょう。\\n",
    "    *   `\\"metadata\\": {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"}`: 実行に関する追加情報（例: テストケース名、環境、バージョンなど）をキーと値のペアで付与できます。これにより、後でトレースをフィルタリングしたり分析したりする際に役立ちます。\\n",
    "*   **LangSmith UIでの確認:**\\n",
    "    *   コードを実行した後、LangSmithのウェブサイトにログインし、指定したプロジェクトを選択します。\\n",
    "    *   実行リストの中に、`run_name` で指定した名前（または自動生成された名前）の実行トレースが見つかるはずです。\\n",
    "    *   トレース詳細画面では、グラフ全体の構造、各ノードの実行順序、各ノードの入力と出力（状態の差分）、LLMノードの場合は使用されたプロンプトと生成された応答、ツールノードの場合はツール呼び出しの引数と結果、各ステップの実行時間（レイテンシ）などが視覚的に表示されます。\\n",
    "    *   エラーが発生した場合は、そのエラー情報も記録され、デバッグに役立ちます。\\n",
    "*   **チェックポインターとの関係:** LangSmithは主に実行の「トレース」と「デバッグ」を目的としており、グラフ状態の「永続化と再開」はチェックポインター（`MemorySaver`, `SqliteSaver`など）の役割です。両者は独立して使用することも、組み合わせて使用することも可能です。LangSmith自体も状態の永続化機能（`LangSmithSaver`）を提供していますが、この問題ではトレース機能の側面に焦点を当てています。\\n",
    "\\n",
    "LangSmithは、複雑化するLLMアプリケーションやエージェントシステムの開発・運用において、その挙動を理解し、問題を特定し、パフォーマンスを改善するための強力なツールです。LangGraphと組み合わせることで、グラフベースのアプリケーションの内部動作を詳細に追跡できます。\\n",
    "\\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題006: 状態のカスタム更新ロジック (Annotated と Reducer)\\n",
    "\\n",
    "LangGraphの状態更新は、デフォルトではノードが返した辞書のキーと値をそのまま状態にマージ（上書き）します。しかし、より複雑な状態更新ロジックが必要な場合があります。例えば、リスト型の状態キーに対して要素を追加するのではなく、常に固定長のリストを維持したい、あるいは特定の数値キーを常に合計したい、などです。`typing.Annotated` とカスタムの `Reducer` 関数を組み合わせることで、このような独自の更新セマンティクスを状態キーに定義できます。\\n",
    "\\n",
    "*   **学習内容:**\\n",
    "    *   `typing.Annotated` を使って、`TypedDict`で定義する状態のキーにカスタムメタデータを付与する方法。\\n",
    "    *   `Reducer(lambda current_value, new_value: ...)` を使って、状態キーが更新される際の具体的なロジック（現在の値と新しい値をどう組み合わせるか）を定義する方法。\\n",
    "    *   例として、カウンター（常に値を加算する）、固定長ログ（最新N件のみ保持する）、フラグ（一度Trueになったら変わらない）など、いくつかのカスタム更新ロジックを持つ状態キーを実装してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄006 - グラフ構築\\n",
    "from typing import TypedDict, Annotated, List, Optional, Callable, Dict, Any # Dict, Any を追加 (解答例より)\\n",
    "from langgraph.graph import StateGraph, END # (既にインポート済みだが明示)\\n",
    "from langgraph.graph.state import Reducer \\n",
    "from collections import deque\\n",
    "from uuid import uuid4 # 解答例より\\n",
    "from langgraph.checkpoint.memory import MemorySaver # 解答例より\\n",
    "\\n",
    "# --- 1. カスタムReducer関数の定義 ---\\n",
    "def sum_reducer_fn(current_value: Optional[int], new_value: Optional[int]) -> Optional[int]: # 解答例より Optional[int] に変更\\n",
    "    if new_value is None: return current_value # 解答例より\\n",
    "    return (current_value or 0) + new_value\\n",
    "sum_reducer = Reducer(sum_reducer_fn)\\n",
    "\\n",
    "class FixedLengthListReducer:\\n",
    "    def __init__(self, max_len: int):\\n",
    "        self.max_len = max_len\\n",
    "    def __call__(self, current_value: Optional[List[str]], new_item: Optional[str]) -> Optional[List[str]]: # 解答例より Optional に変更\\n",
    "        if new_item is None: return current_value # 解答例より\\n",
    "        current_deque = deque(current_value or [], maxlen=self.max_len)\\n",
    "        current_deque.append(new_item)\\n",
    "        return list(current_deque)\\n",
    "fixed_log_reducer = FixedLengthListReducer(max_len=3) \\n",
    "\\n",
    "def once_true_flag_reducer_fn(current_value: Optional[bool], new_value: Optional[bool]) -> Optional[bool]: # 解答例より Optional[bool] に変更\\n",
    "    if new_value is None: return current_value # 解答例より\\n",
    "    return (current_value or False) or new_value \\n",
    "once_true_flag_reducer = Reducer(once_true_flag_reducer_fn)\\n",
    "\\n",
    "# --- 2. カスタム更新ロジックを持つ状態定義 ---\\n",
    "class CustomUpdateState(TypedDict):\\n",
    "    total_updates: Annotated[Optional[int], sum_reducer]  # sum_reducer を適用 (解答例より Optional追加)\\n",
    "    recent_logs: Annotated[Optional[List[str]], fixed_log_reducer] # fixed_log_reducer を適用 (解答例より Optional追加)\\n",
    "    action_triggered_flag: Annotated[Optional[bool], once_true_flag_reducer] # once_true_flag_reducer を適用 (解答例より Optional追加)\\n",
    "    last_message: Optional[str]\\n",
    "\\n",
    "# --- 3. ノード定義 ---\\n",
    "def update_generating_node(state: CustomUpdateState, update_value: int, log_message: str, trigger_action: Optional[bool] = None) -> dict: # 解答例より trigger_action を Optional に変更\\n",
    "    print(f\\"\\n[更新生成ノード] update_value={update_value}, log='{log_message}', trigger={trigger_action}\\")\\n",
    "    update_dict: Dict[str, Any] = { # 解答例より型アノテーション追加\\n",
    "        \\"total_updates\\": update_value,\\n",
    "        \\"recent_logs\\": log_message,\\n",
    "        \\"last_message\\": f\\"Log: {log_message} (Update val: {update_value}) Trigger: {trigger_action}\\" # 解答例よりメッセージ変更\\n",
    "    }\\n",
    "    if trigger_action is not None: # 解答例より\\n",
    "        update_dict[\\"action_triggered_flag\\"] = trigger_action\\n",
    "    \\n",
    "    print(f\\"  -> ノードからの戻り値: {update_dict}\\")\\n",
    "    return update_dict\\n",
    "\\n",
    "def node_A(state: CustomUpdateState): return update_generating_node(state, 10, \\"ログA from NodeA\\", trigger_action=False)\\n",
    "def node_B(state: CustomUpdateState): return update_generating_node(state, 5, \\"ログB from NodeB\\", trigger_action=True)\\n",
    "def node_C(state: CustomUpdateState): return update_generating_node(state, 7, \\"ログC from NodeC (flagはTrueのままのはず)\\", trigger_action=False) \\n",
    "def node_D(state: CustomUpdateState): return update_generating_node(state, 3, \\"ログD from NodeD (ログ溢れ確認)\\") \\n",
    "\\n",
    "# --- 4. グラフ構築 ---\\n",
    "workflow_q6_ch5 = StateGraph(CustomUpdateState)\\n",
    "workflow_q6_ch5.add_node(\\"A\\", node_A)\\n",
    "workflow_q6_ch5.add_node(\\"B\\", node_B)\\n",
    "workflow_q6_ch5.add_node(\\"C\\", node_C)\\n",
    "workflow_q6_ch5.add_node(\\"D\\", node_D)\\n",
    "\\n",
    "workflow_q6_ch5.set_entry_point(\\"A\\")\\n",
    "workflow_q6_ch5.add_edge(\\"A\\", \\"B\\")\\n",
    "workflow_q6_ch5.add_edge(\\"B\\", \\"C\\")\\n",
    "workflow_q6_ch5.add_edge(\\"C\\", \\"D\\")\\n",
    "workflow_q6_ch5.add_edge(\\"D\\", END)\\n",
    "\\n",
    "custom_reducer_memory = MemorySaver() # 解答例より\\n",
    "graph_q6_ch5 = workflow_q6_ch5.compile(checkpointer=custom_reducer_memory) # 解答例より checkpointer 追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄006 - グラフ可視化\\n",
    "from IPython.display import Image, display # 解答例より\\n",
    "\\n",
    "try:\\n",
    "    display(Image(graph_q6_ch5.get_graph().draw_png()))\\n",
    "except Exception as e:\\n",
    "    print(f\\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄006 - グラフ実行\\n",
    "print(\\"--- カスタム状態更新ロジックテスト開始 ---\\")\\n",
    "initial_state_q6 = {\\n",
    "    \\"last_message\\": \\"初期メッセージ\\", \\n",
    "    \\"total_updates\\": None, \\n",
    "    \\"recent_logs\\": None,\\n",
    "    \\"action_triggered_flag\\": None \\n",
    "} # 解答例より初期値設定\\n",
    "thread_id_q6 = {\\"configurable\\": {\\"thread_id\\": f\\"custom-reducer-test-{uuid4()[:4]}\\"}}}\\n",
    "\\n",
    "print(\\"グラフ実行中の各ノード完了後の状態:\\")\\n",
    "final_event_data_q6 = None # 解答例より\\n",
    "for i, event_chunk in enumerate(graph_q6_ch5.stream(initial_state_q6, config=thread_id_q6, recursion_limit=10)): # 解答例より recursion_limit は元の解答から\\n",
    "    node_name = list(event_chunk.keys())[0]\\n",
    "    print(f\\"\\n--- イベント {i+1}: ノード「{node_name}」完了後 ---\\")\\n",
    "    current_full_state = graph_q6_ch5.get_state(thread_id_q6) \\n",
    "    if current_full_state:\\n",
    "        print(f\\"  total_updates: {current_full_state.values.get('total_updates')}\\")\\n",
    "        print(f\\"  recent_logs: {current_full_state.values.get('recent_logs')}\\")\\n",
    "        print(f\\"  action_triggered_flag: {current_full_state.values.get('action_triggered_flag')}\\")\\n",
    "        print(f\\"  last_message: {current_full_state.values.get('last_message')}\\")\\n",
    "    if node_name == END or END in event_chunk : # 解答例より END in event_chunk 追加\\n",
    "        final_event_data_q6 = event_chunk.get(END, current_full_state.values if current_full_state else None) # 解答例より\\n",
    "        break \\n",
    "\\n",
    "print(\\"\\n--- カスタム状態更新ロジックテスト終了 (最終状態) --- \\") # 解答例より\\n",
    "if final_event_data_q6: # 解答例より\\n",
    "    print(f\\"  total_updates: {final_event_data_q6.get('total_updates')}\\")\\n",
    "    print(f\\"  recent_logs: {final_event_data_q6.get('recent_logs')}\\")\\n",
    "    print(f\\"  action_triggered_flag: {final_event_data_q6.get('action_triggered_flag')}\\")\\n",
    "    print(f\\"  last_message: {final_event_data_q6.get('last_message')}\\")\\n",
    "else:\\n",
    "    print(\\"最終状態が取得できませんでした。\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答006</summary>\\n",
    "\\n",
    "``````python\\n",
    "from typing import TypedDict, Annotated, List, Optional, Callable\\n",
    "from langgraph.graph import StateGraph, END\\n",
    "from langgraph.graph.state import Reducer # Reducer をインポート\\n",
    "from collections import deque # 固定長リストのためにdequeを使用\\n",
    "from IPython.display import Image, display\\n",
    "from uuid import uuid4\\n",
    "import time\\n",
    "\\n",
    "# --- 1. カスタムReducer関数の定義 ---\\n",
    "def sum_reducer_fn(current_value: Optional[int], new_value: Optional[int]) -> Optional[int]:\\n",
    "    if new_value is None: return current_value # 新しい値がなければ現在値を維持\\n",
    "    return (current_value or 0) + new_value\\n",
    "sum_reducer = Reducer(sum_reducer_fn)\\n",
    "\\n",
    "class FixedLengthListReducer:\\n",
    "    def __init__(self, max_len: int):\\n",
    "        self.max_len = max_len\\n",
    "    def __call__(self, current_value: Optional[List[str]], new_item: Optional[str]) -> Optional[List[str]]:\\n",
    "        if new_item is None: return current_value\\n",
    "        current_deque = deque(current_value or [], maxlen=self.max_len)\\n",
    "        current_deque.append(new_item)\\n",
    "        return list(current_deque)\\n",
    "fixed_log_reducer = FixedLengthListReducer(max_len=3)\\n",
    "\\n",
    "def once_true_flag_reducer_fn(current_value: Optional[bool], new_value: Optional[bool]) -> Optional[bool]:\\n",
    "    if new_value is None: return current_value\\n",
    "    return (current_value or False) or new_value\\n",
    "once_true_flag_reducer = Reducer(once_true_flag_reducer_fn)\\n",
    "\\n",
    "# --- 2. カスタム更新ロジックを持つ状態定義 ---\\n",
    "class CustomUpdateState(TypedDict):\\n",
    "    total_updates: Annotated[Optional[int], sum_reducer]  \\n",
    "    recent_logs: Annotated[Optional[List[str]], fixed_log_reducer] \\n",
    "    action_triggered_flag: Annotated[Optional[bool], once_true_flag_reducer] \\n",
    "    last_message: Optional[str]\\n",
    "\\n",
    "# --- 3. ノード定義 ---\\n",
    "def update_generating_node(state: CustomUpdateState, update_value: int, log_message: str, trigger_action: Optional[bool] = None) -> dict:\\n",
    "    print(f\\"\\n[更新生成ノード] update_value={update_value}, log='{log_message}', trigger={trigger_action}\\")\\n",
    "    update_dict: Dict[str, Any] = {\\n",
    "        \\"total_updates\\": update_value,\\n",
    "        \\"recent_logs\\": log_message,\\n",
    "        \\"last_message\\": f\\"Log: {log_message} (Update val: {update_value}) Trigger: {trigger_action}\\"\\n",
    "    }\\n",
    "    if trigger_action is not None: # trigger_actionが指定された場合のみキーを含める\\n",
    "        update_dict[\\"action_triggered_flag\\"] = trigger_action\\n",
    "    \\n",
    "    print(f\\"  -> ノードからの戻り値: {update_dict}\\")\\n",
    "    return update_dict\\n",
    "\\n",
    "def node_A(state: CustomUpdateState): return update_generating_node(state, 10, \\"ログA from NodeA\\", trigger_action=False)\\n",
    "def node_B(state: CustomUpdateState): return update_generating_node(state, 5, \\"ログB from NodeB\\", trigger_action=True)\\n",
    "def node_C(state: CustomUpdateState): return update_generating_node(state, 7, \\"ログC from NodeC (flagはTrueのままのはず)\\", trigger_action=False) # trigger_action=Falseでも一度TrueならTrueのはず\\n",
    "def node_D(state: CustomUpdateState): return update_generating_node(state, 3, \\"ログD from NodeD (ログ溢れ確認)\\") # trigger_actionは指定なし(None)\\n",
    "\\n",
    "# --- 4. グラフ構築 ---\\n",
    "workflow_q6_ch5 = StateGraph(CustomUpdateState)\\n",
    "workflow_q6_ch5.add_node(\\"A\\", node_A)\\n",
    "workflow_q6_ch5.add_node(\\"B\\", node_B)\\n",
    "workflow_q6_ch5.add_node(\\"C\\", node_C)\\n",
    "workflow_q6_ch5.add_node(\\"D\\", node_D)\\n",
    "\\n",
    "workflow_q6_ch5.set_entry_point(\\"A\\")\\n",
    "workflow_q6_ch5.add_edge(\\"A\\", \\"B\\")\\n",
    "workflow_q6_ch5.add_edge(\\"B\\", \\"C\\")\\n",
    "workflow_q6_ch5.add_edge(\\"C\\", \\"D\\")\\n",
    "workflow_q6_ch5.add_edge(\\"D\\", END)\\n",
    "\\n",
    "# MemorySaverを使って状態遷移を追跡可能にする (Reducerの動作確認のため)\\n",
    "custom_reducer_memory = MemorySaver()\\n",
    "graph_q6_ch5 = workflow_q6_ch5.compile(checkpointer=custom_reducer_memory)\\n",
    "try: display(Image(graph_q6_ch5.get_graph().draw_png()))\\n",
    "except Exception as e: print(f\\"グラフ描画失敗: {e}\\")\\n",
    "\\n",
    "# --- 5. 実行と状態変化の確認 ---\\n",
    "print(\\"\\n--- カスタム状態更新ロジックテスト開始 ---\\")\\n",
    "initial_state_q6 = {\\n",
    "    \\"last_message\\": \\"初期メッセージ\\", \\n",
    "    # Reducerが適用されるキーは、初期値がNoneでもReducer関数内で適切に処理される想定\\n",
    "    \\"total_updates\\": None, \\n",
    "    \\"recent_logs\\": None,\\n",
    "    \\"action_triggered_flag\\": None \\n",
    "}\\n",
    "thread_id_q6 = {\\"configurable\\": {\\"thread_id\\": f\\"custom-reducer-test-{uuid4()[:4]}\\"}}}\\n",
    "\\n",
    "print(\\"グラフ実行中の各ノード完了後の状態:\\")\\n",
    "final_event_data_q6 = None\\n",
    "for i, event_chunk in enumerate(graph_q6_ch5.stream(initial_state_q6, config=thread_id_q6, recursion_limit=10)):\\n",
    "    node_name = list(event_chunk.keys())[0]\\n",
    "    print(f\\"\\n--- イベント {i+1}: ノード「{node_name}」完了後 ---\\")\\n",
    "    current_full_state = graph_q6_ch5.get_state(thread_id_q6) \\n",
    "    if current_full_state:\\n",
    "        print(f\\"  total_updates: {current_full_state.values.get('total_updates')}\\")\\n",
    "        print(f\\"  recent_logs: {current_full_state.values.get('recent_logs')}\\")\\n",
    "        print(f\\"  action_triggered_flag: {current_full_state.values.get('action_triggered_flag')}\\")\\n",
    "        print(f\\"  last_message: {current_full_state.values.get('last_message')}\\")\\n",
    "    if node_name == END or END in event_chunk : \\n",
    "        final_event_data_q6 = event_chunk.get(END, current_full_state.values if current_full_state else None)\\n",
    "        break \\n",
    "\\n",
    "print(\\"\\n--- カスタム状態更新ロジックテスト終了 (最終状態) --- \\")\\n",
    "if final_event_data_q6:\\n",
    "    print(f\\"  total_updates: {final_event_data_q6.get('total_updates')}\\")\\n",
    "    print(f\\"  recent_logs: {final_event_data_q6.get('recent_logs')}\\")\\n",
    "    print(f\\"  action_triggered_flag: {final_event_data_q6.get('action_triggered_flag')}\\")\\n",
    "    print(f\\"  last_message: {final_event_data_q6.get('last_message')}\\")\\n",
    "else:\\n",
    "    print(\\"最終状態が取得できませんでした。\\")\\n",
    "``````\\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説006</summary>\\n",
    "\\n",
    "#### この問題のポイント\\n",
    "\\n",
    "*   **`typing.Annotated` と `Reducer`:**\\n",
    "    *   状態クラス (`CustomUpdateState`) のキーを定義する際に、`Annotated[<型>, <Reducerインスタンス>]` という形式を使います。これにより、そのキーが更新される際に、指定した `Reducer` のロジックが適用されます。\\n",
    "    *   `Reducer` は、`Callable[[Optional[Any], Any], Any]` 型の関数（または `__call__` メソッドを持つクラスインスタンス）を引数に取ります。この関数は `(現在の値, 新しい値) -> 更新後の値` というシグネチャを持ちます。\\n",
    "*   **カスタムReducerの例:**\\n",
    "    *   `sum_reducer`: 新しい値を現在の値に単純に加算します。`current_value` が `None`（初回など）の場合は0として扱います。\\n",
    "    *   `fixed_log_reducer` (クラスとして実装): `collections.deque(maxlen=N)` を利用して、リストの長さを常に最大N件に保ちます。新しいログが追加されると、古いものから順に押し出されます。\\n",
    "    *   `once_true_flag_reducer`: 一度でも `True` が設定されたら、その後 `False` が新しい値として渡されても `True` のまま維持します。\\n",
    "*   **ノードからの戻り値とReducerの動作:**\\n",
    "    *   ノード（例: `update_generating_node`）は、更新したい状態キーと「新しい値」の辞書を返します。\\n",
    "    *   LangGraphは、この「新しい値」と、そのキーの「現在の値」を、`Annotated` で指定された `Reducer` 関数に渡します。\\n",
    "    *   `Reducer` 関数の戻り値が、そのキーの新しい状態値として実際に保存されます。\\n",
    "    *   もしノードが `Reducer` が適用されたキーを返さなかった場合、そのキーの `Reducer` は呼び出されず、状態も更新されません（`Reducer` の設計で `new_value` が `None` の場合の挙動を定義しておくことが重要です）。\\n",
    "*   **状態の初期値:** `Reducer` が適用されるキーの初期値は、`TypedDict` の定義では通常指定しません。`Reducer` 関数自体が、`current_value` が `None` である（つまり初回更新時）場合の初期化ロジックを持つように設計します（例: `current_value or 0`）。\\n",
    "*   **ユースケース:**\\n",
    "    *   集計値の管理（例: 合計スコア、処理回数カウンター）。\\n",
    "    *   有限長の履歴保持（例: 最新N件のメッセージ、直近のエラーログ）。\\n",
    "    *   状態フラグの特殊な更新（例: 一度セットされたらクリアされないフラグ、特定条件下でのみトグルするフラグ）。\\n",
    "    *   より複雑なデータ構造（例: 辞書やネストしたオブジェクト）の部分的な更新やマージ。\\n",
    "\\n",
    "`Reducer` を使うことで、LangGraphの状態管理をより柔軟かつ宣言的に行うことができ、ノード側のロジックを状態更新の詳細から分離するのに役立ちます。\\n",
    "\\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題007: 非同期ノードによるI/O処理の効率化\\n",
    "\\n",
    "グラフ内のノードが外部API呼び出しやファイル読み書きのようなI/Oバウンドな処理を含む場合、それらを同期的に実行すると、I/O待ち時間中にグラフ全体の処理がブロックされてしまいます。Pythonの`asyncio`と`async def`を使ってノードを非同期関数として定義することで、I/O待ち時間中に他のタスク（グラフ内の他の非同期ノードや、グラフ実行フレームワーク自体の処理）にCPU時間を譲ることができ、全体の効率が向上する可能性があります。\\n",
    "\\n",
    "*   **学習内容:**\\n",
    "    *   `async def` を使って非同期ノード関数を定義する方法。\\n",
    "    *   非同期ノード内で `await asyncio.sleep()` や非同期対応のHTTPクライアント（例: `aiohttp`）などを使って、ノンブロッキングなI/O処理をシミュレートまたは実行する方法。\\n",
    "    *   非同期ノードを含むグラフをコンパイルし、`graph.ainvoke()` や `graph.astream()` を使って非同期に実行する方法。\\n",
    "    *   （注意）LangGraphのグラフ自体が自動的にノードを並列実行するわけではありません。`async`ノードは、主にそのノード自身のI/O待ち時間を効率的に使うためのものであり、グラフ全体の並列性を高めるにはファンアウト・ファンインなどの設計が必要です。ただし、複数の非同期ツールコールを一度に行う場合などには効果があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄007 - グラフ構築\\n",
    "import asyncio\\n",
    "from typing import TypedDict, Annotated, List, Optional # (既にインポート済みだが明示)\\n",
    "import time # (既にインポート済みだが明示)\\n",
    "from langgraph.graph import StateGraph, END # (既にインポート済みだが明示)\\n",
    "from langgraph.graph.message import add_messages # (既にインポート済みだが明示)\\n",
    "from langchain_core.messages import AIMessage # (既にインポート済みだが明示)\\n",
    "from uuid import uuid4 # (既にインポート済みだが明示)\\n",
    "\\n",
    "# --- 1. 状態定義 (StreamingStateを再利用) ---\\n",
    "# class StreamingState(TypedDict): ... (解答例よりコメントアウト)\\n",
    "\\n",
    "# --- 2. 非同期ノードの定義 ---\\n",
    "async def async_io_node_A(state: StreamingState) -> dict:\\n",
    "    task_name = \\"非同期I/O処理A (例: API呼び出し1)\\"\\n",
    "    print(f\\"\\n[{task_name}] 開始 (0.5秒待機シミュレーション)\\")\\n",
    "    await asyncio.sleep(0.5) # 0.5秒の非同期スリープ (解答例より)\\n",
    "    result = f\\"{task_name} 完了。データ「AlphaContent」取得。\\"\\n",
    "    print(f\\"  -> {task_name} 結果: {result}\\")\\n",
    "    current_outputs = state.get(\\"step_outputs\\", [])\\n",
    "    return {\\"current_step_name\\": \\"AsyncNodeA_Done\\", \\"step_outputs\\": current_outputs + [result]}\\n",
    "\\n",
    "async def async_io_node_B(state: StreamingState) -> dict:\\n",
    "    task_name = \\"非同期I/O処理B (例: DBクエリ)\\"\\n",
    "    print(f\\"\\n[{task_name}] 開始 (0.3秒待機シミュレーション)\\")\\n",
    "    await asyncio.sleep(0.3) # 0.3秒の非同期スリープ (解答例より)\\n",
    "    result = f\\"{task_name} 完了。データ「BetaPayload」取得。\\"\\n",
    "    print(f\\"  -> {task_name} 結果: {result}\\")\\n",
    "    current_outputs = state.get(\\"step_outputs\\", [])\\n",
    "    return {\\"current_step_name\\": \\"AsyncNodeB_Done\\", \\"step_outputs\\": current_outputs + [result]}\\n",
    "\\n",
    "async def async_finalizing_node(state: StreamingState) -> dict:\\n",
    "    print(\\"\\n[非同期最終処理ノード] 結果を統合しています...\\")\\n",
    "    await asyncio.sleep(0.1)\\n",
    "    all_outputs = state.get(\\"step_outputs\\", [])\\n",
    "    final_summary = f\\"非同期処理完了。全{len(all_outputs)}ステップの結果: {'; '.join(all_outputs)}\\"\\n",
    "    print(f\\"  -> 最終サマリー: {final_summary}\\")\\n",
    "    return {\\"current_step_name\\": \\"AsyncFinal_Done\\", \\"messages\\": [AIMessage(content=final_summary)]}\\n",
    "\\n",
    "# --- 3. グラフ構築 (非同期ノードを含む) ---\\n",
    "workflow_q7_ch5 = StateGraph(StreamingState)\\n",
    "workflow_q7_ch5.add_node(\\"async_A\\", async_io_node_A) \\n",
    "workflow_q7_ch5.add_node(\\"async_B\\", async_io_node_B)\\n",
    "workflow_q7_ch5.add_node(\\"async_finalizer\\", async_finalizing_node)\\n",
    "\\n",
    "workflow_q7_ch5.set_entry_point(\\"async_A\\")\\n",
    "\\n",
    "workflow_q7_ch5.add_edge(\\"async_A\\", \\"async_B\\")\\n",
    "workflow_q7_ch5.add_edge(\\"async_B\\", \\"async_finalizer\\")\\n",
    "workflow_q7_ch5.add_edge(\\"async_finalizer\\", END)\\n",
    "\\n",
    "graph_q7_ch5 = workflow_q7_ch5.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄007 - グラフ可視化\\n",
    "from IPython.display import Image, display # 解答例より\\n",
    "\\n",
    "try:\\n",
    "    display(Image(graph_q7_ch5.get_graph().draw_png()))\\n",
    "except Exception as e:\\n",
    "    print(f\\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄007 - グラフ実行\\n",
    "async def main_async_runner_q7():\\n",
    "    print(\\"\\n--- 非同期ノードテスト開始 (逐次実行) ---\\")\\n",
    "    initial_input_q7 = {\\"messages\\": [], \\"step_outputs\\": [], \\"current_step_name\\": \\"Async_Initial\\"}\\n",
    "    thread_id_q7_ainvoke = {\\"configurable\\": {\\"thread_id\\": f\\"async-test-ainvoke-{uuid4()[:4]}\\"}} # 解答例より\\n",
    "    thread_id_q7_astream = {\\"configurable\\": {\\"thread_id\\": f\\"async-test-astream-{uuid4()[:4]}\\"}} # 解答例より\\n",
    "\\n",
    "    print(\\"\\n1. ainvoke() を使った非同期実行:\\")\\n",
    "    start_time_ainvoke = time.perf_counter() # 解答例より time.perf_counter() に変更\\n",
    "    final_state_ainvoke = await graph_q7_ch5.ainvoke(initial_input_q7, config=thread_id_q7_ainvoke) # graph_q7_ch5 に修正 (解答例より)\\n",
    "    end_time_ainvoke = time.perf_counter() # 解答例より time.perf_counter() に変更\\n",
    "    print(f\\"  ainvoke() 実行完了。所要時間: {end_time_ainvoke - start_time_ainvoke:.3f}秒\\")\\n",
    "    if final_state_ainvoke:\\n",
    "        print(f\\"    最終ステップ名: {final_state_ainvoke.get('current_step_name')}\\")\\n",
    "        final_messages_ainvoke = final_state_ainvoke.get('messages', []) # 解答例より\\n",
    "        print(f\\"    最終メッセージ: {final_messages_ainvoke[-1].content if final_messages_ainvoke else 'N/A'}\\") # 解答例より\\n",
    "\\n",
    "    print(\\"\\n2. astream() を使った非同期ストリーミング実行:\\")\\n",
    "    start_time_astream = time.perf_counter() # 解答例より time.perf_counter() に変更\\n",
    "    async for i, event_chunk in enumerate(graph_q7_ch5.astream(initial_input_q7, config=thread_id_q7_astream, recursion_limit=10)): # graph_q7_ch5 に修正 (解答例より)\\n",
    "        node_completed = list(event_chunk.keys())[0] # 解答例より\\n",
    "        print(f\\"  Async Stream Event {i+1}: ノード「{node_completed}」完了。更新: {event_chunk[node_completed]}\\") # 解答例より\\n",
    "    end_time_astream = time.perf_counter() # 解答例より time.perf_counter() に変更\\n",
    "    print(f\\"  astream() 実行完了。所要時間: {end_time_astream - start_time_astream:.3f}秒\\")\\n",
    "    \\n",
    "    print(\\"\\n--- 非同期ノードテスト終了 ---\\")\\n",
    "\\n",
    "print(\\"非同期実行関数 main_async_runner_q7 が定義されました。\\")\\n",
    "print(\\"Jupyter Notebookでこのセルを実行後、新しいセルで await main_async_runner_q7() を実行してください。\\")\\n",
    "print(\\"(または、スクリプトとして実行する場合は if __name__ == '__main__': asyncio.run(main_async_runner_q7()) を使います)\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答007</summary>\\n",
    "\\n",
    "``````python\\n",
    "import asyncio\\n",
    "from typing import TypedDict, Annotated, List, Optional\\n",
    "import time\\n",
    "from langgraph.graph import StateGraph, END\\n",
    "from langgraph.graph.message import add_messages\\n",
    "from langchain_core.messages import AIMessage\\n",
    "from IPython.display import Image, display\\n",
    "from uuid import uuid4\\n",
    "\\n",
    "# --- 1. 状態定義 (StreamingStateを再利用) ---\\n",
    "class StreamingState(TypedDict):\\n",
    "    messages: Annotated[List[AIMessage], add_messages]\\n",
    "    current_step_name: Optional[str]\\n",
    "    step_outputs: List[str]\\n",
    "\\n",
    "# --- 2. 非同期ノードの定義 ---\\n",
    "async def async_io_node_A(state: StreamingState) -> dict:\\n",
    "    task_name = \\"非同期I/O処理A (例: API呼び出し1)\\"\\n",
    "    print(f\\"\\n[{task_name}] 開始 (0.5秒待機シミュレーション)\\")\\n",
    "    await asyncio.sleep(0.5) \\n",
    "    result = f\\"{task_name} 完了。データ「AlphaContent」取得。\\"\\n",
    "    print(f\\"  -> {task_name} 結果: {result}\\")\\n",
    "    current_outputs = state.get(\\"step_outputs\\", [])\\n",
    "    return {\\"current_step_name\\": \\"AsyncNodeA_Done\\", \\"step_outputs\\": current_outputs + [result]}\\n",
    "\\n",
    "async def async_io_node_B(state: StreamingState) -> dict:\\n",
    "    task_name = \\"非同期I/O処理B (例: DBクエリ)\\"\\n",
    "    print(f\\"\\n[{task_name}] 開始 (0.3秒待機シミュレーション)\\")\\n",
    "    await asyncio.sleep(0.3) \\n",
    "    result = f\\"{task_name} 完了。データ「BetaPayload」取得。\\"\\n",
    "    print(f\\"  -> {task_name} 結果: {result}\\")\\n",
    "    current_outputs = state.get(\\"step_outputs\\", [])\\n",
    "    return {\\"current_step_name\\": \\"AsyncNodeB_Done\\", \\"step_outputs\\": current_outputs + [result]}\\n",
    "\\n",
    "async def async_finalizing_node(state: StreamingState) -> dict:\\n",
    "    print(\\"\\n[非同期最終処理ノード] 結果を統合しています...\\")\\n",
    "    await asyncio.sleep(0.1)\\n",
    "    all_outputs = state.get(\\"step_outputs\\", [])\\n",
    "    final_summary = f\\"非同期処理完了。全{len(all_outputs)}ステップの結果: {'; '.join(all_outputs)}\\"\\n",
    "    print(f\\"  -> 最終サマリー: {final_summary}\\")\\n",
    "    return {\\"current_step_name\\": \\"AsyncFinal_Done\\", \\"messages\\": [AIMessage(content=final_summary)]}\\n",
    "\\n",
    "# --- 3. グラフ構築 (非同期ノードを含む) ---\\n",
    "workflow_q7_ch5 = StateGraph(StreamingState)\\n",
    "workflow_q7_ch5.add_node(\\"async_A\\", async_io_node_A)\\n",
    "workflow_q7_ch5.add_node(\\"async_B\\", async_io_node_B)\\n",
    "workflow_q7_ch5.add_node(\\"async_finalizer\\", async_finalizing_node)\\n",
    "\\n",
    "workflow_q7_ch5.set_entry_point(\\"async_A\\")\\n",
    "workflow_q7_ch5.add_edge(\\"async_A\\", \\"async_B\\")\\n",
    "workflow_q7_ch5.add_edge(\\"async_B\\", \\"async_finalizer\\")\\n",
    "workflow_q7_ch5.add_edge(\\"async_finalizer\\", END)\\n",
    "\\n",
    "graph_q7_ch5 = workflow_q7_ch5.compile()\\n",
    "try: display(Image(graph_q7_ch5.get_graph().draw_png()))\\n",
    "except Exception as e: print(f\\"グラフ描画失敗: {e}\\")\\n",
    "\\n",
    "# --- 4. 非同期実行 (ainvoke / astream) ---\\n",
    "async def main_async_runner_q7():\\n",
    "    print(\\"\\n--- 非同期ノードテスト開始 (逐次実行) ---\\")\\n",
    "    initial_input_q7 = {\\"messages\\": [], \\"step_outputs\\": [], \\"current_step_name\\": \\"Async_Initial\\"}\\n",
    "    thread_id_q7_ainvoke = {\\"configurable\\": {\\"thread_id\\": f\\"async-test-ainvoke-{uuid4()[:4]}\\"}}}\\n",
    "    thread_id_q7_astream = {\\"configurable\\": {\\"thread_id\\": f\\"async-test-astream-{uuid4()[:4]}\\"}}}\\n",
    "\\n",
    "    print(\\"\\n1. ainvoke() を使った非同期実行:\\")\\n",
    "    start_time_ainvoke = time.perf_counter()\\n",
    "    final_state_ainvoke = await graph_q7_ch5.ainvoke(initial_input_q7, config=thread_id_q7_ainvoke)\\n",
    "    end_time_ainvoke = time.perf_counter()\\n",
    "    print(f\\"  ainvoke() 実行完了。所要時間: {end_time_ainvoke - start_time_ainvoke:.3f}秒\\")\\n",
    "    if final_state_ainvoke:\\n",
    "        print(f\\"    最終ステップ名: {final_state_ainvoke.get('current_step_name')}\\")\\n",
    "        final_messages_ainvoke = final_state_ainvoke.get('messages', [])\\n",
    "        print(f\\"    最終メッセージ: {final_messages_ainvoke[-1].content if final_messages_ainvoke else 'N/A'}\\")\\n",
    "\\n",
    "    print(\\"\\n2. astream() を使った非同期ストリーミング実行:\\")\\n",
    "    start_time_astream = time.perf_counter()\\n",
    "    async for i, event_chunk in enumerate(graph_q7_ch5.astream(initial_input_q7, config=thread_id_q7_astream, recursion_limit=10)):\\n",
    "        # event_chunkのキーはノード名、値はそのノードが返した状態の更新辞書\\n",
    "        node_completed = list(event_chunk.keys())[0]\\n",
    "        print(f\\"  Async Stream Event {i+1}: ノード「{node_completed}」完了。更新: {event_chunk[node_completed]}\\")\\n",
    "    end_time_astream = time.perf_counter()\\n",
    "    print(f\\"  astream() 実行完了。所要時間: {end_time_astream - start_time_astream:.3f}秒\\")\\n",
    "    \\n",
    "    print(\\"\\n--- 非同期ノードテスト終了 ---\\")\\n",
    "\\n",
    "# このセルをJupyter Notebookで実行する場合、通常は以下のようにします:\\n",
    "# await main_async_runner_q7()\\n",
    "# または、もしトップレベルのawaitが使えない古い環境なら:\\n",
    "# asyncio.run(main_async_runner_q7())\\n",
    "# この解答では、関数定義に留め、ユーザーが次のセルで実行することを促します。\\n",
    "print(\\"非同期実行関数 main_async_runner_q7 が定義されました。\\")\\n",
    "print(\\"Jupyter Notebookでこのセルを実行後、新しいセルで await main_async_runner_q7() を実行してください。\\")\\n",
    "print(\\"(または、スクリプトとして実行する場合は if __name__ == '__main__': asyncio.run(main_async_runner_q7()) を使います)\\")\\n",
    "``````\\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説007</summary>\\n",
    "\\n",
    "#### この問題のポイント\\n",
    "\\n",
    "*   **非同期ノード (`async def`):** ノード関数を `async def` で定義し、内部で `await` 式（例: `await asyncio.sleep()` や非同期ライブラリの呼び出し）を使うことで、そのノードは非同期に実行可能になります。`asyncio.sleep()` は、I/Oバウンドな処理（例: ネットワークリクエスト、ファイルアクセス）の待ち時間をシミュレートするのによく使われます。\\n",
    "*   **グラフへの追加:** 非同期関数も同期関数と同様に `workflow.add_node(\\"node_name\\", async_function_name)` のようにしてグラフに追加できます。LangGraphはこれらを適切に処理します。\\n",
    "*   **非同期実行メソッド (`ainvoke`, `astream`):**\\n",
    "    *   `graph.ainvoke()`: グラフ全体を非同期に実行し、最終的な状態が確定したらそれを返します。`invoke()` の非同期版です。\\n",
    "    *   `graph.astream()`: グラフの実行を非同期にストリーミングします。各ノードの完了イベントを非同期イテレータとして順次取得できます。`stream()` の非同期版です。\\n",
    "    *   これらのメソッドは `await` キーワードと共に呼び出す必要があります（例: `final_state = await graph.ainvoke(...)`）。\\n",
    "*   **イベントループ:** `async/await` を使うコードは、イベントループの中で実行される必要があります。Pythonスクリプトのトップレベルで実行する場合は `asyncio.run(main_async_function())` を使います。Jupyter Notebookのような既にイベントループが実行されている環境では、`await main_async_function()` のように直接 `await` できることが多いです（必要に応じて `nest_asyncio` などのライブラリが必要になることもあります）。\\n",
    "*   **I/Oバウンド処理の効率化:** 非同期ノードの主な利点は、I/O待ち時間中に他の処理（同じグラフ内の他の準備ができた非同期タスクや、Pythonのイベントループが管理する他のコルーチン）に実行を移せることです。これにより、多数のI/O処理を同時に扱うようなアプリケーション全体の応答性やスループットを向上させることができます。\\n",
    "    *   **注意:** LangGraphの個々のノードが `async` であっても、グラフの構造（例: 直列エッジ）が逐次的であれば、ノード自体は順番に実行されます。`async` はノード「内部」のI/O効率化であり、グラフ全体の並列実行（複数のノードを同時に動かすこと）とは直接的には異なります。グラフ全体の並列性を高めるには、問題004（ファンアウト・ファンイン）のような設計が必要です。ただし、`ToolNode` が複数の非同期ツールを呼び出す場合など、`async` はその並列性を活かすのに役立ちます。\\n",
    "*   **所要時間の比較:** この問題の解答例では、ノードは逐次的に接続されているため、`ainvoke()` や `astream()` の全体の所要時間は、各ノードの `asyncio.sleep()` の合計時間に近くなります。もし、`async_A` と `async_B` が並列実行可能なようにグラフが設計されていれば（例: エントリポイントから両方にエッジがあり、その後で結果を統合するファンイン構造）、全体の所要時間は最も時間のかかるブランチの処理時間に近づく可能性があります。\\n",
    "\\n",
    "非同期処理は、特にネットワーク通信やファイル操作が頻繁に発生するLLMアプリケーションやエージェントにおいて、パフォーマンスを最適化するための重要なテクニックです。\\n",
    "\\n",
    "---</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
"""
