{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章: 発展的なグラフ技術"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "以下のセルを順番に実行して、演習に必要な環境をセットアップします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "\n",
    "このセルは、LangGraphおよび関連するLangChainライブラリをインストールします。実行には数分かかる場合があります。\n",
    "ご利用になるLLMプロバイダーに応じて、コメントアウトを解除して必要なライブラリをインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ライブラリのインストール ===\n",
    "# 基本ライブラリ (LangGraphとLangChain Core)\n",
    "!%pip install -qU langchain langgraph\n",
    "\n",
    "# --- LLMプロバイダー別ライブラリ ---\n",
    "# ご利用になるLLMプロバイダーに応じて、以下の該当する行のコメントを解除して実行してください。\n",
    "\n",
    "# OpenAI (GPTシリーズ)\n",
    "# !%pip install -qU langchain_openai\n",
    "\n",
    "# Azure OpenAI\n",
    "# !%pip install -qU langchain_openai # Azureもlangchain_openaiを利用\n",
    "\n",
    "# Google Cloud Vertex AI (Gemini, PaLM等)\n",
    "# !%pip install -qU langchain_google_vertexai\n",
    "\n",
    "# Google Gemini API (Google AI Studioで利用するGemini)\n",
    "# !%pip install -qU langchain_google_genai\n",
    "\n",
    "# Anthropic (Claudeシリーズ)\n",
    "# !%pip install -qU langchain_anthropic\n",
    "\n",
    "# Amazon Bedrock (AWS上の各種モデル、Claudeも含む)\n",
    "# !%pip install -qU langchain_aws boto3 # Bedrock利用時はboto3も必要\n",
    "\n",
    "# --- その他の推奨ライブラリ ---\n",
    "# グラフの可視化や環境変数管理など、演習全体を通して利用する可能性のあるライブラリ\n",
    "!%pip install -qU python-dotenv pygraphviz pydotplus graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMプロバイダーの選択\n",
    "\n",
    "このセルでは、使用するLLMプロバイダーを選択します。\n",
    "`LLM_PROVIDER` 変数に、利用したいプロバイダー名を設定してください。\n",
    "選択可能なプロバイダー: `\"openai\"`, `\"azure\"`, `\"google\"` (Vertex AI), `\"google_genai\"` (Gemini API), `\"anthropic\"`, `\"bedrock\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMプロバイダーの選択 ===\n",
    "# 利用したいLLMプロバイダーを以下の変数で指定してください。\n",
    "# \"openai\", \"azure\", \"google\" (Vertex AI), \"google_genai\" (Gemini API), \"anthropic\", \"bedrock\" のいずれかを選択できます。\n",
    "LLM_PROVIDER = \"openai\"  # 例: OpenAI を利用する場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIキー/環境変数の設定\n",
    "\n",
    "以下のセルを実行する前に、選択したLLMプロバイダーに応じたAPIキーまたは環境変数を設定する必要があります。\n",
    "\n",
    "**一般的な手順:**\n",
    "1.  `.env.sample` ファイルをコピーして `.env` ファイルを作成します。\n",
    "2.  `.env` ファイルを開き、選択したLLMプロバイダーに対応するAPIキーや必要な情報を記述します。\n",
    "    *   **OpenAI:** `OPENAI_API_KEY`\n",
    "    *   **Azure OpenAI:** `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `OPENAI_API_VERSION`, `AZURE_OPENAI_DEPLOYMENT_NAME`\n",
    "    *   **Google (Vertex AI):** `GOOGLE_CLOUD_PROJECT_ID`, `GOOGLE_CLOUD_LOCATION` (Colab環境外で実行する場合、`GOOGLE_APPLICATION_CREDENTIALS` 環境変数の設定も必要になることがあります)\n",
    "    *   **Google (Gemini API):** `GOOGLE_API_KEY`\n",
    "    *   **Anthropic:** `ANTHROPIC_API_KEY`\n",
    "    *   **AWS Bedrock:** `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION_NAME` (IAMロールを使用する場合は、これらのキー設定は不要な場合がありますが、リージョン名は必須です)\n",
    "3.  ファイルを保存します。\n",
    "\n",
    "**Google Colab を使用している場合:**\n",
    "上記の `.env` ファイルを使用する代わりに、Colabのシークレットマネージャーに必要なキーを登録してください。\n",
    "例えば、OpenAIを使用する場合は `OPENAI_API_KEY` という名前でシークレットを登録します。\n",
    "Vertex AI を利用する場合は、Colab上での認証 (`google.colab.auth.authenticate_user()`) が実行されます。\n",
    "\n",
    "このセルは、設定された情報に基づいて環境変数をロードし、LLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === APIキー/環境変数の設定 ===\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envファイルから環境変数を読み込む (存在する場合)\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# --- OpenAI ---\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY and IS_COLAB:\n",
    "        OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"OpenAI APIキーが設定されていません。環境変数 OPENAI_API_KEY を設定するか、Colab環境の場合はシークレットに OPENAI_API_KEY を設定してください。\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# --- Azure OpenAI ---\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not AZURE_OPENAI_API_KEY: AZURE_OPENAI_API_KEY = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not AZURE_OPENAI_ENDPOINT: AZURE_OPENAI_ENDPOINT = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        if not OPENAI_API_VERSION: OPENAI_API_VERSION = userdata.get(\"OPENAI_API_VERSION\") # 例: \"2023-07-01-preview\"\n",
    "        if not AZURE_OPENAI_DEPLOYMENT_NAME: AZURE_OPENAI_DEPLOYMENT_NAME = userdata.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if not AZURE_OPENAI_API_KEY: raise ValueError(\"Azure OpenAI APIキー (AZURE_OPENAI_API_KEY) が設定されていません。\")\n",
    "    if not AZURE_OPENAI_ENDPOINT: raise ValueError(\"Azure OpenAI エンドポイント (AZURE_OPENAI_ENDPOINT) が設定されていません。\")\n",
    "    if not OPENAI_API_VERSION: OPENAI_API_VERSION = \"2023-07-01-preview\" # デフォルトを設定することも可能\n",
    "    if not AZURE_OPENAI_DEPLOYMENT_NAME: raise ValueError(\"Azure OpenAI デプロイメント名 (AZURE_OPENAI_DEPLOYMENT_NAME) が設定されていません。\")\n",
    "\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "    os.environ[\"OPENAI_API_VERSION\"] = OPENAI_API_VERSION\n",
    "\n",
    "# --- Google Cloud Vertex AI (Gemini) ---\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT_ID\") # .env 用に修正\n",
    "    LOCATION = os.environ.get(\"GOOGLE_CLOUD_LOCATION\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not PROJECT_ID: PROJECT_ID = userdata.get(\"GOOGLE_CLOUD_PROJECT_ID\")\n",
    "        if not LOCATION: LOCATION = userdata.get(\"GOOGLE_CLOUD_LOCATION\") # 例: \"us-central1\"\n",
    "        from google.colab import auth as google_auth\n",
    "        google_auth.authenticate_user() # Vertex AI を使う場合は Colab での認証を推奨\n",
    "    else: # Colab外の場合、.envから読み込んだ値で環境変数を設定\n",
    "        if PROJECT_ID: os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID # Vertex AI SDKが参照する標準的な環境変数名\n",
    "        if LOCATION: os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "\n",
    "    if not PROJECT_ID: raise ValueError(\"Google Cloud Project ID が設定されていません。環境変数 GOOGLE_CLOUD_PROJECT_ID を設定するか、Colab環境の場合はシークレットに GOOGLE_CLOUD_PROJECT_ID を設定してください。\")\n",
    "    if not LOCATION: LOCATION = \"us-central1\" # デフォルトロケーション\n",
    "\n",
    "# --- Google Gemini API (langchain-google-genai) ---\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY and IS_COLAB:\n",
    "        GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"Google APIキーが設定されていません。環境変数 GOOGLE_API_KEY を設定するか、Colab環境の場合はシークレットに GOOGLE_API_KEY を設定してください。\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# --- Anthropic (Claude) ---\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY and IS_COLAB:\n",
    "        ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        raise ValueError(\"Anthropic APIキーが設定されていません。環境変数 ANTHROPIC_API_KEY を設定するか、Colab環境の場合はシークレットに ANTHROPIC_API_KEY を設定してください。\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "# --- Amazon Bedrock (Claude) ---\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    AWS_REGION_NAME = os.environ.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if IS_COLAB: \n",
    "        if not AWS_ACCESS_KEY_ID: AWS_ACCESS_KEY_ID = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
    "        if not AWS_SECRET_ACCESS_KEY: AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        if not AWS_REGION_NAME: AWS_REGION_NAME = userdata.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if not AWS_REGION_NAME:\n",
    "         raise ValueError(\"AWSリージョン名 (AWS_REGION_NAME) が設定されていません。Bedrock利用にはリージョン指定が必要です。\")\n",
    "\n",
    "    # 環境変数に設定 (boto3がこれらを自動で読み込む)\n",
    "    if AWS_ACCESS_KEY_ID: os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "    if AWS_SECRET_ACCESS_KEY: os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION_NAME # boto3が参照する標準的なリージョン環境変数名\n",
    "    os.environ[\"AWS_REGION\"] = AWS_REGION_NAME # いくつかのライブラリはこちらを参照することもある\n",
    "\n",
    "print(f\"APIキー/環境変数の設定完了 (プロバイダー: {LLM_PROVIDER})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMクライアントの初期化\n",
    "\n",
    "このセルは、上で選択・設定したLLMプロバイダーに基づいて、対応するLLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMクライアントの動的初期化 ===\n",
    "llm = None\n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), # 環境変数から取得\n",
    "        openai_api_version=os.environ.get(\"OPENAI_API_VERSION\"), # 環境変数から取得\n",
    "        temperature=0,\n",
    "    )\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    from langchain_google_vertexai import ChatVertexAI\n",
    "    # PROJECT_ID, LOCATION は前のセルで環境変数に設定済みか、Colabの場合は直接利用\n",
    "    llm = ChatVertexAI(model_name=\"gemini-1.5-flash-001\", temperature=0, project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"), location=os.environ.get(\"GOOGLE_CLOUD_LOCATION\"))\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0, convert_system_message_to_human=True)\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    from langchain_anthropic import ChatAnthropic\n",
    "    llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    from langchain_aws import ChatBedrock # langchain_community.chat_models から langchain_aws に変更の可能性あり\n",
    "    # AWS_REGION_NAME は前のセルで環境変数 AWS_DEFAULT_REGION に設定済み\n",
    "    llm = ChatBedrock( # BedrockChat ではなく ChatBedrock が一般的\n",
    "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        # region_name=os.environ.get(\"AWS_DEFAULT_REGION\"), # 通常、boto3が環境変数から自動で読み込む\n",
    "        model_kwargs={\"temperature\": 0},\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Unsupported LLM_PROVIDER: {LLM_PROVIDER}. \"\n",
    "        \"Please choose from 'openai', 'azure', 'google', 'google_genai', 'anthropic', or 'bedrock'.\"\n",
    "    )\n",
    "\n",
    "print(f\"LLM Provider: {LLM_PROVIDER}\")\n",
    "if llm:\n",
    "    print(f\"LLM Client Type: {type(llm)}\")\n",
    "    # モデル名取得の試行を汎用的に\n",
    "    model_attr = getattr(llm, 'model', None) or \\ \n",
    "                 getattr(llm, 'model_name', None) or \\ \n",
    "                 getattr(llm, 'model_id', None) or \\ \n",
    "                 (hasattr(llm, 'llm') and getattr(llm.llm, 'model', None)) # 一部のLLMクライアントのネスト構造に対応\n",
    "    if hasattr(llm, 'azure_deployment') and not model_attr: # Azure特有の属性\n",
    "        model_attr = llm.azure_deployment\n",
    "        \n",
    "    if model_attr:\n",
    "        print(f\"LLM Model: {model_attr}\")\n",
    "    else:\n",
    "        print(\"LLM Model: (Could not determine model name from client attributes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この章では、Agent Swarm、永続化(Checkpointer)、ストリーミング、カスタムエージェントなど、LangGraphのより高度な機能やテクニックについて学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題001: グラフ実行のストリーミング出力の基本\n",
    "\n",
    "`app.stream()` メソッドを使用すると、グラフの実行中に各ノードの入出力やイベントをリアルタイムで監視できます。これにより、グラフがどのように動作しているかを詳細に把握したり、ユーザーに逐次的なフィードバックを提供したりすることが可能になります。この問題では、簡単な2ステップのグラフを作成し、その実行過程をストリーミングで取得する方法を学びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "import time\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class StreamingState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    processed_text: str\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def entry_node(state: StreamingState):\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    print(f\"入力ノード実行: '{user_message}' を受信\")\n",
    "    time.sleep(0.5) # 処理を模倣\n",
    "    return {\"processed_text\": f\"入力: {user_message}\"}\n",
    "\n",
    "def processing_node(state: StreamingState):\n",
    "    current_text = state[\"processed_text\"]\n",
    "    print(f\"処理ノード実行: '{current_text}' を大文字に変換\")\n",
    "    time.sleep(0.5) # 処理を模倣\n",
    "    processed_text = current_text.upper()\n",
    "    return {\"processed_text\": processed_text, \"messages\": [AIMessage(content=f\"処理結果: {processed_text}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(StreamingState) \n",
    "\n",
    "workflow.add_node(\"entry\", entry_node)\n",
    "workflow.add_node(\"processor\", processing_node)\n",
    "\n",
    "workflow.____(\"entry\") \n",
    "workflow.____(\"entry\", \"processor\") \n",
    "workflow.____(\"processor\", ____) \n",
    "\n",
    "app = workflow.____() \n",
    "\n",
    "# --- グラフの実行とストリーミング表示 ---\n",
    "inputs = {\"messages\": [HumanMessage(content=\"こんにちはストリーミング\")]}\n",
    "print(\"\\n--- ストリーミング開始 ---\")\n",
    "for event in app.____(inputs, stream_mode=\"values\"): \n",
    "    print(event)\n",
    "print(\"--- ストリーミング終了 ---\")\n",
    "\n",
    "# 参考: invokeでの最終結果\n",
    "final_result = app.invoke(inputs)\n",
    "print(f\"\\n最終結果 (invoke): {final_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答001</summary>\n",
    "\n",
    "```python\n",
    "# 解答001\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "import time\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class StreamingState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    processed_text: str\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def entry_node(state: StreamingState):\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    print(f\"入力ノード実行: '{user_message}' を受信\")\n",
    "    time.sleep(0.5) # 処理を模倣\n",
    "    return {\"processed_text\": f\"入力: {user_message}\"}\n",
    "\n",
    "def processing_node(state: StreamingState):\n",
    "    current_text = state[\"processed_text\"]\n",
    "    print(f\"処理ノード実行: '{current_text}' を大文字に変換\")\n",
    "    time.sleep(0.5) # 処理を模倣\n",
    "    processed_text = current_text.upper()\n",
    "    return {\"processed_text\": processed_text, \"messages\": [AIMessage(content=f\"処理結果: {processed_text}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(StreamingState)\n",
    "\n",
    "workflow.add_node(\"entry\", entry_node)\n",
    "workflow.add_node(\"processor\", processing_node)\n",
    "\n",
    "workflow.set_entry_point(\"entry\")\n",
    "workflow.add_edge(\"entry\", \"processor\")\n",
    "workflow.add_edge(\"processor\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行とストリーミング表示 ---\n",
    "inputs = {\"messages\": [HumanMessage(content=\"こんにちはストリーミング\")]}\n",
    "print(\"\\n--- ストリーミング開始 ---\")\n",
    "for event in app.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "print(\"--- ストリーミング終了 ---\")\n",
    "\n",
    "# 参考: invokeでの最終結果\n",
    "final_result = app.invoke(inputs)\n",
    "print(f\"\\n最終結果 (invoke): {final_result}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説001</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** `app.stream()`メソッドの基本的な使い方と、それによって得られるイベントの種類を理解します。`stream_mode=\"values\"`を指定すると、各ステップでの状態全体が辞書形式でストリーミングされます。\n",
    "*   **コード解説:**\n",
    "    *   `StreamingState`には、メッセージ履歴の他に、処理中のテキストを保持する`processed_text`フィールドを定義しました。\n",
    "    *   `entry_node`は入力メッセージを受け取り、`processed_text`を初期化します。\n",
    "    *   `processing_node`は`processed_text`を大文字に変換し、最終的なメッセージを`messages`に追加します。\n",
    "    *   `app.stream(inputs, stream_mode=\"values\")`を呼び出すと、イテレータが返されます。このイテレータをループ処理することで、グラフの各実行ステップ（ノードの実行完了時など）の状態を取得できます。\n",
    "    *   出力される`event`は、`{'node_name': state_after_node_execution}` のような形式の辞書です。例えば、`{'entry': {'messages': [...], 'processed_text': '入力: こんにちはストリーミング'}}` や `{'processor': {'messages': [...], 'processed_text': '入力: こんにちはストリーミング'}}` (processor実行前の状態)、そして `{'processor': {'messages': [...], 'processed_text': '入力: コンニチハストリーミング'}}` (processor実行後の状態) などが出力されます。\n",
    "    *   `stream_mode`には他にも`\"updates\"`（変更差分のみ）や`\"events\"`（より詳細なイベント情報）があります。`\"values\"`は状態全体を見たい場合に便利です。\n",
    "    *   `time.sleep(0.5)` は、ストリーミングが逐次的であることを視覚的に分かりやすくするためのものです。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題002: LLMからのストリーミング応答をリアルタイムで処理する\n",
    "\n",
    "LLM（大規模言語モデル）をグラフに組み込む際、応答が完了するまで待つのではなく、生成されるテキストをトークンごと、またはチャンクごとにリアルタイムで受け取りたい場合があります。これは特に、ユーザーインターフェースに逐次的にテキストを表示するチャットボットなどで重要です。\n",
    "この問題では、LLMノードからの出力をストリーミングで受け取り、部分的な応答を処理する方法を学びます。`stream_mode=\"updates\"` を使用して、変更があった部分のみを取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002\n",
    "from typing import TypedDict, Annotated, AsyncIterator\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import asyncio\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class LLMStreamingState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    current_response_chunk: str # LLMからの最新の応答チャンク\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "async def streaming_llm_node(state: LLMStreamingState, config: RunnableConfig) -> AsyncIterator[LLMStreamingState]:\n",
    "    print(\"\\nstreaming_llm_node: LLM呼び出し開始\")\n",
    "    # `llm.astream` を使ってストリーミング応答を取得\n",
    "    async for chunk in ____.____(state[\"messages\"], config=config): \n",
    "        print(f\"  受信チャンク: {chunk.content}\", end=\"\", flush=True)\n",
    "        yield {____: chunk.content} \n",
    "    print(\"\\nstreaming_llm_node: LLM呼び出し完了\")\n",
    "    yield {\"messages\": [AIMessage(content=\"(ストリーム完了)\")]} \n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(LLMStreamingState) \n",
    "\n",
    "workflow.____(\"llm_streamer\", streaming_llm_node) \n",
    "workflow.____(\"llm_streamer\") \n",
    "workflow.____(\"llm_streamer\", ____) \n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの非同期実行とストリーミング表示 ---\n",
    "async def main():\n",
    "    inputs = {\"messages\": [HumanMessage(content=\"LangGraphのストリーミングについて3つのポイントで教えてください。\")]}\n",
    "    print(\"--- 非同期ストリーミング開始 ---\")\n",
    "    full_response_content = \"\"\n",
    "    async for update_event in app.____(inputs, stream_mode=____): \n",
    "        for node_name, state_update in update_event.items():\n",
    "            if \"current_response_chunk\" in state_update:\n",
    "                chunk_content = state_update[\"current_response_chunk\"]\n",
    "                full_response_content += chunk_content\n",
    "            else:\n",
    "                print(f\"\\nイベント({node_name}): {state_update}\")\n",
    "\n",
    "    print(\"\\n--- 非同期ストリーミング終了 ---\")\n",
    "    print(f\"\\n最終的な組み立てられた応答:\\n{full_response_content}\")\n",
    "\n",
    "    final_result = await app.ainvoke(inputs)\n",
    "    print(f\"\\n最終結果 (ainvoke): {final_result}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            loop.run_until_complete(main())\n",
    "        else:\n",
    "            loop.run_until_complete(main())\n",
    "    except RuntimeError as e:\n",
    "        if \"cannot run new tasks\" in str(e) or \"already running\" in str(e):\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            asyncio.run(main())\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答002</summary>\n",
    "\n",
    "```python\n",
    "# 解答002\n",
    "from typing import TypedDict, Annotated, AsyncIterator\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import asyncio\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class LLMStreamingState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    current_response_chunk: str # LLMからの最新の応答チャンク\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "async def streaming_llm_node(state: LLMStreamingState, config: RunnableConfig) -> AsyncIterator[LLMStreamingState]:\n",
    "    print(\"\\nstreaming_llm_node: LLM呼び出し開始\")\n",
    "    # `llm.astream` を使ってストリーミング応答を取得\n",
    "    async for chunk in llm.astream(state[\"messages\"], config=config):\n",
    "        # chunk は AIMessageChunk オブジェクトであることが多い\n",
    "        print(f\"  受信チャンク: {chunk.content}\", end=\"\", flush=True)\n",
    "        yield {\"current_response_chunk\": chunk.content} # 部分的な更新をyield\n",
    "    print(\"\\nstreaming_llm_node: LLM呼び出し完了\")\n",
    "    # ストリームの最後に最終的なメッセージを messages に追加する (オプション)\n",
    "    # この例では current_response_chunk をつなぎ合わせる処理は省略し、各チャンクをそのまま流す\n",
    "    # 完全なメッセージを messages に追加したい場合は、別途状態更新ロジックが必要\n",
    "    yield {\"messages\": [AIMessage(content=\"(ストリーム完了)\")]} # 完了を示すメッセージ (内容は任意)\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(LLMStreamingState)\n",
    "\n",
    "workflow.add_node(\"llm_streamer\", streaming_llm_node)\n",
    "workflow.set_entry_point(\"llm_streamer\")\n",
    "workflow.add_edge(\"llm_streamer\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの非同期実行とストリーミング表示 ---\n",
    "async def main():\n",
    "    inputs = {\"messages\": [HumanMessage(content=\"LangGraphのストリーミングについて3つのポイントで教えてください。\")]}\n",
    "    print(\"--- 非同期ストリーミング開始 ---\")\n",
    "    full_response_content = \"\"\n",
    "    # stream_mode=\"updates\" を使うと、変更があった部分のみが辞書で返る\n",
    "    async for update_event in app.astream(inputs, stream_mode=\"updates\"):\n",
    "        for node_name, state_update in update_event.items():\n",
    "            if \"current_response_chunk\" in state_update:\n",
    "                chunk_content = state_update[\"current_response_chunk\"]\n",
    "                # print(f\"イベント({node_name}): {chunk_content}\", end=\"\", flush=True) # ノード内でprintしているのでここでは省略\n",
    "                full_response_content += chunk_content\n",
    "            else:\n",
    "                # messages の更新など、他の状態変化もここで確認可能\n",
    "                print(f\"\\nイベント({node_name}): {state_update}\")\n",
    "\n",
    "    print(\"\\n--- 非同期ストリーミング終了 ---\")\n",
    "    print(f\"\\n最終的な組み立てられた応答:\\n{full_response_content}\")\n",
    "\n",
    "    # 参考: ainvokeでの最終結果\n",
    "    final_result = await app.ainvoke(inputs)\n",
    "    print(f\"\\n最終結果 (ainvoke): {final_result}\")\n",
    "\n",
    "# Jupyter Notebookで非同期コードを実行するためにイベントループを取得して実行\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # ネストされたイベントループが許可されていない場合、新しいイベントループを作成\n",
    "            # (Jupyter Notebookなど、既にイベントループが実行中の環境向け)\n",
    "            print(\"既存のイベントループが実行中です。新しいループで実行します。\")\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            loop.run_until_complete(main())\n",
    "        else:\n",
    "            loop.run_until_complete(main())\n",
    "    except RuntimeError as e:\n",
    "        if \"cannot run new tasks\" in str(e) or \"already running\" in str(e):\n",
    "            # フォールバックとして nest_asyncio を使う\n",
    "            print(f\"RuntimeError: {e}. nest_asyncio を使用して再試行します。\")\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            asyncio.run(main()) # nest_asyncio.apply() 後は asyncio.run でも動作するはず\n",
    "        else:\n",
    "            raise\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説002</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:**\n",
    "    1.  **非同期ノード:** ノード関数を `async def` で定義し、戻り値のアノテーションに `AsyncIterator[StateType]` を使用することで、ノード自体がストリーミング出力（部分的な状態更新）を行えるようになります。\n",
    "    2.  **LLMのストリーミング:** `llm.astream()` メソッド（または対応するストリーミングメソッド）を使用して、LLMからの応答をチャンクで非同期に受け取ります。\n",
    "    3.  **部分的な状態更新:** ノード内で `yield {key: value}` を使うことで、そのキーに対応する状態のみを更新し、グラフの呼び出し元にストリーミングします。\n",
    "    4.  **`app.astream()` と `stream_mode=\"updates\"`:** コンパイルされたグラフを `app.astream()` で呼び出し、`stream_mode=\"updates\"` を指定することで、各ステップで変更があった状態の部分だけを効率的に受け取ることができます。\n",
    "    5.  **Jupyter Notebookでの非同期実行:** `asyncio.get_event_loop().run_until_complete()` や `nest_asyncio` を使って、Jupyter Notebookのような既にイベントループが実行されている可能性のある環境で非同期コードを実行する方法を学びます。\n",
    "*   **コード解説:**\n",
    "    *   `LLMStreamingState` に `current_response_chunk` を追加し、LLMからの最新のテキストチャンクを保持します。\n",
    "    *   `streaming_llm_node`:\n",
    "        *   `async def` で定義され、`AsyncIterator[LLMStreamingState]` を返します。\n",
    "        *   内部で `llm.astream(state[\"messages\"], config=config)` を呼び出します。`config` 引数は、LangChainの実行コンテキスト（コールバックなど）を伝播させるために重要です。\n",
    "        *   `llm.astream` から得られた各 `chunk`（通常は `AIMessageChunk` オブジェクト）の内容を `yield {\"current_response_chunk\": chunk.content}` のようにしてストリーミングします。これにより、`current_response_chunk` のみが更新されたイベントがグラフの外部に送られます。\n",
    "        *   最後に `yield {\"messages\": [AIMessage(content=\"(ストリーム完了)\")]}` で `messages` 状態を更新しています。実際のアプリケーションでは、全てのチャンクを結合して完全なAIメッセージとして `messages` に追加することが一般的です。\n",
    "    *   `app.astream(inputs, stream_mode=\"updates\")`:\n",
    "        *   非同期でグラフを実行し、ストリーミング出力を受け取ります。\n",
    "        *   `stream_mode=\"updates\"` のため、イベントは `{\"node_name\": {\"state_key_updated\": new_value}}` のような形式になります。この例では `{\"llm_streamer\": {\"current_response_chunk\": \"some text\"}}` や `{\"llm_streamer\": {\"messages\": [...]}}` といった更新が流れてきます。\n",
    "    *   `main` 関数内のループでは、`update_event` から `current_response_chunk` を取り出し、`full_response_content` に結合しています。\n",
    "    *   `if __name__ == '__main__':` ブロックは、Jupyter Notebook環境で `asyncio` のイベントループを適切に処理するための一般的なパターンです。`nest_asyncio` は、既に実行中のイベントループ内でさらに非同期コードを実行可能にするために使われます。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題003: `MemorySaver` を使ったグラフ状態の永続化と再開\n",
    "\n",
    "LangGraphでは、グラフの実行状態を保存し、後で同じ状態から再開することができます。これを実現するのがチェックポイント機能です。`MemorySaver` は、状態をインメモリに保存する最もシンプルなチェックポインターです。\n",
    "この問題では、`MemorySaver` を使ってグラフの実行状態を一時的に保存し、異なる `invoke` 呼び出し間で状態が引き継がれる（再開される）ことを確認します。スレッドID (`thread_id`) を使って、特定の実行シーケンスを識別します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver # MemorySaverをインポート\n",
    "import uuid\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class CheckpointState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    counter: int\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def counter_node(state: CheckpointState):\n",
    "    print(f\"counter_node: 現在のカウンター = {state['counter']}\")\n",
    "    new_counter = state[\"counter\"] + 1\n",
    "    if state[\"messages\"]:\n",
    "        print(f\"counter_node: 最新メッセージ = '{state['messages'][-1].content}'\")\n",
    "    return {\"counter\": new_counter, \"messages\": [AIMessage(content=f\"カウンターが {new_counter} になりました\")]}\n",
    "\n",
    "def check_finish(state: CheckpointState):\n",
    "    if state[\"counter\"] >= 3:\n",
    "        print(\"check_finish: カウンターが3以上なので終了します。\")\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        print(f\"check_finish: カウンターは {state['counter']} なので続行します。\")\n",
    "        return \"continue_counting\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(CheckpointState)\n",
    "workflow.add_node(\"counter\", counter_node)\n",
    "workflow.set_entry_point(\"counter\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"counter\",\n",
    "    check_finish,\n",
    "    {\n",
    "        \"continue_counting\": \"counter\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- チェックポインターの設定 ---\n",
    "memory_saver = ____() \n",
    "\n",
    "app = workflow.compile(checkpointer=____) \n",
    "\n",
    "# --- グラフの実行と状態の永続化・再開 ---\n",
    "thread_id = str(uuid.uuid4())\n",
    "config_1 = {\"configurable\": {\"thread_id\": ____}} \n",
    "\n",
    "print(\"\\n--- 1回目の実行 (カウンターを1に) ---\")\n",
    "inputs_1 = {\"messages\": [HumanMessage(content=\"最初の呼び出し\")], \"counter\": 0}\n",
    "result_1 = app.invoke(inputs_1, config=config_1)\n",
    "print(f\"1回目 結果: {result_1}\")\n",
    "\n",
    "print(\"\\n--- 2回目の実行 (状態を再開し、カウンターを2に) ---\")\n",
    "inputs_2 = {\"messages\": [HumanMessage(content=\"2回目の呼び出し\")]}\n",
    "result_2 = app.invoke(inputs_2, config=____) \n",
    "print(f\"2回目 結果: {result_2}\")\n",
    "\n",
    "print(\"\\n--- 3回目の実行 (状態を再開し、カウンターを3にして終了) ---\")\n",
    "inputs_3 = {\"messages\": [HumanMessage(content=\"3回目の呼び出し\")]}\n",
    "result_3 = app.invoke(inputs_3, config=config_1)\n",
    "print(f\"3回目 結果: {result_3}\")\n",
    "\n",
    "print(\"\\n--- 新しいスレッドでの実行 (カウンターは1から開始) ---\")\n",
    "thread_id_new = str(uuid.uuid4())\n",
    "config_new = {\"configurable\": {\"thread_id\": thread_id_new}}\n",
    "inputs_new = {\"messages\": [HumanMessage(content=\"新しいスレッドの呼び出し\")], \"counter\": 0}\n",
    "result_new = app.invoke(inputs_new, config=config_new)\n",
    "print(f\"新スレッド 結果: {result_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答003</summary>\n",
    "\n",
    "```python\n",
    "# 解答003\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver # MemorySaverをインポート\n",
    "import uuid\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class CheckpointState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    counter: int\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def counter_node(state: CheckpointState):\n",
    "    print(f\"counter_node: 現在のカウンター = {state['counter']}\")\n",
    "    new_counter = state[\"counter\"] + 1\n",
    "    # ユーザーからの最新メッセージも表示（もしあれば）\n",
    "    if state[\"messages\"]:\n",
    "        print(f\"counter_node: 最新メッセージ = '{state['messages'][-1].content}'\")\n",
    "    return {\"counter\": new_counter, \"messages\": [AIMessage(content=f\"カウンターが {new_counter} になりました\")]}\n",
    "\n",
    "def check_finish(state: CheckpointState):\n",
    "    #カウンターが3以上なら終了、それ以外はcounter_nodeへ\n",
    "    if state[\"counter\"] >= 3:\n",
    "        print(\"check_finish: カウンターが3以上なので終了します。\")\n",
    "        return \"__end__\" # ENDノードへの特別な名前\n",
    "    else:\n",
    "        print(f\"check_finish: カウンターは {state['counter']} なので続行します。\")\n",
    "        return \"continue_counting\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(CheckpointState)\n",
    "workflow.add_node(\"counter\", counter_node)\n",
    "\n",
    "workflow.set_entry_point(\"counter\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"counter\",\n",
    "    check_finish,\n",
    "    {\n",
    "        \"continue_counting\": \"counter\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- チェックポインターの設定 ---\n",
    "memory_saver = MemorySaver()\n",
    "\n",
    "# グラフのコンパイル時にチェックポインターを渡す\n",
    "app = workflow.compile(checkpointer=memory_saver)\n",
    "\n",
    "# --- グラフの実行と状態の永続化・再開 ---\n",
    "thread_id = str(uuid.uuid4()) # 各実行シーケンスの一意なID\n",
    "config_1 = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "print(\"\\n--- 1回目の実行 (カウンターを1に) ---\")\n",
    "inputs_1 = {\"messages\": [HumanMessage(content=\"最初の呼び出し\")], \"counter\": 0}\n",
    "result_1 = app.invoke(inputs_1, config=config_1)\n",
    "print(f\"1回目 結果: {result_1}\")\n",
    "assert result_1['counter'] == 1, f\"期待値1, 実際値{result_1['counter']}\"\n",
    "\n",
    "print(\"\\n--- 2回目の実行 (状態を再開し、カウンターを2に) ---\")\n",
    "# 2回目の入力では counter を指定しない -> 保存された状態から再開されるはず\n",
    "inputs_2 = {\"messages\": [HumanMessage(content=\"2回目の呼び出し\")]}\n",
    "result_2 = app.invoke(inputs_2, config=config_1) # 同じthread_idを使用\n",
    "print(f\"2回目 結果: {result_2}\")\n",
    "assert result_2['counter'] == 2, f\"期待値2, 実際値{result_2['counter']}\"\n",
    "\n",
    "print(\"\\n--- 3回目の実行 (状態を再開し、カウンターを3にして終了) ---\")\n",
    "inputs_3 = {\"messages\": [HumanMessage(content=\"3回目の呼び出し\")]}\n",
    "result_3 = app.invoke(inputs_3, config=config_1)\n",
    "print(f\"3回目 結果: {result_3}\")\n",
    "assert result_3['counter'] == 3, f\"期待値3, 実際値{result_3['counter']}\"\n",
    "assert result_3['messages'][-1].content == \"カウンターが 3 になりました\", \"最終メッセージが期待通りではありません\"\n",
    "\n",
    "print(\"\\n--- 新しいスレッドでの実行 (カウンターは1から開始) ---\")\n",
    "thread_id_new = str(uuid.uuid4())\n",
    "config_new = {\"configurable\": {\"thread_id\": thread_id_new}}\n",
    "inputs_new = {\"messages\": [HumanMessage(content=\"新しいスレッドの呼び出し\")], \"counter\": 0}\n",
    "result_new = app.invoke(inputs_new, config=config_new)\n",
    "print(f\"新スレッド 結果: {result_new}\")\n",
    "assert result_new['counter'] == 1, f\"新スレッド期待値1, 実際値{result_new['counter']}\"\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説003</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:**\n",
    "    1.  `MemorySaver` の基本的な使い方。\n",
    "    2.  グラフコンパイル時の `checkpointer` 引数の指定方法。\n",
    "    3.  `app.invoke` や `app.stream` 時に `config={\"configurable\": {\"thread_id\": \"some_unique_id\"}}` を渡すことで、特定の実行シーケンス（スレッド）の状態を管理する方法。\n",
    "    4.  同じ `thread_id` を使って `invoke` を呼び出すと、前回の状態から再開されること。\n",
    "    5.  新しい `thread_id` を使うと、初期状態から開始されること。\n",
    "*   **コード解説:**\n",
    "    *   `CheckpointState` に `counter` を追加し、ノードが実行されるたびにインクリメントされるようにしました。\n",
    "    *   `counter_node` はカウンターを1増やし、現在のカウンター値をメッセージとして返します。\n",
    "    *   `check_finish` は条件分岐ノードで、カウンターが3以上になるとグラフを終了 (`END`) し、そうでなければ `counter_node` に戻って処理を続行 (`continue_counting`) します。\n",
    "    *   `memory_saver = MemorySaver()` でインメモリのチェックポインターを作成します。\n",
    "    *   `app = workflow.compile(checkpointer=memory_saver)` で、コンパイルされたアプリケーションにチェックポインターを接続します。\n",
    "    *   `thread_id = str(uuid.uuid4())` で、この一連の実行のためのユニークなIDを生成します。\n",
    "    *   `config_1 = {\"configurable\": {\"thread_id\": thread_id}}` の形式でコンフィグを作成し、`app.invoke` 時に渡します。これにより、LangGraphはこのスレッドIDに関連付けられた状態を `memory_saver` から探し、存在すればそこから再開します。\n",
    "    *   最初の `app.invoke(inputs_1, config=config_1)` では、`inputs_1` で `counter: 0` を渡しているため、初期状態から開始されます。`counter_node` が実行され、カウンターは1になります。この状態が `thread_id` に関連付けられて保存されます。\n",
    "    *   2回目の `app.invoke(inputs_2, config=config_1)` では、同じ `config_1` (つまり同じ `thread_id`) を使用します。入力 `inputs_2` には `counter` を含めていません。LangGraphは `memory_saver` から `thread_id` の状態（カウンターが1の状態）をロードし、そこからグラフの実行を再開します。そのため、`counter_node` が再度実行され、カウンターは2になります。\n",
    "    *   3回目の呼び出しも同様にカウンターが3になり、`check_finish` で `END` に遷移します。\n",
    "    *   最後に、新しい `thread_id_new` を使って実行すると、`memory_saver` にはそのIDの状態が存在しないため、`inputs_new` で指定された初期状態（`counter: 0`）からグラフが開始され、カウンターは1になります。これにより、異なる実行コンテキストが正しく分離されていることが確認できます。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題004: `SqliteSaver` を使った永続化\n",
    "\n",
    "`MemorySaver` は手軽ですが、プログラムが終了すると状態は失われます。より永続的な保存のためには、データベースバックエンドのチェックポインターを使用します。`SqliteSaver` は、SQLiteデータベースファイルにグラフの状態を保存します。\n",
    "この問題では、`SqliteSaver` を設定し、グラフの実行状態がSQLiteファイルに保存され、異なるプログラムの実行や長期間の後でも再開できることを（概念的に）示します。実際にファイルが作成されることを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver # SqliteSaverをインポート\n",
    "import sqlite3\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "class CheckpointState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    counter: int\n",
    "\n",
    "def counter_node_sqlite(state: CheckpointState):\n",
    "    print(f\"counter_node_sqlite: 現在のカウンター = {state['counter']}\")\n",
    "    new_counter = state[\"counter\"] + 1\n",
    "    if state[\"messages\"]:\n",
    "        print(f\"counter_node_sqlite: 最新メッセージ = '{state['messages'][-1].content}'\")\n",
    "    return {\"counter\": new_counter, \"messages\": [AIMessage(content=f\"カウンターが {new_counter} になりました (sqlite)\")]}\n",
    "\n",
    "def check_finish_sqlite(state: CheckpointState):\n",
    "    if state[\"counter\"] >= 2:\n",
    "        print(\"check_finish_sqlite: カウンターが2以上なので終了します。\")\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        print(f\"check_finish_sqlite: カウンターは {state['counter']} なので続行します。\")\n",
    "        return \"continue_counting\"\n",
    "\n",
    "workflow_sqlite = StateGraph(CheckpointState)\n",
    "workflow_sqlite.add_node(\"counter_sqlite_node\", counter_node_sqlite)\n",
    "workflow_sqlite.set_entry_point(\"counter_sqlite_node\")\n",
    "workflow_sqlite.add_conditional_edges(\n",
    "    \"counter_sqlite_node\",\n",
    "    check_finish_sqlite,\n",
    "    {\n",
    "        \"continue_counting\": \"counter_sqlite_node\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "db_file = \"langgraph_checkpoints.sqlite\"\n",
    "if os.path.exists(db_file):\n",
    "    os.remove(db_file)\n",
    "\n",
    "sqlite_conn = ____.____(db_file, check_same_thread=False) \n",
    "sqlite_saver = ____(conn=____) \n",
    "\n",
    "app_sqlite = workflow_sqlite.compile(checkpointer=____) \n",
    "\n",
    "thread_id_sqlite = str(uuid.uuid4())\n",
    "config_sqlite_1 = {\"configurable\": {\"thread_id\": ____}} \n",
    "\n",
    "print(\"\\n--- SQLite 1回目の実行 (カウンターを1に) ---\")\n",
    "inputs_sqlite_1 = {\"messages\": [HumanMessage(content=\"SQLite最初の呼び出し\")], \"counter\": 0}\n",
    "result_sqlite_1 = app_sqlite.invoke(inputs_sqlite_1, config=config_sqlite_1)\n",
    "print(f\"SQLite 1回目 結果: {result_sqlite_1}\")\n",
    "\n",
    "print(\"\\n--- DBコネクションを一度閉じて再オープン ---\")\n",
    "sqlite_conn.close()\n",
    "sqlite_conn_reopened = sqlite3.connect(db_file, check_same_thread=False)\n",
    "sqlite_saver_reopened = SqliteSaver(conn=sqlite_conn_reopened)\n",
    "app_sqlite_reopened = workflow_sqlite.compile(checkpointer=sqlite_saver_reopened)\n",
    "\n",
    "print(\"\\n--- SQLite 2回目の実行 (状態を再開し、カウンターを2にして終了) ---\")\n",
    "inputs_sqlite_2 = {\"messages\": [HumanMessage(content=\"SQLite 2回目の呼び出し\")]}\n",
    "result_sqlite_2 = app_sqlite_reopened.invoke(inputs_sqlite_2, config=config_sqlite_1)\n",
    "print(f\"SQLite 2回目 結果: {result_sqlite_2}\")\n",
    "\n",
    "sqlite_conn_reopened.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答004</summary>\n",
    "\n",
    "```python\n",
    "# 解答004\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver # SqliteSaverをインポート\n",
    "import sqlite3\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "# --- 状態定義 (State) は問題003と同じものを使用 ---\n",
    "class CheckpointState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    counter: int\n",
    "\n",
    "# --- ノード定義 (Nodes) は問題003と同じものを使用 ---\n",
    "def counter_node_sqlite(state: CheckpointState): # 関数名を変更して区別\n",
    "    print(f\"counter_node_sqlite: 現在のカウンター = {state['counter']}\")\n",
    "    new_counter = state[\"counter\"] + 1\n",
    "    if state[\"messages\"]:\n",
    "        print(f\"counter_node_sqlite: 最新メッセージ = '{state['messages'][-1].content}'\")\n",
    "    return {\"counter\": new_counter, \"messages\": [AIMessage(content=f\"カウンターが {new_counter} になりました (sqlite)\")]}\n",
    "\n",
    "def check_finish_sqlite(state: CheckpointState): # 関数名を変更して区別\n",
    "    if state[\"counter\"] >= 2: # この問題では2回で終了させてシンプルに\n",
    "        print(\"check_finish_sqlite: カウンターが2以上なので終了します。\")\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        print(f\"check_finish_sqlite: カウンターは {state['counter']} なので続行します。\")\n",
    "        return \"continue_counting\"\n",
    "\n",
    "# --- グラフ構築 (Graph) は問題003と同様のロジック ---\n",
    "workflow_sqlite = StateGraph(CheckpointState)\n",
    "workflow_sqlite.add_node(\"counter_sqlite_node\", counter_node_sqlite) # ノード名を変更\n",
    "workflow_sqlite.set_entry_point(\"counter_sqlite_node\")\n",
    "workflow_sqlite.add_conditional_edges(\n",
    "    \"counter_sqlite_node\",\n",
    "    check_finish_sqlite,\n",
    "    {\n",
    "        \"continue_counting\": \"counter_sqlite_node\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- SQLiteチェックポインターの設定 ---\n",
    "db_file = \"langgraph_checkpoints.sqlite\"\n",
    "# 既存のDBファイルがあれば削除してクリーンな状態で開始\n",
    "if os.path.exists(db_file):\n",
    "    os.remove(db_file)\n",
    "    print(f\"既存のDBファイル '{db_file}' を削除しました。\")\n",
    "\n",
    "sqlite_conn = sqlite3.connect(db_file, check_same_thread=False) # sqlite3.connect, check_same_thread=False を追加\n",
    "sqlite_saver = SqliteSaver(conn=sqlite_conn)\n",
    "\n",
    "# グラフのコンパイル時にチェックポインターを渡す\n",
    "app_sqlite = workflow_sqlite.compile(checkpointer=sqlite_saver)\n",
    "\n",
    "# --- グラフの実行と状態の永続化・再開 ---\n",
    "thread_id_sqlite = str(uuid.uuid4())\n",
    "config_sqlite_1 = {\"configurable\": {\"thread_id\": thread_id_sqlite}}\n",
    "\n",
    "print(\"\\n--- SQLite 1回目の実行 (カウンターを1に) ---\")\n",
    "inputs_sqlite_1 = {\"messages\": [HumanMessage(content=\"SQLite最初の呼び出し\")], \"counter\": 0}\n",
    "result_sqlite_1 = app_sqlite.invoke(inputs_sqlite_1, config=config_sqlite_1)\n",
    "print(f\"SQLite 1回目 結果: {result_sqlite_1}\")\n",
    "assert result_sqlite_1['counter'] == 1\n",
    "\n",
    "# ここで一度コネクションを閉じて、再度開くことで、永続化をより明確にテスト\n",
    "print(\"\\n--- DBコネクションを一度閉じて再オープン ---\")\n",
    "sqlite_conn.close()\n",
    "sqlite_conn_reopened = sqlite3.connect(db_file, check_same_thread=False) # check_same_thread=False を追加\n",
    "sqlite_saver_reopened = SqliteSaver(conn=sqlite_conn_reopened)\n",
    "app_sqlite_reopened = workflow_sqlite.compile(checkpointer=sqlite_saver_reopened)\n",
    "\n",
    "print(\"\\n--- SQLite 2回目の実行 (状態を再開し、カウンターを2にして終了) ---\")\n",
    "inputs_sqlite_2 = {\"messages\": [HumanMessage(content=\"SQLite 2回目の呼び出し\")]}\n",
    "# configは同じ thread_id を持つものを使用\n",
    "result_sqlite_2 = app_sqlite_reopened.invoke(inputs_sqlite_2, config=config_sqlite_1)\n",
    "print(f\"SQLite 2回目 結果: {result_sqlite_2}\")\n",
    "assert result_sqlite_2['counter'] == 2\n",
    "\n",
    "print(f\"\\nSQLiteデータベースファイル '{db_file}' が作成/更新されました。\")\n",
    "assert os.path.exists(db_file), f\"DBファイル '{db_file}' が見つかりません。\"\n",
    "\n",
    "# クリーンアップ（通常は不要だが、テスト実行のために）\n",
    "sqlite_conn_reopened.close()\n",
    "# if os.path.exists(db_file):\n",
    "#     os.remove(db_file)\n",
    "#     print(f\"DBファイル '{db_file}' をクリーンアップしました。\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説004</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:**\n",
    "    1.  `SqliteSaver` の基本的な使い方と、SQLiteデータベースへの接続方法。\n",
    "    2.  状態がSQLiteファイルに永続化され、異なるセッションやアプリケーションの再起動後も（同じDBファイルとスレッドIDを使えば）状態を復元できることの理解。\n",
    "    3.  実際にSQLiteデータベースファイル (`.sqlite`) が作業ディレクトリに作成されることの確認。\n",
    "    4.  `sqlite3.connect` 時の `check_same_thread=False` の指定について（LangGraphが内部で異なるスレッドからDBアクセスする可能性があるため）。\n",
    "*   **コード解説:**\n",
    "    *   グラフの定義やロジックは問題003（`MemorySaver`）とほぼ同じですが、カウンターの終了条件を2に変更してシンプルにし、関数名やノード名に `_sqlite` を追加して区別しやすくしています。\n",
    "    *   `db_file = \"langgraph_checkpoints.sqlite\"` でデータベースファイル名を定義します。\n",
    "    *   `if os.path.exists(db_file): os.remove(db_file)`: この部分は、演習を繰り返し実行する際に、前回の状態が残らないようにするためのおまじないです。\n",
    "    *   `sqlite_conn = sqlite3.connect(db_file, check_same_thread=False)` でSQLiteデータベースへの接続を確立します。`check_same_thread=False` は、LangGraphが内部で非同期処理や異なるスレッドからデータベースにアクセスする場合があるため、推奨される設定です。これがないと、スレッド関連のエラーが発生することがあります。\n",
    "    *   `sqlite_saver = SqliteSaver(conn=sqlite_conn)` で、確立した接続を使って`SqliteSaver`のインスタンスを作成します。\n",
    "    *   最初の `invoke` 後、`sqlite_conn.close()` で一度データベース接続を閉じ、再度接続を開いています。これは、状態がメモリだけでなく、実際にDBファイルに書き込まれ、そこから読み出せることをより明確に示すための手順です。\n",
    "    *   2回目の `invoke` では、同じ `thread_id_sqlite` を使って、再オープンされたDB接続を持つアプリケーション (`app_sqlite_reopened`) を呼び出します。\n",
    "    *   最後に `os.path.exists(db_file)` で、実際にデータベースファイルが作成されていることを確認します。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題005: カスタムエージェントの状態に独自の情報を追加・更新する\n",
    "\n",
    "LangGraphの `AgentState` は通常、`messages` フィールドを含みますが、エージェントの動作をより細かく制御したり、追加情報を追跡したりするために、状態をカスタマイズしたい場合があります。例えば、エージェントが特定のツールを何回呼び出したか、特定の状態を何回経験したかなどを記録できます。\n",
    "この問題では、標準のメッセージリストに加えて、カスタムフィールド（例: `tool_call_count`）を状態に定義し、エージェントのノードがそのカスタムフィールドを更新する方法を学びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005\n",
    "from typing import TypedDict, Annotated, Sequence, Literal, Optional\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI \n",
    "import operator\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str):\n",
    "    \"\"\"指定された都市の現在の天気を取得します。\"\"\"\n",
    "    if city == \"東京\":\n",
    "        return \"東京の天気は晴れです。\"\n",
    "    elif city == \"大阪\":\n",
    "        return \"大阪の天気は雨です。\"\n",
    "    else:\n",
    "        return f\"{city}の天気は不明です。\"\n",
    "\n",
    "tools = [get_weather]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "class CustomAgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    tool_call_count: Annotated[int, ____] \n",
    "    last_tool_name: Optional[str] \n",
    "\n",
    "def agent_node(state: CustomAgentState):\n",
    "    print(f\"エージェントノード実行: 現在のツール呼び出し回数 = {state['tool_call_count']}\")\n",
    "    response = ____.____(state[\"messages\"]) \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def tool_executor_node(state: CustomAgentState):\n",
    "    print(\"ツール実行ノード実行\")\n",
    "    ai_message = state[\"messages\"][-1]\n",
    "    tool_calls = ai_message.tool_calls\n",
    "    tool_outputs = []\n",
    "    current_tool_call_count = state.get(\"tool_call_count\", 0)\n",
    "    last_tool_name = state.get(\"last_tool_name\") # 前回のツール名を取得\n",
    "\n",
    "    if tool_calls:\n",
    "        for tc in tool_calls:\n",
    "            selected_tool = {tool.name: tool for tool in tools}[tc[\"name\"]]\n",
    "            tool_output = selected_tool.invoke(tc[\"args\"])\n",
    "            tool_outputs.append(ToolMessage(content=str(tool_output), tool_call_id=tc[\"id\"]))\n",
    "            # current_tool_call_count は Annotated により自動加算されるので、ここでは差分(1)を返す\n",
    "            last_tool_name = tc[\"name\"]\n",
    "            print(f\"  ツール '{tc['name']}' を実行しました。\")\n",
    "    \n",
    "    updates = {\"messages\": tool_outputs}\n",
    "    if tool_calls: # ツールが実際に呼び出された場合のみカウントを増やす\n",
    "        updates[____] = 1 # operator.add が現在の値にこれを加算する\n",
    "    if last_tool_name:\n",
    "        updates[____] = last_tool_name\n",
    "    return updates\n",
    "\n",
    "def should_continue(state: CustomAgentState) -> Literal[\"tools_node\", \"__end__\"]:\n",
    "    if isinstance(state[\"messages\"][-1], AIMessage) and state[\"messages\"][-1].tool_calls:\n",
    "        return \"tools_node\"\n",
    "    return \"__end__\"\n",
    "\n",
    "custom_agent_workflow = ____(CustomAgentState) \n",
    "\n",
    "custom_agent_workflow.add_node(\"agent\", agent_node)\n",
    "custom_agent_workflow.add_node(\"tools_node\", tool_executor_node)\n",
    "custom_agent_workflow.set_entry_point(\"agent\")\n",
    "custom_agent_workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools_node\": \"tools_node\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "custom_agent_workflow.add_edge(\"tools_node\", \"agent\")\n",
    "\n",
    "app_custom_agent = custom_agent_workflow.____() # compile\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"東京の天気を教えてください。その後、大阪の天気もお願いします。\") ],\n",
    "    \"tool_call_count\": ____, \n",
    "    \"last_tool_name\": None\n",
    "}\n",
    "print(f\"\\n--- カスタムエージェント実行 (初期状態: {initial_state}) ---\")\n",
    "final_state = app_custom_agent.invoke(initial_state)\n",
    "\n",
    "print(\"\\n--- 最終状態 ---\")\n",
    "for message in final_state[\"messages\"]:\n",
    "    print(f\"  {message.type}: {message.content}\")\n",
    "    if isinstance(message, AIMessage) and message.tool_calls:\n",
    "        print(f\"    ツール呼び出し: {message.tool_calls}\")\n",
    "print(f\"最終ツール呼び出し回数: {final_state['tool_call_count']}\")\n",
    "print(f\"最後に呼び出されたツール: {final_state['last_tool_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答005</summary>\n",
    "\n",
    "```python\n",
    "# 解答005\n",
    "from typing import TypedDict, Annotated, Sequence, Literal, Optional\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI # この問題ではLLMを直接使うためインポート\n",
    "import operator\n",
    "\n",
    "# --- ツールの定義 ---\n",
    "@tool\n",
    "def get_weather(city: str):\n",
    "    \"\"\"指定された都市の現在の天気を取得します。\"\"\"\n",
    "    if city == \"東京\":\n",
    "        return \"東京の天気は晴れです。\"\n",
    "    elif city == \"大阪\":\n",
    "        return \"大阪の天気は雨です。\"\n",
    "    else:\n",
    "        return f\"{city}の天気は不明です。\"\n",
    "\n",
    "tools = [get_weather]\n",
    "# OpenAIファンクションコーリングを利用する場合、LLMにツールをバインドする\n",
    "# ここではノートブック冒頭の `llm` を使う想定だが、この問題専用のLLMインスタンスを作成しても良い\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# --- カスタム状態定義 (State) ---\n",
    "class CustomAgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    tool_call_count: Annotated[int, operator.add] # ツール呼び出し回数をカウントするカスタムフィールド\n",
    "    last_tool_name: Optional[str] # 最後に呼び出されたツール名（デモ用）\n",
    "\n",
    "# --- エージェントノード定義 ---\n",
    "def agent_node(state: CustomAgentState):\n",
    "    print(f\"エージェントノード実行: 現在のツール呼び出し回数 = {state['tool_call_count']}\")\n",
    "    # LLMに必要な情報を渡して応答を生成\n",
    "    # state['messages'] にはユーザー入力や過去のAI応答、ツール結果が含まれる\n",
    "    response = llm_with_tools.invoke(state[\"messages\"]) \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# --- ツール実行ノード定義 ---\n",
    "def tool_executor_node(state: CustomAgentState):\n",
    "    print(\"ツール実行ノード実行\")\n",
    "    ai_message = state[\"messages\"][-1] # 直前のAIメッセージ（ツール呼び出しを含むはず）\n",
    "    tool_calls = ai_message.tool_calls\n",
    "    tool_outputs = []\n",
    "    # tool_call_count は Annotated の operator.add で自動加算されるので、ここでは更新差分を返す\n",
    "    # last_tool_name は単純な上書き\n",
    "    num_tool_calls_in_this_step = 0\n",
    "    current_last_tool_name = None\n",
    "\n",
    "    if tool_calls:\n",
    "        for tc in tool_calls:\n",
    "            selected_tool = {tool.name: tool for tool in tools}[tc[\"name\"]]\n",
    "            tool_output = selected_tool.invoke(tc[\"args\"])\n",
    "            tool_outputs.append(ToolMessage(content=str(tool_output), tool_call_id=tc[\"id\"]))\n",
    "            num_tool_calls_in_this_step += 1\n",
    "            current_last_tool_name = tc[\"name\"]\n",
    "            print(f\"  ツール '{tc['name']}' を実行しました。 (このステップでの呼び出し: {num_tool_calls_in_this_step})\")\n",
    "    \n",
    "    updates = {\"messages\": tool_outputs}\n",
    "    if num_tool_calls_in_this_step > 0:\n",
    "        updates[\"tool_call_count\"] = num_tool_calls_in_this_step \n",
    "    if current_last_tool_name:\n",
    "        updates[\"last_tool_name\"] = current_last_tool_name\n",
    "    return updates\n",
    "\n",
    "# --- ルーティングロジック ---\n",
    "def should_continue(state: CustomAgentState) -> Literal[\"tools_node\", \"__end__\"]:\n",
    "    if isinstance(state[\"messages\"][-1], AIMessage) and state[\"messages\"][-1].tool_calls:\n",
    "        print(\"ルーティング: ツール呼び出しあり -> tool_executor_node へ\")\n",
    "        return \"tools_node\"\n",
    "    print(\"ルーティング: ツール呼び出しなし -> 終了\")\n",
    "    return \"__end__\"\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "custom_agent_workflow = StateGraph(CustomAgentState) \n",
    "\n",
    "custom_agent_workflow.add_node(\"agent\", agent_node)\n",
    "custom_agent_workflow.add_node(\"tools_node\", tool_executor_node)\n",
    "\n",
    "custom_agent_workflow.set_entry_point(\"agent\")\n",
    "\n",
    "custom_agent_workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools_node\": \"tools_node\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "custom_agent_workflow.add_edge(\"tools_node\", \"agent\") # ツール実行後、再度エージェントノードへ\n",
    "\n",
    "app_custom_agent = custom_agent_workflow.compile()\n",
    "\n",
    "# --- グラフ実行 ---\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"東京の天気を教えてください。その後、大阪の天気もお願いします。\") ],\n",
    "    \"tool_call_count\": 0, \n",
    "    \"last_tool_name\": None\n",
    "}\n",
    "print(f\"\\n--- カスタムエージェント実行 (初期状態: {initial_state}) ---\")\n",
    "final_state = app_custom_agent.invoke(initial_state)\n",
    "\n",
    "print(\"\\n--- 最終状態 ---\")\n",
    "for message in final_state[\"messages\"]:\n",
    "    print(f\"  {message.type}: {message.content}\")\n",
    "    if isinstance(message, AIMessage) and message.tool_calls:\n",
    "        print(f\"    ツール呼び出し: {message.tool_calls}\")\n",
    "print(f\"最終ツール呼び出し回数: {final_state['tool_call_count']}\")\n",
    "print(f\"最後に呼び出されたツール: {final_state['last_tool_name']}\")\n",
    "\n",
    "assert final_state['tool_call_count'] >= 1, \"ツール呼び出し回数がカウントされていません\"\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説005</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:**\n",
    "    1.  `TypedDict` を使って、グラフの状態に独自のフィールド（例: `tool_call_count`, `last_tool_name`）を追加する方法。\n",
    "    2.  `Annotated[int, operator.add]` のようにアノテーションとリデューサー関数 (この場合は `operator.add`) を組み合わせることで、状態の特定フィールドがどのように更新されるかを定義する方法。これにより、ノードが新しい値を返すだけで、グラフが自動的に値を加算してくれます。\n",
    "    3.  ノード関数内でこれらのカスタム状態フィールドを読み書きする方法。\n",
    "    4.  ツール呼び出しを含む標準的なエージェントループ（エージェントノード -> ツール実行ノード -> エージェントノード）の中で、カスタム状態がどのように維持・更新されるか。\n",
    "*   **コード解説:**\n",
    "    *   `CustomAgentState`:\n",
    "        *   `messages: Annotated[Sequence[BaseMessage], add_messages]` は標準的なメッセージ履歴です。\n",
    "        *   `tool_call_count: Annotated[int, operator.add]` がカスタムフィールドです。`operator.add` を指定することで、このフィールドに新しい値が返されるたびに、既存の値に加算されます。初期値は `invoke` 時に指定します（この例では0）。このリデューサーのおかげで、`tool_executor_node` はそのステップで実行されたツール呼び出しの数（例えば1）を返すだけで、LangGraphが自動的に総カウントに加算します。\n",
    "        *   `last_tool_name: Optional[str]` もカスタムフィールドで、最後に呼び出されたツール名を保持します。こちらは単純な上書きで更新されます（リデューサーなし）。\n",
    "    *   `agent_node`:\n",
    "        *   LLM（`llm_with_tools`）を呼び出し、応答（AIメッセージ、ツール呼び出しを含む可能性あり）を返します。このノード自体は `tool_call_count` を直接変更しません。\n",
    "    *   `tool_executor_node`:\n",
    "        *   AIメッセージからツール呼び出し情報を取得し、対応するツールを実行します。\n",
    "        *   `num_tool_calls_in_this_step` で、このノードの1回の実行中に行われたツール呼び出しの数をカウントします。\n",
    "        *   返す辞書に `{\"tool_call_count\": num_tool_calls_in_this_step}` を含めます。`operator.add` のアノテーションにより、グラフはこの値を既存の `tool_call_count` に「加算」します。\n",
    "        *   `last_tool_name` も更新します。\n",
    "    *   `should_continue`:\n",
    "        *   エージェントの応答にツール呼び出しが含まれていれば `tools_node` へ、そうでなければグラフを終了します。\n",
    "    *   グラフの実行:\n",
    "        *   `initial_state` で `tool_call_count: 0` と `last_tool_name: None` を初期値として設定します。\n",
    "        *   エージェントはまず「東京の天気」を尋ねるツール呼び出しを行い、`tool_executor_node` で `tool_call_count` が1に（0+1）、`last_tool_name` が `get_weather` になります。\n",
    "        *   次にエージェントは「大阪の天気」を尋ねるツール呼び出しを行い、`tool_executor_node` で `tool_call_count` が2に（1+1）なります。\n",
    "        *   最終的に、`final_state` でこれらのカスタムフィールドが正しく更新されていることを確認します。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題006: エージェントスウォームの概念: スーパーバイザーとワーカーエージェント\n",
    "\n",
    "エージェントスウォームは、複数の専門エージェントが協調して複雑なタスクを解決するアーキテクチャです。多くの場合、タスクを適切なワーカーエージェントに振り分ける「スーパーバイザー」エージェントが存在します。\n",
    "この問題では、エージェントスウォームの基本的な概念を理解するために、非常にシンプルなスーパーバイザーと2つのワーカーエージェント（実際には単純なノード）からなるグラフを作成します。スーパーバイザーは、入力メッセージの内容に基づいて、タスクを「一般的な質問応答ワーカー」または「天気情報ワーカー」のいずれかにルーティングします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄006\n",
    "from typing import TypedDict, Annotated, Literal, Optional, List\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class SwarmState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    next_node: Optional[str] \n",
    "    task_description: str      \n",
    "    result: Optional[str]      \n",
    "\n",
    "# --- ワーカーノード定義 ---\n",
    "def general_qa_worker_node(state: SwarmState):\n",
    "    print(f\"汎用QAワーカー実行: タスク「{state['task_description']}」\")\n",
    "    response = f\"汎用QAワーカーがタスク「{state['task_description']}」を処理しました。\"\n",
    "    return {\"result\": response, \"messages\": [AIMessage(content=response)]}\n",
    "\n",
    "def weather_info_worker_node(state: SwarmState):\n",
    "    print(f\"天気情報ワーカー実行: タスク「{state['task_description']}」\")\n",
    "    if \"天気\" in state['task_description']:\n",
    "        response = f\"天気情報ワーカー: {state['task_description']}の天気は快晴です。\"\n",
    "    else:\n",
    "        response = f\"天気情報ワーカー: 天気に関するタスクではありません。「{state['task_description']}」\"\n",
    "    return {\"result\": response, \"messages\": [AIMessage(content=response)]}\n",
    "\n",
    "# --- スーパーバイザーノード定義 (ルーティングロジック) ---\n",
    "def supervisor_node(state: SwarmState) -> dict[str, str]:\n",
    "    print(\"スーパーバイザー実行\")\n",
    "    task_desc = state[\"task_description\"]\n",
    "    print(f\"  タスク内容: 「{task_desc}」\")\n",
    "    \n",
    "    if \"天気\" in task_desc.lower() or \"weather\" in task_desc.lower():\n",
    "        print(\"  ルーティング先: 天気情報ワーカー\")\n",
    "        return { \"next_node\": ____ } \n",
    "    else:\n",
    "        print(\"  ルーティング先: 汎用QAワーカー\")\n",
    "        return { \"next_node\": ____ } \n",
    "\n",
    "# --- グラフ構築 ---\n",
    "swarm_workflow = ____(SwarmState) \n",
    "\n",
    "swarm_workflow.add_node(\"supervisor\", supervisor_node)\n",
    "swarm_workflow.add_node(\"general_qa_worker\", general_qa_worker_node)\n",
    "swarm_workflow.add_node(\"weather_worker\", weather_info_worker_node)\n",
    "\n",
    "swarm_workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "swarm_workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda state: state[____], \n",
    "    {\n",
    "        \"general_qa_worker\": \"general_qa_worker\",\n",
    "        \"weather_worker\": \"weather_worker\",\n",
    "    }\n",
    ")\n",
    "\n",
    "swarm_workflow.add_edge(\"general_qa_worker\", ____) \n",
    "swarm_workflow.add_edge(\"weather_worker\", ____) \n",
    "\n",
    "app_swarm = swarm_workflow.compile()\n",
    "\n",
    "# --- グラフ実行テスト ---\n",
    "print(\"\\n--- スウォーム実行テスト1 (天気タスク) ---\")\n",
    "initial_input_weather = {\n",
    "    \"messages\": [HumanMessage(content=\"今日の東京の天気を教えて\")],\n",
    "    \"task_description\": \"今日の東京の天気を教えて\", \n",
    "}\n",
    "final_state_weather = app_swarm.invoke(initial_input_weather)\n",
    "print(f\"  最終結果 (天気): {final_state_weather.get('result')}\")\n",
    "\n",
    "print(\"\\n--- スウォーム実行テスト2 (一般タスク) ---\")\n",
    "initial_input_qa = {\n",
    "    \"messages\": [HumanMessage(content=\"LangGraphとは何ですか？\")],\n",
    "    \"task_description\": \"LangGraphとは何ですか？\",\n",
    "}\n",
    "final_state_qa = app_swarm.invoke(initial_input_qa)\n",
    "print(f\"  最終結果 (QA): {final_state_qa.get('result')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答006</summary>\n",
    "\n",
    "```python\n",
    "# 解答006\n",
    "from typing import TypedDict, Annotated, Literal, Optional, List\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class SwarmState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    next_node: Optional[str] # 次に実行するノード名を保持\n",
    "    task_description: str      # 現在のタスク記述\n",
    "    result: Optional[str]      # ワーカーからの結果\n",
    "\n",
    "# --- ワーカーノード定義 ---\n",
    "def general_qa_worker_node(state: SwarmState):\n",
    "    print(f\"汎用QAワーカー実行: タスク「{state['task_description']}」\")\n",
    "    # 実際にはLLM呼び出しなどを行う\n",
    "    response = f\"汎用QAワーカーがタスク「{state['task_description']}」を処理しました。\"\n",
    "    return {\"result\": response, \"messages\": [AIMessage(content=response)]}\n",
    "\n",
    "def weather_info_worker_node(state: SwarmState):\n",
    "    print(f\"天気情報ワーカー実行: タスク「{state['task_description']}」\")\n",
    "    # 実際には天気ツール呼び出しなどを行う\n",
    "    if \"天気\" in state['task_description']:\n",
    "        response = f\"天気情報ワーカー: {state['task_description']}の天気は快晴です。\"\n",
    "    else:\n",
    "        response = f\"天気情報ワーカー: 天気に関するタスクではありません。「{state['task_description']}」\"\n",
    "    return {\"result\": response, \"messages\": [AIMessage(content=response)]}\n",
    "\n",
    "# --- スーパーバイザーノード定義 (ルーティングロジック) ---\n",
    "def supervisor_node(state: SwarmState) -> dict[str, str]:\n",
    "    print(\"スーパーバイザー実行\")\n",
    "    # messagesから最新のユーザー入力を取得\n",
    "    # この例では、簡略化のため initial_input で task_description を直接設定\n",
    "    task_desc = state[\"task_description\"]\n",
    "    print(f\"  タスク内容: 「{task_desc}」\")\n",
    "    \n",
    "    if \"天気\" in task_desc.lower() or \"weather\" in task_desc.lower():\n",
    "        print(\"  ルーティング先: 天気情報ワーカー\")\n",
    "        return { \"next_node\": \"weather_worker\" }\n",
    "    else:\n",
    "        print(\"  ルーティング先: 汎用QAワーカー\")\n",
    "        return { \"next_node\": \"general_qa_worker\" }\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "swarm_workflow = StateGraph(SwarmState)\n",
    "\n",
    "swarm_workflow.add_node(\"supervisor\", supervisor_node)\n",
    "swarm_workflow.add_node(\"general_qa_worker\", general_qa_worker_node)\n",
    "swarm_workflow.add_node(\"weather_worker\", weather_info_worker_node)\n",
    "\n",
    "swarm_workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "# スーパーバイザーから各ワーカーへの条件付きエッジ\n",
    "swarm_workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda state: state[\"next_node\"], # next_node の値に基づいて遷移\n",
    "    {\n",
    "        \"general_qa_worker\": \"general_qa_worker\",\n",
    "        \"weather_worker\": \"weather_worker\",\n",
    "        # 想定外の next_node の値の場合のフォールバック (オプション)\n",
    "        # \"__end__\": END \n",
    "    }\n",
    ")\n",
    "\n",
    "# 各ワーカーの処理が終わったらグラフを終了\n",
    "swarm_workflow.add_edge(\"general_qa_worker\", END)\n",
    "swarm_workflow.add_edge(\"weather_worker\", END)\n",
    "\n",
    "app_swarm = swarm_workflow.compile()\n",
    "\n",
    "# --- グラフ実行テスト ---\n",
    "print(\"\\n--- スウォーム実行テスト1 (天気タスク) ---\")\n",
    "initial_input_weather = {\n",
    "    \"messages\": [HumanMessage(content=\"今日の東京の天気を教えて\")],\n",
    "    \"task_description\": \"今日の東京の天気を教えて\", # スーパーバイザーがこの情報を使う\n",
    "}\n",
    "final_state_weather = app_swarm.invoke(initial_input_weather)\n",
    "print(f\"  最終結果 (天気): {final_state_weather.get('result')}\")\n",
    "assert \"快晴\" in final_state_weather.get('result', \"\"), \"天気の応答が期待通りではありません。\"\n",
    "\n",
    "print(\"\\n--- スウォーム実行テスト2 (一般タスク) ---\")\n",
    "initial_input_qa = {\n",
    "    \"messages\": [HumanMessage(content=\"LangGraphとは何ですか？\")],\n",
    "    \"task_description\": \"LangGraphとは何ですか？\",\n",
    "}\n",
    "final_state_qa = app_swarm.invoke(initial_input_qa)\n",
    "print(f\"  最終結果 (QA): {final_state_qa.get('result')}\")\n",
    "assert \"汎用QAワーカー\" in final_state_qa.get('result', \"\"), \"QA応答が期待通りではありません。\"\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説006</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:**\n",
    "    1.  エージェントスウォームにおけるスーパーバイザーの役割（タスクの振り分け）の概念的な理解。\n",
    "    2.  状態 (`SwarmState`) にルーティング情報 (`next_node`) を持たせ、スーパーバイザーノードがこの情報を更新する方法。\n",
    "    3.  `add_conditional_edges` を使用して、スーパーバイザーノードの決定に基づいて異なるワーカーノードに処理を分岐させる方法。\n",
    "    4.  ワーカーノードがそれぞれの専門タスクを処理し、結果を状態に返す基本的な流れ。\n",
    "*   **コード解説:**\n",
    "    *   `SwarmState`:\n",
    "        *   `messages`: 会話履歴。\n",
    "        *   `next_node`: スーパーバイザーが次にどのワーカーを実行すべきかを格納します。\n",
    "        *   `task_description`: 現在処理中のタスクの記述。実際のスーパーバイザーは `messages` からこれを抽出・生成しますが、この例では簡略化のため初期入力で与えます。\n",
    "        *   `result`: ワーカーノードからの処理結果を格納します。\n",
    "    *   `general_qa_worker_node`, `weather_info_worker_node`:\n",
    "        *   それぞれの専門タスクを模倣する単純な関数です。実際のアプリケーションでは、これらのノードは自身がLLMエージェントであったり、特定のツールセットを持っていたりします。\n",
    "        *   処理結果を `result` フィールドに、そしてユーザー向けの応答を `messages` に追加して返します。\n",
    "    *   `supervisor_node`:\n",
    "        *   `task_description` を見て、タスクの内容に応じて `next_node` の値を `\"weather_worker\"` または `\"general_qa_worker\"` に設定します。\n",
    "        *   このノードが返すのは、状態の更新差分 (`{\"next_node\": ...}`) です。\n",
    "    *   グラフ構築:\n",
    "        *   スーパーバイザーノードと2つのワーカーノードを追加します。\n",
    "        *   エントリーポイントは `supervisor` です。\n",
    "        *   `add_conditional_edges`:\n",
    "            *   `supervisor` ノードの後に実行されます。\n",
    "            *   2番目の引数 `lambda state: state[\"next_node\"]` は、状態の `next_node` フィールドの値（例: `\"weather_worker\"`）を返します。\n",
    "            *   3番目の引数の辞書は、この返された値と実際の遷移先ノード名をマッピングします。例えば、`next_node` が `\"weather_worker\"` なら `weather_worker` ノードに遷移します。\n",
    "        *   各ワーカーノードの処理が終わったら `END` に遷移し、グラフを終了します。\n",
    "    *   実行テスト:\n",
    "        *   「天気タスク」と「一般タスク」の2つのシナリオでグラフを実行し、スーパーバイザーが正しくルーティングし、適切なワーカーが応答を生成することを確認します。\n",
    "*   **補足:**\n",
    "    *   これは非常に単純化されたスウォームの例です。実際の高度なエージェントスウォームでは、スーパーバイザーが複数のワーカーに並列でタスクを依頼したり、ワーカー間で連携したり、結果を集約したりするなど、より複雑なロジックを持ちます。\n",
    "    *   LangGraphの `Pregel` エンジンは、このような複雑なマルチエージェントのワークフローを構築するのに非常に強力です。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今後の実装に向けた空のコードセル"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
