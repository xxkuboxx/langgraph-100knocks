{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章: 発展技術と運用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "以下のセルを順番に実行して、演習に必要な環境をセットアップします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "\n",
    "このセルは、LangGraphおよび関連するLangChainライブラリ、永続化のための`psycopg2-binary`（PostgreSQL用）や`sqlite3`（Python標準）、デバッグ用の`langsmith`などをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ライブラリのインストール ===\n",
    "!pip install -qU langchain langgraph langchain_openai psycopg2-binary langsmith\n",
    "\n",
    "# --- その他の推奨ライブラリ ---\n",
    "!pip install -qU python-dotenv pygraphviz pydotplus graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIキー/環境変数の設定\n",
    "\n",
    "このノートブックでは、いくつかの問題でOpenAIのLLMやLangSmithのトレーシング機能を利用します。\n",
    "事前に以下の環境変数を設定するか、Google Colabの場合はColabのシークレットマネージャーに登録してください。\n",
    "\n",
    "*   `OPENAI_API_KEY`: OpenAIのAPIキー\n",
    "*   `LANGCHAIN_API_KEY`: LangSmithのAPIキー (LangSmithを利用する場合)\n",
    "*   `LANGCHAIN_TRACING_V2=\"true\"`: LangSmithトレーシングを有効化\n",
    "*   `LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"`: LangSmithのエンドポイント\n",
    "*   `LANGCHAIN_PROJECT=\"[Your LangSmith Project Name]\"`: LangSmithのプロジェクト名（任意の名前に置き換えてください）\n",
    "\n",
    "`.env` ファイルを使用する場合:\n",
    "1. リポジトリのルートにある `.env.sample` をコピーして `.env` という名前のファイルを作成します。\n",
    "2. `.env` ファイルを開き、上記の各キーに対応する値を記述して保存します。\n",
    "\n",
    "**注意:** LangSmithの設定はオプションですが、問題005でその連携を試すためには設定が推奨されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_env_var(var_name, secret_manager_fn=None, is_colab_env=False, required=False, default_value=None):\n",
    "    value = os.getenv(var_name)\n",
    "    if not value and is_colab_env and secret_manager_fn:\n",
    "        try:\n",
    "            value = secret_manager_fn(var_name)\n",
    "            if value: print(f\"{var_name} をColabシークレットからロードしました。\")\n",
    "        except Exception:\n",
    "            if required and not default_value:\n",
    "                print(f\"警告: Colabシークレット {var_name} が見つかりません。\")\n",
    "            value = None # エラー時もNoneを返す\n",
    "    \n",
    "    if not value and default_value:\n",
    "        print(f\"{var_name} が未設定のため、デフォルト値「{default_value}」を使用します。\")\n",
    "        value = default_value\n",
    "        \n",
    "    if required and not value:\n",
    "        raise ValueError(f\"{var_name} が設定されていません。環境変数またはColabシークレットを確認してください。\")\n",
    "    elif not value and not required:\n",
    "        print(f\"{var_name} は設定されていません（オプション）。\")\n",
    "    elif value and var_name != \"OPENAI_API_KEY\": # APIキー自体は表示しない\n",
    "        print(f\"{var_name} が設定されました。\")\n",
    "    elif value and var_name == \"OPENAI_API_KEY\":\n",
    "        print(f\"OPENAI_API_KEY が設定されました。\")\n",
    "    \n",
    "    if value: # 環境変数として実際に設定する\n",
    "        os.environ[var_name] = value\n",
    "    return value\n",
    "\n",
    "IS_COLAB = False\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IS_COLAB = True\n",
    "    colab_secrets_getter = userdata.get\n",
    "except ImportError:\n",
    "    colab_secrets_getter = lambda key: None # Colabでない場合は何もしない関数\n",
    "\n",
    "# OpenAI APIキー (必須)\n",
    "OPENAI_API_KEY = get_env_var(\"OPENAI_API_KEY\", colab_secrets_getter, IS_COLAB, required=True)\n",
    "\n",
    "# LangSmith設定 (オプション)\n",
    "LANGCHAIN_TRACING_V2 = get_env_var(\"LANGCHAIN_TRACING_V2\", colab_secrets_getter, IS_COLAB, default_value=\"false\")\n",
    "if LANGCHAIN_TRACING_V2.lower() == \"true\":\n",
    "    LANGCHAIN_API_KEY = get_env_var(\"LANGCHAIN_API_KEY\", colab_secrets_getter, IS_COLAB, required=True)\n",
    "    LANGCHAIN_ENDPOINT = get_env_var(\"LANGCHAIN_ENDPOINT\", colab_secrets_getter, IS_COLAB, default_value=\"https://api.smith.langchain.com\")\n",
    "    LANGCHAIN_PROJECT = get_env_var(\"LANGCHAIN_PROJECT\", colab_secrets_getter, IS_COLAB, default_value=\"langgraph-100knocks-ch5\")\n",
    "else:\n",
    "    print(\"LangSmithトレーシングは無効です。問題005を試す場合は有効にしてください。\")\n",
    "\n",
    "# LLMの準備\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "print(f\"LLM ({llm.model_name}) の準備ができました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題001: グラフ実行のストリーミング出力（状態のリアルタイム取得）\n",
    "\n",
    "### 課題\n",
    "LangGraphのグラフ実行時、最終結果だけでなく、各ノードが実行されるたびに更新される状態（State）をリアルタイムで取得したい場合があります。これにより、処理の進行状況をユーザーに逐次表示したり、デバッグに役立てたりすることができます。この問題では、`graph.stream()`メソッドを使用し、グラフの実行過程で状態がどのように変化していくかを観測します。\n",
    "\n",
    "*   **学習内容:** `graph.stream()`メソッドの基本的な使い方と、返されるイベントの構造を理解します。各イベントがどのノードに対応し、その時点でどのような状態になっているかを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import time\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class StreamingState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    current_step_name: Optional[str]\n",
    "    processed_data: List[str]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def node_alpha(state: StreamingState):\n",
    "    print(\"Node Alpha: 実行中...\")\n",
    "    time.sleep(0.5) # 処理を模倣\n",
    "    new_data = \"アルファノード処理完了\"\n",
    "    return {\"current_step_name\": \"Alpha\", \"processed_data\": state.get(\"processed_data\", []) + [new_data], \"messages\": [AIMessage(content=new_data, name=\"Alpha\")]}\n",
    "\n",
    "def node_beta(state: StreamingState):\n",
    "    print(\"Node Beta: 実行中...\")\n",
    "    time.sleep(0.5)\n",
    "    new_data = \"ベータノード処理完了\"\n",
    "    return {\"current_step_name\": \"Beta\", \"processed_data\": state.get(\"processed_data\", []) + [new_data], \"messages\": [AIMessage(content=new_data, name=\"Beta\")]}\n",
    "\n",
    "def node_gamma(state: StreamingState):\n",
    "    print(\"Node Gamma: 実行中...\")\n",
    "    time.sleep(0.5)\n",
    "    new_data = \"ガンマノード処理完了、最終ステップです。\"\n",
    "    return {\"current_step_name\": \"Gamma\", \"processed_data\": state.get(\"processed_data\", []) + [new_data], \"messages\": [AIMessage(content=new_data, name=\"Gamma\")]}\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(StreamingState)\n",
    "workflow.add_node(\"alpha_processor\", node_alpha)\n",
    "workflow.add_node(\"beta_processor\", node_beta)\n",
    "workflow.add_node(\"gamma_processor\", node_gamma)\n",
    "\n",
    "workflow.set_entry_point(\"alpha_processor\")\n",
    "workflow.add_edge(\"alpha_processor\", \"beta_processor\")\n",
    "workflow.add_edge(\"beta_processor\", \"gamma_processor\")\n",
    "workflow.add_edge(\"gamma_processor\", END)\n",
    "\n",
    "graph_streaming = workflow.compile()\n",
    "\n",
    "# --- グラフ実行のストリーミング ---\n",
    "print(\"--- グラフ実行のストリーミング開始 ---\")\n",
    "initial_input_streaming = {\"messages\": [HumanMessage(content=\"ストリーミング処理を開始してください\")], \"processed_data\": []}\n",
    "\n",
    "for i, event_chunk in enumerate(graph_streaming.stream(initial_input_streaming, {\"recursion_limit\": 10})):\n",
    "    print(f\"\\nイベント {i+1}:\")\n",
    "    # event_chunk は {ノード名: 更新された状態} という形式の辞書\n",
    "    for node_name, partial_state_update in event_chunk.items():\n",
    "        print(f\"  ノード '{node_name}' からの出力:\")\n",
    "        print(f\"    現在のステップ名: {partial_state_update.get('current_step_name')}\")\n",
    "        print(f\"    処理済みデータリスト: {partial_state_update.get('processed_data')}\")\n",
    "        if partial_state_update.get('messages'):\n",
    "            print(f\"    最新メッセージ: {partial_state_update['messages'][-1].pretty_repr()[:100]}...\")\n",
    "print(\"\\n--- ストリーミング終了 ---\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import time\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class StreamingState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages] # 処理ログやメッセージ履歴\n",
    "    current_step_name: Optional[str]                   # 現在実行が完了したノード名\n",
    "    processed_data_log: List[str]                      # 各ノードで処理されたデータのログ\n",
    "    final_result_summary: Optional[str]                # 最終的な結果の要約\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def node_A_processing(state: StreamingState):\n",
    "    step_name = \"Node_A\"\n",
    "    print(f\"{step_name}: 実行中... データ処理を開始します。\")\n",
    "    time.sleep(0.3) # 処理時間を模倣\n",
    "    log_entry = f\"{step_name} - 初期データ処理完了。\"\n",
    "    updated_log = state.get(\"processed_data_log\", []) + [log_entry]\n",
    "    return {\n",
    "        \"current_step_name\": step_name,\n",
    "        \"processed_data_log\": updated_log,\n",
    "        \"messages\": [AIMessage(content=log_entry, name=step_name)]\n",
    "    }\n",
    "\n",
    "def node_B_analysis(state: StreamingState):\n",
    "    step_name = \"Node_B\"\n",
    "    previous_log_count = len(state.get(\"processed_data_log\", []))\n",
    "    print(f\"{step_name}: 実行中... これまでのログ件数 {previous_log_count} 件を元に分析します。\")\n",
    "    time.sleep(0.4)\n",
    "    log_entry = f\"{step_name} - データ分析完了。洞察Xを発見。\"\n",
    "    updated_log = state.get(\"processed_data_log\", []) + [log_entry]\n",
    "    return {\n",
    "        \"current_step_name\": step_name,\n",
    "        \"processed_data_log\": updated_log,\n",
    "        \"messages\": [AIMessage(content=log_entry, name=step_name)]\n",
    "    }\n",
    "\n",
    "def node_C_summarization(state: StreamingState):\n",
    "    step_name = \"Node_C\"\n",
    "    all_logs = state.get(\"processed_data_log\", [])\n",
    "    print(f\"{step_name}: 実行中... 全ログ ({len(all_logs)}件) を要約します。\")\n",
    "    time.sleep(0.2)\n",
    "    summary = f\"最終要約: 全{len(all_logs)}ステップの処理が完了しました。主要な成果は「洞察X」です。\"\n",
    "    log_entry = f\"{step_name} - 全体要約完了。\"\n",
    "    updated_log = all_logs + [log_entry]\n",
    "    return {\n",
    "        \"current_step_name\": step_name,\n",
    "        \"processed_data_log\": updated_log,\n",
    "        \"final_result_summary\": summary,\n",
    "        \"messages\": [AIMessage(content=summary, name=step_name)] # 最終結果をメッセージにも\n",
    "    }\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(StreamingState)\n",
    "\n",
    "workflow.add_node(\"step_A_processor\", node_A_processing)\n",
    "workflow.add_node(\"step_B_analyzer\", node_B_analysis)\n",
    "workflow.add_node(\"step_C_summarizer\", node_C_summarization)\n",
    "\n",
    "workflow.set_entry_point(\"step_A_processor\")\n",
    "workflow.add_edge(\"step_A_processor\", \"step_B_analyzer\")\n",
    "workflow.add_edge(\"step_B_analyzer\", \"step_C_summarizer\")\n",
    "workflow.add_edge(\"step_C_summarizer\", END)\n",
    "\n",
    "graph_for_streaming = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "try:\n",
    "    display(Image(graph_for_streaming.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- グラフ実行のストリーミングと状態変化の観測 ---\n",
    "print(\"\\n--- グラフ実行のストリーミング開始 --- (各ノード完了時の状態を表示)\")\n",
    "initial_streaming_input = {\n",
    "    \"messages\": [HumanMessage(content=\"シーケンシャルなデータ処理パイプラインのストリーミング実行を開始します。\")],\n",
    "    \"processed_data_log\": [] # 初期ログは空\n",
    "}\n",
    "\n",
    "streamed_events_count = 0\n",
    "final_streamed_state = None\n",
    "\n",
    "for i, event_update_chunk in enumerate(graph_for_streaming.stream(initial_streaming_input, {\"recursion_limit\": 10})):\n",
    "    streamed_events_count += 1\n",
    "    print(f\"\\nイベントチャンク {i+1}:\")\n",
    "    # event_update_chunk は {ノード名: 更新された状態の差分または全体} という形式の辞書\n",
    "    # (streamの実装により、差分(patch)か完全な状態か異なる場合がある。通常は更新後の完全な状態が入るキーを持つ)\n",
    "    for node_name_key, state_at_node_completion in event_update_chunk.items():\n",
    "        print(f\"  ノード '{node_name_key}' の実行完了時の状態スナップショット:\")\n",
    "        print(f\"    current_step_name: {state_at_node_completion.get('current_step_name')}\")\n",
    "        print(f\"    processed_data_log: {state_at_node_completion.get('processed_data_log')}\")\n",
    "        print(f\"    final_result_summary: {state_at_node_completion.get('final_result_summary')}\")\n",
    "        current_messages = state_at_node_completion.get('messages', [])\n",
    "        if current_messages:\n",
    "            print(f\"    最新メッセージ ({len(current_messages)}件中): {current_messages[-1].pretty_repr()[:120]}...\")\n",
    "        final_streamed_state = state_at_node_completion # 最後のイベントの状態を保持\n",
    "\n",
    "print(f\"\\n--- ストリーミング終了 ({streamed_events_count} イベント) ---\")\n",
    "\n",
    "# invokeでも最終状態を確認（streamの最後の状態と一致するはず）\n",
    "final_state_via_invoke = graph_for_streaming.invoke(initial_streaming_input, {\"recursion_limit\": 10})\n",
    "print(\"\\nInvokeによる最終状態:\")\n",
    "print(f\"  final_result_summary: {final_state_via_invoke.get('final_result_summary')}\")\n",
    "assert final_streamed_state is not None\n",
    "assert final_streamed_state.get('final_result_summary') == final_state_via_invoke.get('final_result_summary')\n",
    "assert len(final_state_via_invoke.get('processed_data_log',[])) == 3 + 1 # 初期メッセージ + 3ノードのログメッセージ\n",
    "print(\"アサーション成功！ストリームの最終状態とinvokeの結果が一致し、ログも期待通り。\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題002: LLM からのストリーミング応答のリアルタイム処理\n",
    "\n",
    "### 課題\n",
    "LLMが長い応答を生成する場合、全てのテキストが揃うまで待つのではなく、生成されるトークンを逐次受け取り（ストリーミングし）、リアルタイムで表示または処理したいことがあります。この問題では、LangChainのLLMクライアントが提供するストリーミング機能とLangGraphを組み合わせ、LLMからの応答トークンを一つずつ（またはチャンクごとに）受け取り、それを状態に少しずつ追加していくノードを作成します。\n",
    "\n",
    "*   **学習内容:** LLMの`.stream()`メソッド（または非同期版の`.astream()`）を使ってトークンストリームを取得する方法、そしてそのストリームをLangGraphのノード内で処理し、状態を逐次更新する方法を学びます。これにより、ユーザー体験を向上させるリアルタイム応答が可能になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List, Optional, AsyncIterator\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import asyncio # 非同期処理用\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class LLMStreamingState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    user_prompt: str\n",
    "    llm_full_response: str # 最終的に結合されたLLM応答\n",
    "    # llm_stream_chunks: List[str] # (オプション) 各チャンクを保存する場合\n",
    "\n",
    "# --- ノード定義 (非同期ストリーミング対応) ---\n",
    "async def streaming_llm_node(state: LLMStreamingState, config: RunnableConfig):\n",
    "    print(f\"streaming_llm_node: プロンプト「{state['user_prompt']}」でLLMストリーミング開始...\")\n",
    "    current_full_response = \"\"\n",
    "    \n",
    "    # llm.stream() を使用してトークンをストリーミング取得\n",
    "    # config を渡すことで、LangSmithトレーシングなどが有効な場合にコールバックが機能する\n",
    "    async for chunk in llm.astream(state['user_prompt'], config=config):\n",
    "        # chunk は AIMessageChunk または str の場合がある (モデルによる)\n",
    "        # ここでは content 属性があると仮定 (AIMessageChunkなど)\n",
    "        token = chunk.content if hasattr(chunk, 'content') else str(chunk) \n",
    "        print(token, end=\"\", flush=True) # トークンをリアルタイム表示\n",
    "        current_full_response += token\n",
    "        # (オプション) 状態を部分的に更新したい場合は yield を使うが、\n",
    "        # この問題では最後にまとめて更新するシンプルな形とする。\n",
    "        # LangGraphのノードがgeneratorを返す場合、より細かい制御が可能 (今回は扱わない)\n",
    "    \n",
    "    print(\"\\nLLMストリーミング完了。\")\n",
    "    return {\"llm_full_response\": current_full_response, \"messages\": [AIMessage(content=current_full_response)]}\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(LLMStreamingState)\n",
    "workflow.add_node(\"llm_streamer\", streaming_llm_node) # 非同期関数をノードとして追加\n",
    "workflow.set_entry_point(\"llm_streamer\")\n",
    "workflow.add_edge(\"llm_streamer\", END)\n",
    "\n",
    "# 非同期実行に対応したコンパイルが必要な場合があるが、\n",
    "# StateGraph は非同期ノードをそのまま扱えることが多い。\n",
    "graph_llm_streaming = workflow.compile()\n",
    "\n",
    "# --- グラフ実行 (非同期) ---\n",
    "async def run_streaming_graph():\n",
    "    print(\"--- LLMからのストリーミング応答テスト ---\")\n",
    "    prompt = \"LangGraphの主な利点を3つ、簡潔に説明してください。\"\n",
    "    initial_input_llm_stream = {\n",
    "        \"messages\": [HumanMessage(content=prompt)],\n",
    "        \"user_prompt\": prompt,\n",
    "        \"llm_full_response\": \"\"\n",
    "    }\n",
    "    \n",
    "    # graph.astream() を使って非同期にストリーミングイベントを処理\n",
    "    # この例では、ノード自体がストリームを内部処理し最終結果を返すので、\n",
    "    # graph.stream() の各イベントはノード完了時の状態となる。\n",
    "    # ノードが yield する場合は、graph.stream() のイベントがより細かくなる。\n",
    "    async for event_chunk in graph_llm_streaming.astream(initial_input_llm_stream, {\"recursion_limit\": 5}):\n",
    "        print(f\"\\nGraph Event: {event_chunk}\")\n",
    "    \n",
    "    final_state = await graph_llm_streaming.ainvoke(initial_input_llm_stream, {\"recursion_limit\": 5})\n",
    "    print(\"\\n--- ストリーミング実行完了後の最終状態 ---\")\n",
    "    print(f\"  ユーザープロンプト: {final_state['user_prompt']}\")\n",
    "    print(f\"  LLMの全応答: {final_state['llm_full_response']}\")\n",
    "    print(f\"  最後のメッセージ: {final_state['messages'][-1].content if final_state['messages'] else 'N/A'}\")\n",
    "\n",
    "# asyncio.run(run_streaming_graph()) # Jupyter Notebook等では直接awaitできる場合もある\n",
    "# Google Colabなどトップレベルawaitが許可されている環境では以下で実行可能:\n",
    "# await run_streaming_graph()\n",
    "# そうでない場合は、イベントループを作成して実行:\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        asyncio.run(run_streaming_graph())\n",
    "    except RuntimeError as e:\n",
    "        if \" asyncio.run() cannot be called from a running event loop\" in str(e) and 'google.colab' in str(type(asyncio.get_event_loop())):\n",
    "            print(\"\\nColab環境で実行します。上記の streaming_llm_node 内の print(token, end='') を確認してください。\")\n",
    "            # Colabでは await run_streaming_graph() が直接動作することが多い\n",
    "            # ここでは、デモとしてエラーをキャッチし、ユーザーに手動実行を促すメッセージを残す\n",
    "            # もしくは、nest_asyncio を使うなどの対策があるが、この問題の範囲外とする\n",
    "            pass # Colabではこのセルを直接実行すれば動くはず\n",
    "        else:\n",
    "            raise\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List, Optional, AsyncIterator\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import asyncio # 非同期処理用\n",
    "from IPython.display import display, Markdown # Markdown表示用\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class LLMStreamingState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    user_prompt_for_llm: str                # LLMに渡すプロンプト\n",
    "    accumulated_llm_response: str           # 逐次結合されたLLM応答文字列\n",
    "    # last_stream_chunk: Optional[str]      # (オプション)最後に受け取ったチャンク\n",
    "\n",
    "# --- ノード定義 (非同期ストリーミング対応ノード) ---\n",
    "# LangGraphのノードが AsyncIterator[StateUpdateType] を返すことで、\n",
    "# グラフ自体がストリーミング出力に対応できる (yield を使う)\n",
    "async def llm_token_streaming_node(state: LLMStreamingState, config: RunnableConfig) -> AsyncIterator[LLMStreamingState]:\n",
    "    prompt_text = state[\"user_prompt_for_llm\"]\n",
    "    print(f\"\\nllm_token_streaming_node: プロンプト「{prompt_text}」でLLMトークンストリーミングを開始します...\")\n",
    "    \n",
    "    full_response_buffer = \"\"\n",
    "    print(\"LLMからのストリーミング応答: \", end=\"\")\n",
    "\n",
    "    # LLMの非同期ストリーム (`.astream()`) を使用\n",
    "    async for chunk in llm.astream(prompt_text, config=config):\n",
    "        # chunk は通常 AIMessageChunk オブジェクト\n",
    "        token_text = chunk.content if hasattr(chunk, 'content') else str(chunk)\n",
    "        print(token_text, end=\"\", flush=True) # トークンをコンソールにリアルタイム表示\n",
    "        full_response_buffer += token_text\n",
    "        \n",
    "        # 状態を部分的に更新してyieldすることで、グラフの呼び出し元にストリーミングで状態変化を伝える\n",
    "        yield {\"accumulated_llm_response\": full_response_buffer} # type: ignore\n",
    "    \n",
    "    print(\"\\nLLMトークンストリーミング完了。\")\n",
    "    # 最後に完全な応答をmessagesにも追加\n",
    "    yield {\"messages\": [AIMessage(content=full_response_buffer)]} # type: ignore\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(LLMStreamingState)\n",
    "workflow.add_node(\"llm_token_streamer_node\", llm_token_streaming_node) # 非同期ジェネレータ関数をノードとして追加\n",
    "workflow.set_entry_point(\"llm_token_streamer_node\")\n",
    "workflow.add_edge(\"llm_token_streamer_node\", END)\n",
    "\n",
    "graph_llm_token_streaming = workflow.compile()\n",
    "\n",
    "# --- グラフ実行 (非同期ストリーミング) ---\n",
    "async def run_llm_token_streaming_graph():\n",
    "    print(\"--- LLMからのトークンストリーミング応答のリアルタイム処理テスト ---\")\n",
    "    user_query = \"LangGraphの主な特徴を3点挙げ、それぞれ1-2文で説明してください。箇条書きでお願いします。\"\n",
    "    \n",
    "    initial_input_for_token_stream = {\n",
    "        \"messages\": [HumanMessage(content=user_query)],\n",
    "        \"user_prompt_for_llm\": user_query,\n",
    "        \"accumulated_llm_response\": \"\" # 初期化\n",
    "    }\n",
    "    \n",
    "    print(f\"ユーザーの質問: {user_query}\\n\")\n",
    "    accumulated_response_for_display = \"\"\n",
    "    final_event_state = None\n",
    "\n",
    "    # graph.astream() を使って非同期にストリーミングイベントを処理\n",
    "    # ノードがyieldするたびに、このループ内でイベント (部分的な状態更新) を受け取る\n",
    "    async for event_update in graph_llm_token_streaming.astream(initial_input_for_token_stream, {\"recursion_limit\": 5}):\n",
    "        # event_update は {ノード名: yieldされた状態更新} という形式\n",
    "        if \"llm_token_streamer_node\" in event_update:\n",
    "            partial_state = event_update[\"llm_token_streamer_node\"]\n",
    "            if \"accumulated_llm_response\" in partial_state:\n",
    "                # ここではコンソールへのリアルタイム表示はノード内で行っているため、\n",
    "                # accumulated_response_for_display = partial_state[\"accumulated_llm_response\"]\n",
    "                # display(Markdown(f\"**途中経過:**\\n{accumulated_response_for_display}\")) # Jupyterならこれで動的表示も\n",
    "                pass # ノード内でprintしているのでここでは何もしない\n",
    "            if \"messages\" in partial_state: # 最後のyield (AIMessage追加)\n",
    "                print(f\"\\n  グラフイベント: 最終メッセージがmessagesに追加されました -> {partial_state['messages'][-1].type}\")\n",
    "        final_event_state = event_update # 最後のイベントを保持\n",
    "    \n",
    "    print(\"\\n--- トークンストリーミング実行完了後の最終状態 (astreamの最後のイベントより) ---\")\n",
    "    if final_event_state and \"llm_token_streamer_node\" in final_event_state:\n",
    "        final_node_state = final_event_state[\"llm_token_streamer_node\"]\n",
    "        print(f\"  ユーザープロンプト: {final_node_state.get('user_prompt_for_llm')}\")\n",
    "        print(f\"  LLMの全応答 (accumulated):\\n{final_node_state.get('accumulated_llm_response')}\")\n",
    "        final_messages = final_node_state.get('messages', [])\n",
    "        print(f\"  Messagesの最後のAI応答: {final_messages[-1].content if final_messages and isinstance(final_messages[-1], AIMessage) else 'N/A'}\")\n",
    "        assert final_node_state.get('accumulated_llm_response') == (final_messages[-1].content if final_messages and isinstance(final_messages[-1], AIMessage) else None)\n",
    "    else:\n",
    "        print(\"  最終状態の取得に失敗しました。\")\n",
    "    print(\"アサーション成功（accumulatedと最終messageが一致）\")\n",
    "\n",
    "# Jupyter Notebook/Colabなど、トップレベルawaitが使える環境では await run_llm_token_streaming_graph() で実行\n",
    "# 通常のPythonスクリプトでは asyncio.run() を使用\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        asyncio.run(run_llm_token_streaming_graph())\n",
    "    except RuntimeError as e:\n",
    "        # Colab環境で稀に発生する asyncio.run の二重呼び出しエラーを回避する試み\n",
    "        if \" asyncio.run() cannot be called from a running event loop\" in str(e) and 'google.colab' in str(type(asyncio.get_event_loop())):\n",
    "            print(\"\\nColab環境での実行を試みます。llm_token_streaming_node内のprint出力を確認してください。\")\n",
    "            # この場合、通常は await run_llm_token_streaming_graph() がセルで直接実行できるはず\n",
    "            # ここではエラーを握りつぶして何もしないことで、Colabでのセル実行を妨げないようにする\n",
    "        else:\n",
    "            raise # それ以外のRuntimeErrorは再送出\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題003: グラフ状態の永続化と再開 (MemorySaver)\n",
    "\n",
    "### 課題\n",
    "長時間の処理や中断を挟む可能性があるワークフローでは、グラフの現在の状態を永続化し、後でその状態から処理を再開できる機能が重要です。LangGraphは`checkpointer`という仕組みを提供しており、`MemorySaver`はその最も基本的なインメモリのチェックポインターです。この問題では、`MemorySaver`を使ってグラフの状態を保存し、別の機会にその状態をロードして処理を途中から再開する方法を学びます。\n",
    "\n",
    "*   **学習内容:** `MemorySaver`のインスタンスを作成し、それを`graph.compile()`時に`checkpointer`として渡す方法、グラフ実行時に`configurable`パラメータでスレッドID（`thread_id`）を指定することで状態が保存・復元される仕組み、そして`graph.get_state(config)`で特定のスレッドの状態を取得する方法を理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver # MemorySaverをインポート\n",
    "import uuid\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class PersistentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    current_phase: str\n",
    "    accumulated_value: int\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def phase_one_node(state: PersistentState):\n",
    "    val = state.get(\"accumulated_value\", 0) + 10\n",
    "    msg = f\"フェーズ1完了。値: {val}\"\n",
    "    return {\"current_phase\": \"PhaseOneDone\", \"accumulated_value\": val, \"messages\": [AIMessage(content=msg)]}\n",
    "\n",
    "def phase_two_node(state: PersistentState):\n",
    "    val = state.get(\"accumulated_value\", 0) * 2\n",
    "    msg = f\"フェーズ2完了。最終値: {val}\"\n",
    "    return {\"current_phase\": \"PhaseTwoDone\", \"accumulated_value\": val, \"messages\": [AIMessage(content=msg)]}\n",
    "\n",
    "# --- グラフ構築とチェックポインター設定 ---\n",
    "memory_checkpointer = MemorySaver() # MemorySaverインスタンス\n",
    "workflow = StateGraph(PersistentState)\n",
    "workflow.add_node(\"phase1\", phase_one_node)\n",
    "workflow.add_node(\"phase2\", phase_two_node)\n",
    "workflow.set_entry_point(\"phase1\")\n",
    "workflow.add_edge(\"phase1\", \"phase2\")\n",
    "workflow.add_edge(\"phase2\", END)\n",
    "\n",
    "graph_with_checkpoint = workflow.compile(checkpointer=memory_checkpointer)\n",
    "\n",
    "# --- 実行と状態の確認・再開 ---\n",
    "thread_id_1 = str(uuid.uuid4()) # 各実行を区別するためのスレッドID\n",
    "config_thread_1 = {\"configurable\": {\"thread_id\": thread_id_1}}\n",
    "\n",
    "print(f\"--- 永続化テスト (スレッドID: {thread_id_1}) ---\")\n",
    "print(\"\\nステップ1: フェーズ1のみ実行 (中断を模倣するため、invokeではなくstreamで一部実行)\")\n",
    "for i, event in enumerate(graph_with_checkpoint.stream({\"messages\": [HumanMessage(content=\"処理開始\")], \"accumulated_value\":0}, config_thread_1)):\n",
    "    print(f\"  イベント {i+1}: {event}\")\n",
    "    if \"phase1\" in event: # phase1が完了したら一旦ループを抜ける (中断の模倣)\n",
    "        print(\"    フェーズ1完了時点でストリームを停止 (中断を模倣)\")\n",
    "        break \n",
    "\n",
    "print(\"\\nステップ2: 保存された状態の確認\")\n",
    "saved_state = graph_with_checkpoint.get_state(config_thread_1)\n",
    "print(f\"  保存された状態: {saved_state.values if saved_state else '状態なし'}\")\n",
    "assert saved_state is not None and saved_state.values[\"current_phase\"] == \"PhaseOneDone\"\n",
    "\n",
    "print(\"\\nステップ3: フェーズ2から処理を再開\")\n",
    "# 再開時は、入力はNone (または空の辞書) でconfigのみ渡す (MemorySaverが状態をロードするため)\n",
    "final_state_resumed = graph_with_checkpoint.invoke(None, config_thread_1)\n",
    "print(f\"  再開後の最終状態: {final_state_resumed}\")\n",
    "assert final_state_resumed[\"current_phase\"] == \"PhaseTwoDone\"\n",
    "assert final_state_resumed[\"accumulated_value"] == (0 + 10) * 2\n",
    "print(\"\\n永続化と再開テスト成功！\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "import uuid # スレッドID生成用\n",
    "import time\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver # インメモリチェックポインター\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class PersistentWorkflowState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    current_operation_phase: str       # 例: \"data_ingestion\", \"processing\", \"validation\"\n",
    "    processed_item_count: int          # 処理したアイテム数\n",
    "    last_processed_item_id: Optional[str] # 最後に処理したアイテムID\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def data_ingestion_phase_node(state: PersistentWorkflowState):\n",
    "    phase_name = \"DataIngestionPhase\"\n",
    "    print(f\"\\n{phase_name}: データ取り込み処理を実行中...\")\n",
    "    time.sleep(0.5) # ダミー処理時間\n",
    "    new_item_id = f\"item_{state.get('processed_item_count', 0) + 1}\"\n",
    "    log_message = f\"{phase_name} 完了。アイテム '{new_item_id}' を取り込みました。\"\n",
    "    return {\n",
    "        \"current_operation_phase\": phase_name,\n",
    "        \"processed_item_count\": state.get(\"processed_item_count\", 0) + 1,\n",
    "        \"last_processed_item_id\": new_item_id,\n",
    "        \"messages\": [AIMessage(content=log_message, name=phase_name)]\n",
    "    }\n",
    "\n",
    "def data_processing_phase_node(state: PersistentWorkflowState):\n",
    "    phase_name = \"DataProcessingPhase\"\n",
    "    item_id = state.get(\"last_processed_item_id\", \"(不明なアイテム)\")\n",
    "    print(f\"\\n{phase_name}: アイテム '{item_id}' のデータ処理を実行中...\")\n",
    "    time.sleep(0.6)\n",
    "    log_message = f\"{phase_name} 完了。アイテム '{item_id}' の処理が成功しました。\"\n",
    "    return {\n",
    "        \"current_operation_phase\": phase_name,\n",
    "        \"messages\": [AIMessage(content=log_message, name=phase_name)]\n",
    "    }\n",
    "\n",
    "def final_validation_phase_node(state: PersistentWorkflowState):\n",
    "    phase_name = \"FinalValidationPhase\"\n",
    "    item_id = state.get(\"last_processed_item_id\", \"(不明なアイテム)\")\n",
    "    total_items = state.get(\"processed_item_count\", 0)\n",
    "    print(f\"\\n{phase_name}: アイテム '{item_id}' (総数: {total_items}) の最終検証を実行中...\")\n",
    "    time.sleep(0.4)\n",
    "    log_message = f\"{phase_name} 完了。アイテム '{item_id}' は検証済みです。全 {total_items} アイテムの処理が完了。\"\n",
    "    return {\n",
    "        \"current_operation_phase\": phase_name,\n",
    "        \"messages\": [AIMessage(content=log_message, name=phase_name)]\n",
    "    }\n",
    "\n",
    "# --- グラフ構築とチェックポインター設定 ---\n",
    "in_memory_checkpointer = MemorySaver() # MemorySaverのインスタンスを作成\n",
    "\n",
    "workflow = StateGraph(PersistentWorkflowState)\n",
    "workflow.add_node(\"ingestion_step\", data_ingestion_phase_node)\n",
    "workflow.add_node(\"processing_step\", data_processing_phase_node)\n",
    "workflow.add_node(\"validation_step\", final_validation_phase_node)\n",
    "\n",
    "workflow.set_entry_point(\"ingestion_step\")\n",
    "workflow.add_edge(\"ingestion_step\", \"processing_step\")\n",
    "workflow.add_edge(\"processing_step\", \"validation_step\")\n",
    "workflow.add_edge(\"validation_step\", END)\n",
    "\n",
    "# compile時にcheckpointerを渡す\n",
    "graph_with_memory_checkpoint = workflow.compile(checkpointer=in_memory_checkpointer)\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "try:\n",
    "    display(Image(graph_with_memory_checkpoint.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- 実行、状態の確認、そして再開のテスト ---\n",
    "unique_thread_id = str(uuid.uuid4()) # 各実行シーケンスを区別するためのID\n",
    "thread_config = {\"configurable\": {\"thread_id\": unique_thread_id}}\n",
    "\n",
    "print(f\"\\n--- グラフ状態の永続化と再開テスト (スレッドID: {unique_thread_id}) ---\")\n",
    "\n",
    "print(\"\\nステップ1: 最初の入力でグラフを実行開始 (ingestion_stepのみ実行されるように途中で止める想定)\")\n",
    "initial_graph_input = {\n",
    "    \"messages\": [HumanMessage(content=\"最初のアイテム処理を開始します。\")],\n",
    "    \"processed_item_count\": 0 # 初期化\n",
    "}\n",
    "\n",
    "# streamを使って、ingestion_stepが完了した時点で止める（中断のシミュレーション）\n",
    "for i, event_chunk in enumerate(graph_with_memory_checkpoint.stream(initial_graph_input, thread_config)):\n",
    "    print(f\"  イベント {i+1} (ストリーム): {event_chunk}\")\n",
    "    # ingestion_stepの完了を検知したら中断\n",
    "    if \"ingestion_step\" in event_chunk:\n",
    "        print(\"    -> ingestion_step 完了。ここで処理を中断したと仮定します。\")\n",
    "        break \n",
    "\n",
    "print(\"\\nステップ2: 保存された状態の確認 (ingestion_step完了後)\")\n",
    "state_after_ingestion = graph_with_memory_checkpoint.get_state(thread_config)\n",
    "if state_after_ingestion:\n",
    "    print(f\"  保存された状態 (ingestion後): {state_after_ingestion.values}\")\n",
    "    assert state_after_ingestion.values[\"current_operation_phase\"] == \"DataIngestionPhase\"\n",
    "    assert state_after_ingestion.values[\"processed_item_count\"] == 1\n",
    "else:\n",
    "    print(\"  エラー: ingestion後の状態が保存されていませんでした。\")\n",
    "\n",
    "print(\"\\nステップ3: グラフを同じスレッドIDで再開 (processing_stepから始まるはず)\")\n",
    "# 再開時は、入力はNone (または空の辞書) で、configでスレッドIDを指定する\n",
    "# MemorySaverがthread_idに基づいて保存された状態を自動的にロードする\n",
    "final_state_after_resume = graph_with_memory_checkpoint.invoke(None, thread_config)\n",
    "\n",
    "print(\"\\nステップ4: 再開後の最終状態の確認 (validation_step完了後)\")\n",
    "if final_state_after_resume:\n",
    "    print(f\"  再開後の最終状態: {final_state_after_resume}\")\n",
    "    assert final_state_after_resume[\"current_operation_phase\"] == \"FinalValidationPhase\"\n",
    "    assert final_state_after_resume[\"processed_item_count\"] == 1 # ingestionで1になり、その後は変わらない\n",
    "    assert \"検証済みです\" in final_state_after_resume[\"messages\"][-1].content\n",
    "else:\n",
    "    print(\"  エラー: 再開後の最終状態が取得できませんでした。\")\n",
    "\n",
    "print(\"\\n--- MemorySaverによる永続化と再開テスト完了 --- \")\n",
    "assert state_after_ingestion is not None and final_state_after_resume is not None\n",
    "print(\"アサーション成功！\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題004: データベースによる永続化 (SqliteSaver)\n",
    "\n",
    "### 課題\n",
    "インメモリの`MemorySaver`は手軽ですが、アプリケーションが終了すると状態は失われます。実際の運用では、データベースを使った永続化が求められます。LangGraphはSQLite, PostgreSQLなどに対応したチェックポインターを提供しています。この問題では、`SqliteSaver`（Python標準の`sqlite3`を使用）を使って、グラフの状態をSQLiteデータベースファイルに永続化し、再開するフローを実装します。\n",
    "\n",
    "*   **学習内容:** `SqliteSaver.from_conn_string(\":memory:\")`（インメモリSQLite）またはファイルパスを指定して`SqliteSaver`をセットアップする方法、それを使ってグラフの状態をDBに保存・復元する基本的な流れを理解します。DB永続化の利点（プロセスをまたいだ状態保持）に触れます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "import uuid\n",
    "import os\n",
    "import sqlite3 # SqliteSaverのために必要\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver # SqliteSaverをインポート\n",
    "\n",
    "# --- 状態定義 (問題003と同じでOK) ---\n",
    "class DBCheckpointState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    current_stage: str\n",
    "    completion_flags: List[bool]\n",
    "\n",
    "# --- ノード定義 ---\n",
    "def stage_A_node(state: DBCheckpointState):\n",
    "    flags = state.get(\"completion_flags\", [False, False])\n",
    "    flags[0] = True\n",
    "    return {\"current_stage\": \"StageA_Done\", \"completion_flags\": flags, \"messages\":[AIMessage(content=\"ステージA完了\")]}\n",
    "\n",
    "def stage_B_node(state: DBCheckpointState):\n",
    "    flags = state.get(\"completion_flags\", [False, False])\n",
    "    flags[1] = True\n",
    "    return {\"current_stage\": \"StageB_Done\", \"completion_flags\": flags, \"messages\":[AIMessage(content=\"ステージB完了\")]}\n",
    "\n",
    "# --- グラフ構築とSQLiteチェックポインター設定 ---\n",
    "db_file = \"langgraph_checkpoint.sqlite\"\n",
    "if os.path.exists(db_file):\n",
    "    os.remove(db_file) # 前回のテストファイルがあれば削除\n",
    "sqlite_checkpointer = SqliteSaver.from_conn_string(db_file)\n",
    "\n",
    "workflow_db = StateGraph(DBCheckpointState)\n",
    "workflow_db.add_node(\"stageA\", stage_A_node)\n",
    "workflow_db.add_node(\"stageB\", stage_B_node)\n",
    "workflow_db.set_entry_point(\"stageA\")\n",
    "workflow_db.add_edge(\"stageA\", \"stageB\")\n",
    "workflow_db.add_edge(\"stageB\", END)\n",
    "\n",
    "graph_sqlite_checkpoint = workflow_db.compile(checkpointer=sqlite_checkpointer)\n",
    "\n",
    "# --- 実行、状態確認、再開 (DB使用) ---\n",
    "thread_id_db = str(uuid.uuid4())\n",
    "config_db = {\"configurable\": {\"thread_id\": thread_id_db}}\n",
    "\n",
    "print(f\"--- SQLite永続化テスト (スレッドID: {thread_id_db}, DBファイル: {db_file}) ---\")\n",
    "print(\"\\nステップ1: ステージAのみ実行 (中断模倣)\")\n",
    "for i, event in enumerate(graph_sqlite_checkpoint.stream({\"messages\":[HumanMessage(content=\"DBテスト開始\")]}, config_db)):\n",
    "    if \"stageA\" in event: break\n",
    "\n",
    "state_after_A_db = graph_sqlite_checkpoint.get_state(config_db)\n",
    "print(f\"  ステージA完了後のDB保存状態: {state_after_A_db.values if state_after_A_db else '状態なし'}\")\n",
    "assert state_after_A_db is not None and state_after_A_db.values[\"current_stage\"] == \"StageA_Done\"\n",
    "\n",
    "print(\"\\nステップ2: 別のグラフインスタンスで同じDBとスレッドIDを使い再開\")\n",
    "# 新しいグラフインスタンスだが、同じcheckpointerを参照\n",
    "graph_reloaded = workflow_db.compile(checkpointer=sqlite_checkpointer) \n",
    "final_state_reloaded_db = graph_reloaded.invoke(None, config_db)\n",
    "print(f\"  再開後の最終状態 (別インスタンス): {final_state_reloaded_db}\")\n",
    "assert final_state_reloaded_db[\"current_stage\"] == \"StageB_Done\"\n",
    "assert final_state_reloaded_db[\"completion_flags\"] == [True, True]\n",
    "print(\"\\nSQLite永続化と再開テスト成功！\")\n",
    "\n",
    "# クリーンアップ (任意)\n",
    "# if os.path.exists(db_file): os.remove(db_file)\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "import uuid\n",
    "import os\n",
    "import sqlite3 # SqliteSaver が内部で使用\n",
    "import time\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver # SQLiteチェックポインター\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class DBCheckpointWorkflowState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    current_processing_stage: str      # 例: \"収集\", \"分析\", \"報告\"\n",
    "    document_id: Optional[str]         # 処理対象のドキュメントID\n",
    "    analysis_summary: Optional[str]    # 分析ステージの結果\n",
    "    stages_completed: List[str]        # 完了したステージ名のリスト\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def collection_stage_node(state: DBCheckpointWorkflowState):\n",
    "    stage_name = \"CollectionStage\"\n",
    "    doc_id = state.get(\"document_id\", f\"doc_{str(uuid.uuid4())[:8]}\")\n",
    "    print(f\"\\n{stage_name}: ドキュメント '{doc_id}' のデータ収集を開始します...\")\n",
    "    time.sleep(0.5)\n",
    "    log_message = f\"{stage_name} 完了。ドキュメント '{doc_id}' の基礎データを収集しました。\"\n",
    "    current_completed = state.get(\"stages_completed\", [])\n",
    "    return {\n",
    "        \"current_processing_stage\": stage_name,\n",
    "        \"document_id\": doc_id, # 新規または既存のID\n",
    "        \"stages_completed\": current_completed + [stage_name],\n",
    "        \"messages\": [AIMessage(content=log_message, name=stage_name)]\n",
    "    }\n",
    "\n",
    "def analysis_stage_node(state: DBCheckpointWorkflowState):\n",
    "    stage_name = \"AnalysisStage\"\n",
    "    doc_id = state.get(\"document_id\", \"(不明なドキュメント)\")\n",
    "    print(f\"\\n{stage_name}: ドキュメント '{doc_id}' のデータ分析を実行します...\")\n",
    "    time.sleep(0.7)\n",
    "    summary_text = f\"ドキュメント '{doc_id}' の分析結果: 主要な洞察は「XYZ」であり、関連データポイントはA,B,Cです。\"\n",
    "    log_message = f\"{stage_name} 完了。ドキュメント '{doc_id}' の分析サマリーを生成しました。\"\n",
    "    current_completed = state.get(\"stages_completed\", [])\n",
    "    return {\n",
    "        \"current_processing_stage\": stage_name,\n",
    "        \"analysis_summary\": summary_text,\n",
    "        \"stages_completed\": current_completed + [stage_name],\n",
    "        \"messages\": [AIMessage(content=log_message, name=stage_name)]\n",
    "    }\n",
    "\n",
    "def reporting_stage_node(state: DBCheckpointWorkflowState):\n",
    "    stage_name = \"ReportingStage\"\n",
    "    doc_id = state.get(\"document_id\", \"(不明なドキュメント)\")\n",
    "    analysis = state.get(\"analysis_summary\", \"(分析結果なし)\")\n",
    "    print(f\"\\n{stage_name}: ドキュメント '{doc_id}' の最終報告書を作成します。分析: '{analysis[:50]}...'\" )\n",
    "    time.sleep(0.4)\n",
    "    report_content = f\"最終報告 ({doc_id}):\\n分析サマリー: {analysis}\\n結論: 全ての処理は正常に完了しました。\"\n",
    "    log_message = f\"{stage_name} 完了。ドキュメント '{doc_id}' の最終報告書が作成されました。\"\n",
    "    current_completed = state.get(\"stages_completed\", [])\n",
    "    return {\n",
    "        \"current_processing_stage\": stage_name,\n",
    "        # final_product: report_content, # 最終成果物を別のキーに持っても良い\n",
    "        \"stages_completed\": current_completed + [stage_name],\n",
    "        \"messages\": [AIMessage(content=report_content, name=stage_name)] # 報告書をメッセージとして追加\n",
    "    }\n",
    "\n",
    "# --- グラフ構築とSQLiteチェックポインター設定 ---\n",
    "db_file_path = \"langgraph_chapter5_checkpoint.sqlite\"\n",
    "# 前回の実行ファイルが残っていれば削除 (テストの独立性のため)\n",
    "if os.path.exists(db_file_path):\n",
    "    os.remove(db_file_path)\n",
    "    print(f\"既存のDBファイル '{db_file_path}' を削除しました。\")\n",
    "\n",
    "# SqliteSaver.from_conn_string でデータベース接続文字列を指定\n",
    "# ここではカレントディレクトリにSQLiteファイルを作成\n",
    "sqlite_db_checkpointer = SqliteSaver.from_conn_string(db_file_path)\n",
    "print(f\"SqliteSaver を '{db_file_path}' で初期化しました。\")\n",
    "\n",
    "workflow_sqlite = StateGraph(DBCheckpointWorkflowState)\n",
    "workflow_sqlite.add_node(\"collection\", collection_stage_node)\n",
    "workflow_sqlite.add_node(\"analysis\", analysis_stage_node)\n",
    "workflow_sqlite.add_node(\"reporting\", reporting_stage_node)\n",
    "\n",
    "workflow_sqlite.set_entry_point(\"collection\")\n",
    "workflow_sqlite.add_edge(\"collection\", \"analysis\")\n",
    "workflow_sqlite.add_edge(\"analysis\", \"reporting\")\n",
    "workflow_sqlite.add_edge(\"reporting\", END)\n",
    "\n",
    "graph_with_sqlite_checkpoint = workflow_sqlite.compile(checkpointer=sqlite_db_checkpointer)\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "try:\n",
    "    display(Image(graph_with_sqlite_checkpoint.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- 実行、状態確認、再開のテスト (SQLite使用) ---\n",
    "unique_thread_id_db = str(uuid.uuid4()) # スレッドID\n",
    "db_thread_config = {\"configurable\": {\"thread_id\": unique_thread_id_db}}\n",
    "\n",
    "print(f\"\\n--- SQLiteデータベースによる永続化テスト (スレッドID: {unique_thread_id_db}) ---\")\n",
    "\n",
    "print(\"\\nステップ1: 最初の入力でグラフを実行開始 (collectionステージのみ実行)\")\n",
    "initial_db_input = {\n",
    "    \"messages\": [HumanMessage(content=\"ドキュメントXの処理を開始\")],\n",
    "    \"document_id\": \"doc_X_123\", # 明示的にIDを指定\n",
    "    \"stages_completed\": []\n",
    "}\n",
    "for i, event_chunk in enumerate(graph_with_sqlite_checkpoint.stream(initial_db_input, db_thread_config)):\n",
    "    print(f\"  イベント {i+1} (ストリーム): {event_chunk}\")\n",
    "    if \"collection\" in event_chunk: # collectionステージ完了で中断\n",
    "        print(\"    -> collectionステージ完了。処理を中断したと仮定。\")\n",
    "        break\n",
    "\n",
    "print(\"\\nステップ2: DBに保存された状態の確認 (collectionステージ完了後)\")\n",
    "state_after_collection_db = graph_with_sqlite_checkpoint.get_state(db_thread_config)\n",
    "if state_after_collection_db:\n",
    "    print(f\"  DB保存状態 (collection後): {state_after_collection_db.values}\")\n",
    "    assert state_after_collection_db.values[\"current_processing_stage\"] == \"CollectionStage\"\n",
    "    assert \"CollectionStage\" in state_after_collection_db.values[\"stages_completed\"]\n",
    "else:\n",
    "    print(\"  エラー: collection後の状態がDBから取得できませんでした。\")\n",
    "\n",
    "print(\"\\nステップ3: ★別のグラフインスタンス★で同じDBとスレッドIDを使って処理を再開\")\n",
    "# 新しいグラフインスタンスを作成するが、同じSQLiteデータベースを参照するcheckpointerを渡す\n",
    "graph_reloaded_from_db = workflow_sqlite.compile(checkpointer=sqlite_db_checkpointer) \n",
    "final_state_reloaded_from_db = graph_reloaded_from_db.invoke(None, db_thread_config)\n",
    "\n",
    "print(\"\\nステップ4: 再開後の最終状態の確認 (reportingステージ完了後)\")\n",
    "if final_state_reloaded_from_db:\n",
    "    print(f\"  再開後の最終状態 (DBより): {final_state_reloaded_from_db}\")\n",
    "    assert final_state_reloaded_from_db[\"current_processing_stage\"] == \"ReportingStage\"\n",
    "    assert \"ReportingStage\" in final_state_reloaded_from_db[\"stages_completed\"]\n",
    "    assert len(final_state_reloaded_from_db[\"stages_completed\"]) == 3\n",
    "else:\n",
    "    print(\"  エラー: 再開後の最終状態が取得できませんでした。\")\n",
    "\n",
    "print(\"\\n--- SqliteSaverによる永続化と再開テスト完了 --- \")\n",
    "assert state_after_collection_db is not None and final_state_reloaded_from_db is not None\n",
    "print(\"アサーション成功！\")\n",
    "\n",
    "# (オプション) テスト後に生成されたSQLiteファイルを削除\n",
    "# if os.path.exists(db_file_path):\n",
    "#     os.remove(db_file_path)\n",
    "#     print(f\"テスト用DBファイル '{db_file_path}' をクリーンアップしました。\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題005: デバッグと可視化 - LangSmith との連携\n",
    "\n",
    "### 課題\n",
    "複雑なLangGraphのワークフローを開発・運用する際には、各ステップの実行状況、LLMの入出力、ツールの呼び出しなどを詳細に追跡・可視化できるデバッグツールが不可欠です。LangSmithはLangChain/LangGraphのための強力なデバッグ・監視プラットフォームです。この問題では、LangSmithと連携するように環境変数を設定し、グラフを実行した際にLangSmith上でトレースデータがどのように記録されるかを確認します。（LangSmithのアカウントとAPIキーが設定済みである前提）\n",
    "\n",
    "*   **学習内容:** 環境変数（`LANGCHAIN_TRACING_V2`, `LANGCHAIN_API_KEY`, `LANGCHAIN_PROJECT`, `LANGCHAIN_ENDPOINT`）を設定することで、LangGraphの実行トレースが自動的にLangSmithに送信されることを理解します。LangSmithのUIで、グラフの構造、各ノードの実行時間、LLMのプロンプトと応答、ツールの入出力などを確認できることを（ここでは手順として）学びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# --- LangSmith連携の確認 ---\n",
    "# 準備セルでLANGCHAIN_TRACING_V2=\"true\"などが設定されていると仮定\n",
    "if os.getenv(\"LANGCHAIN_TRACING_V2\") == \"true\" and os.getenv(\"LANGCHAIN_API_KEY\"):\n",
    "    print(\"LangSmith連携が有効です。実行トレースはLangSmithに送信されます。\")\n",
    "    print(f\"  プロジェクト名: {os.getenv('LANGCHAIN_PROJECT', '(未設定)')}\")\n",
    "    print(f\"  エンドポイント: {os.getenv('LANGCHAIN_ENDPOINT', 'https://api.smith.langchain.com')}\")\n",
    "else:\n",
    "    print(\"LangSmith連携が無効または未設定です。この問題の意図通りに動作させるには、準備セルの指示に従って環境変数を設定してください。続行はしますが、トレースは記録されません。\")\n",
    "\n",
    "# --- 簡単なツールとエージェントの定義 (問題003のものを再利用・簡略化) ---\n",
    "@tool\n",
    "def get_city_population(city_name: str) -> str:\n",
    "    \"\"\"指定された都市の現在の推定人口を返します。\"\"\"\n",
    "    print(f\"Tool 'get_city_population' called for: {city_name}\")\n",
    "    if city_name.lower() == \"tokyo\": return \"約1400万人\"\n",
    "    if city_name.lower() == \"london\": return \"約900万人\"\n",
    "    return f\"{city_name}の人口データはありません。\"\n",
    "\n",
    "langsmith_test_tools = [get_city_population]\n",
    "langsmith_tool_node = ToolNode(langsmith_test_tools)\n",
    "\n",
    "class LangSmithAgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "\n",
    "def langsmith_agent_node(state: LangSmithAgentState, config):\n",
    "    # config引数を追加して、LangSmithのコールバックが正しく機能するようにする\n",
    "    # (LangGraphが内部的にconfigを渡してくれる)\n",
    "    print(\"\\nLangSmith Agent Node: 実行中...\")\n",
    "    llm_with_tools_for_ls = llm.bind_tools(langsmith_test_tools)\n",
    "    response = llm_with_tools_for_ls.invoke(state[\"messages\"], config=config)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def langsmith_router(state: LangSmithAgentState):\n",
    "    if state[\"messages\"][-1].tool_calls: return \"call_ls_tool\"\n",
    "    return END\n",
    "\n",
    "workflow_ls = StateGraph(LangSmithAgentState)\n",
    "workflow_ls.add_node(\"agent_ls\", langsmith_agent_node)\n",
    "workflow_ls.add_node(\"tool_node_ls\", langsmith_tool_node)\n",
    "workflow_ls.set_entry_point(\"agent_ls\")\n",
    "workflow_ls.add_conditional_edges(\"agent_ls\", langsmith_router, {\"call_ls_tool\": \"tool_node_ls\", END: END})\n",
    "workflow_ls.add_edge(\"tool_node_ls\", \"agent_ls\")\n",
    "graph_for_langsmith = workflow_ls.compile()\n",
    "\n",
    "# --- グラフ実行 ---\n",
    "print(\"\\n--- LangSmith連携テスト実行 (東京の人口を質問) ---\")\n",
    "inputs_ls = {\"messages\": [SystemMessage(content=\"あなたは都市情報アシスタントです。\"), HumanMessage(content=\"東京の人口を教えてください。\")]}\n",
    "final_state_ls = graph_for_langsmith.invoke(inputs_ls, {\"recursion_limit\": 5})\n",
    "\n",
    "print(\"\\n最終的なAIの応答:\")\n",
    "for msg in final_state_ls[\"messages\"]:\n",
    "    if isinstance(msg, AIMessage) and not msg.tool_calls: print(f\"  -> {msg.content}\")\n",
    "\n",
    "print(\"\\n--- LangSmithでの確認手順 ---\")\n",
    "print(\"1. LangSmithにログイン (https://smith.langchain.com/)\")\n",
    "print(f\"2. 設定したプロジェクト「{os.getenv('LANGCHAIN_PROJECT', '(未設定)')}」を選択します。\")\n",
    "print(\"3. 最近の実行トレースリストから、この実行に対応するものを探します（通常は最新）。\")\n",
    "print(\"4. トレース詳細を開き、グラフの構造、各ノードの入出力、LLMのプロンプトと応答、ツールの呼び出しと結果などが確認できることを確かめてください。\")\n",
    "print(\"   特に、'agent_ls'ノードでのLLM呼び出しや、'tool_node_ls'でのツール実行が記録されているはずです。\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display, Markdown\n",
    "import os # 環境変数の読み込みに使う\n",
    "\n",
    "# --- LangSmith連携の確認と設定 ---\n",
    "# 準備セルでLANGCHAIN_TRACING_V2=\"true\"などが設定されていることを前提とします。\n",
    "langsmith_is_active = False\n",
    "if os.getenv(\"LANGCHAIN_TRACING_V2\", \"false\").lower() == \"true\" and os.getenv(\"LANGCHAIN_API_KEY\"):\n",
    "    langsmith_is_active = True\n",
    "    print(\"LangSmith連携が有効です。グラフの実行はLangSmithにトレースされます。\")\n",
    "    langsmith_project_name = os.getenv(\"LANGCHAIN_PROJECT\", \"Default LangGraph Project\")\n",
    "    print(f\"  LangSmithプロジェクト名: {langsmith_project_name}\")\n",
    "    print(f\"  LangSmithエンドポイント: {os.getenv('LANGCHAIN_ENDPOINT', 'https://api.smith.langchain.com')}\")\n",
    "    # LangSmith SDKが環境変数を自動的に読み込むため、ここでの追加設定は通常不要。\n",
    "else:\n",
    "    print(\"LangSmith連携が無効または設定不十分です。環境変数を確認してください。\")\n",
    "    print(\"参考: LANGCHAIN_TRACING_V2='true', LANGCHAIN_API_KEY='your_api_key', LANGCHAIN_PROJECT='your_project_name'\")\n",
    "    print(\"この問題の実行自体は可能ですが、LangSmithへのトレースは行われません。\")\n",
    "\n",
    "# --- 簡単なツールとエージェントの定義 (第3章の問題001,002に近いもの) ---\n",
    "@tool\n",
    "def get_city_current_weather(city: str) -> str:\n",
    "    \"\"\"指定された都市の現在の天気を返します。\"\"\"\n",
    "    print(f\"Tool 'get_city_current_weather' called for city: {city}\")\n",
    "    if city.lower() == \"tokyo\":\n",
    "        return \"東京の天気は晴れ、気温25度です。\"\n",
    "    elif city.lower() == \"london\":\n",
    "        return \"ロンドンの天気は曇り、気温18度です。\"\n",
    "    else:\n",
    "        return f\"{city}の天気情報は現在取得できません。\"\n",
    "\n",
    "tools_for_langsmith_demo = [get_city_current_weather]\n",
    "tool_node_for_langsmith = ToolNode(tools_for_langsmith_demo)\n",
    "\n",
    "class AgentStateForLangsmith(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "\n",
    "def agent_node_for_langsmith(state: AgentStateForLangsmith, config):\n",
    "    # config引数はLangGraphから自動的に渡され、LangSmithコールバック等に使われる\n",
    "    print(\"\\nAgent Node (for LangSmith demo): LLMに判断を依頼します...\")\n",
    "    current_msgs = state[\"messages\"]\n",
    "    \n",
    "    # LangSmithでトレースされるように、llmの呼び出しにconfigを渡すことが重要\n",
    "    # (bind_toolsやToolNodeも内部でこれを考慮している)\n",
    "    llm_with_tools_ls = llm.bind_tools(tools_for_langsmith_demo)\n",
    "    ai_response = llm_with_tools_ls.invoke(current_msgs, config=config) \n",
    "    print(f\"  -> LLMからの応答: {ai_response}\")\n",
    "    return {\"messages\": [ai_response]}\n",
    "\n",
    "def router_for_langsmith_demo(state: AgentStateForLangsmith):\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:\n",
    "        return \"call_tool_for_langsmith\"\n",
    "    return END\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow_langsmith = StateGraph(AgentStateForLangsmith)\n",
    "workflow_langsmith.add_node(\"agent_langsmith\", agent_node_for_langsmith)\n",
    "workflow_langsmith.add_node(\"tool_executor_langsmith\", tool_node_for_langsmith)\n",
    "\n",
    "workflow_langsmith.set_entry_point(\"agent_langsmith\")\n",
    "workflow_langsmith.add_conditional_edges(\n",
    "    \"agent_langsmith\", \n",
    "    router_for_langsmith_demo,\n",
    "    {\n",
    "        \"call_tool_for_langsmith\": \"tool_executor_langsmith\", \n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "workflow_langsmith.add_edge(\"tool_executor_langsmith\", \"agent_langsmith\")\n",
    "\n",
    "graph_to_trace_on_langsmith = workflow_langsmith.compile()\n",
    "\n",
    "# --- グラフの可視化 (ローカル確認用) ---\n",
    "try:\n",
    "    display(Image(graph_to_trace_on_langsmith.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"ローカルでのグラフ可視化に失敗: {e}.\")\n",
    "\n",
    "# --- グラフ実行 ---\n",
    "print(\"\\n--- LangSmith連携デモ用グラフの実行開始 --- \")\n",
    "query_for_ls = \"東京の現在の天気を教えてください。\"\n",
    "inputs_for_ls_demo = {\"messages\": [\n",
    "    SystemMessage(content=\"あなたは天気情報アシスタントです。ユーザーの質問に答えるために、必要なら天気確認ツールを使ってください。\"), \n",
    "    HumanMessage(content=query_for_ls)\n",
    "]}\n",
    "\n",
    "# invokeのconfigにrun_nameなどを指定すると、LangSmith上でトレースを見つけやすくなる\n",
    "run_config = {\"recursion_limit\": 5}\n",
    "if langsmith_is_active:\n",
    "    run_config[\"run_name\"] = \"LangGraph100Knocks_Ch5_Q5_Run\"\n",
    "    run_config[\"metadata\"] = {\"question\": query_for_ls, \"chapter\": \"Chapter5_Advanced\"}\n",
    "\n",
    "final_state_langsmith_demo = graph_to_trace_on_langsmith.invoke(inputs_for_ls_demo, config=run_config)\n",
    "\n",
    "print(\"\\n--- グラフ実行完了 --- \")\n",
    "print(\"最終的なAIの応答:\")\n",
    "final_ai_message_content = \"(最終応答なし)\"\n",
    "for msg in reversed(final_state_langsmith_demo.get(\"messages\", [])):\n",
    "    if isinstance(msg, AIMessage) and not msg.tool_calls:\n",
    "        final_ai_message_content = msg.content\n",
    "        break\n",
    "print(f\"  -> {final_ai_message_content}\")\n",
    "assert \"東京の天気は晴れ\" in final_ai_message_content\n",
    "\n",
    "print(\"\\n--- LangSmithでのトレース確認手順 --- \")\n",
    "if langsmith_is_active:\n",
    "    print(\"1. LangSmith (https://smith.langchain.com/) にアクセスし、ログインしてください。\")\n",
    "    print(f\"2. プロジェクト「{os.getenv('LANGCHAIN_PROJECT', '(未設定)')}」を選択します。\")\n",
    "    print(\"3. 左側のメニューから「Traces」または「Runs」を選択します。\")\n",
    "    print(\"4. 今実行したトレースがリストの上位に表示されているはずです。\")\n",
    "    print(\"   (もし見つからなければ、実行名「LangGraph100Knocks_Ch5_Q5_Run」でフィルタリングしてみてください)\")\n",
    "    print(\"5. トレース詳細画面で、グラフの各ノード（agent_langsmith, tool_executor_langsmith）の実行、LLMの入出力、ツールの呼び出しと結果などが階層的に表示されていることを確認してください。\")\n",
    "    display(Markdown(f\"**[LangSmithのプロジェクトへのリンク（目安）](https://smith.langchain.com/o/{os.getenv('LANGCHAIN_TENANT_ID', 'YOUR_TENANT_ID')}/projects/p/{os.getenv('LANGCHAIN_PROJECT_ID', 'YOUR_PROJECT_ID')})** (テナントIDとプロジェクトIDは実際の値に置き換えてください)\"))\n",
    "else:\n",
    "    print(\"LangSmith連携が無効なため、トレースは送信されていません。確認するには、準備セルの指示に従って環境変数を設定し、再度実行してください。\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題006: 状態のカスタム更新ロジック (Annotated と Reducer)\n",
    "\n",
    "### 課題\n",
    "LangGraphの`StateGraph`では、状態のキーが更新される際、デフォルトでは新しい値が古い値を単純に上書きします。しかし、`messages`キーに対する`add_messages`のように、キーごとに特殊な更新ロジック（例: リストへの追加、数値の加算、特定条件に基づく上書きなど）を定義したい場合があります。この問題では、`typing.Annotated`とカスタムの`Reducer`関数（またはラムダ関数）を使って、状態の特定キーに対する更新方法をカスタマイズする方法を学びます。\n",
    "\n",
    "*   **学習内容:** `typing.Annotated`を使って状態キーにメタデータを付与する方法、そしてそのメタデータとしてカスタムのReducer関数を指定し、状態更新時の挙動を制御する方法を理解します。数値カウンターのインクリメントやリストへの要素追加など、単純な上書き以外の更新処理を実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List, Optional, Sequence\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage # add_messagesもReducerの一種\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- カスタムReducer関数の定義 ---\n",
    "def sum_values_reducer(previous_value: Optional[int], new_value: int) -> int:\n",
    "    \"\"\"古い値と新しい値を加算するReducer。初回は新しい値をそのまま使う。\"\"\"\n",
    "    if previous_value is None: # 初回またはキーが存在しない場合\n",
    "        return new_value\n",
    "    return previous_value + new_value\n",
    "\n",
    "def append_to_list_reducer(previous_list: Optional[List[str]], new_item: str) -> List[str]:\n",
    "    \"\"\"リストに新しいアイテムを追加するReducer。初回は新しいアイテムのみのリスト。\"\"\"\n",
    "    if previous_list is None:\n",
    "        return [new_item]\n",
    "    return previous_list + [new_item] # 新しいリストを返すことが重要\n",
    "\n",
    "# --- 状態定義 (AnnotatedとReducerを使用) ---\n",
    "class CustomUpdateState(TypedDict):\n",
    "    # messages は add_messages という組み込みReducerを使用\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    \n",
    "    # total_score は sum_values_reducer を使って加算更新\n",
    "    total_score: Annotated[Optional[int], sum_values_reducer]\n",
    "    \n",
    "    # event_log は append_to_list_reducer を使ってリストに追加更新\n",
    "    event_log: Annotated[Optional[List[str]], append_to_list_reducer]\n",
    "    \n",
    "    # last_event はデフォルトの上書き動作 (Reducer指定なし)\n",
    "    last_event_description: Optional[str]\n",
    "\n",
    "# --- ノード定義 ---\n",
    "def event_processor_node(state: CustomUpdateState):\n",
    "    # ユーザー入力に基づいてスコアとイベントログを更新するデモノード\n",
    "    last_user_message = \"\"\n",
    "    for msg in reversed(state.get(\"messages\", [])):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_user_message = msg.content\n",
    "            break\n",
    "    \n",
    "    score_to_add = 0\n",
    "    event_desc = \"\"\n",
    "    if \"good\" in last_user_message.lower():\n",
    "        score_to_add = 10\n",
    "        event_desc = \"Positive event occurred\"\n",
    "    elif \"bad\" in last_user_message.lower():\n",
    "        score_to_add = -5\n",
    "        event_desc = \"Negative event occurred\"\n",
    "    else:\n",
    "        score_to_add = 1\n",
    "        event_desc = \"Neutral event registered\"\n",
    "        \n",
    "    print(f\"event_processor_node: Adding score {score_to_add}, logging '{event_desc}'\")\n",
    "    return {\n",
    "        \"total_score\": score_to_add, # Reducerが加算する\n",
    "        \"event_log\": event_desc,     # Reducerがリストに追加する\n",
    "        \"last_event_description\": event_desc, # これは上書きされる\n",
    "        \"messages\": [AIMessage(content=f\"Score {score_to_add} added. Event: {event_desc}\")]\n",
    "    }\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow = StateGraph(CustomUpdateState)\n",
    "workflow.add_node(\"event_handler\", event_processor_node)\n",
    "workflow.set_entry_point(\"event_handler\")\n",
    "workflow.add_edge(\"event_handler\", END)\n",
    "\n",
    "graph_custom_reducers = workflow.compile()\n",
    "\n",
    "# --- 実行テスト ---\n",
    "print(\"--- カスタムReducerによる状態更新テスト ---\")\n",
    "thread_id_reducers = str(uuid.uuid4())\n",
    "config_reducers = {\"configurable\": {\"thread_id\": thread_id_reducers}}\n",
    "\n",
    "# 1回目の実行\n",
    "inputs1 = {\"messages\": [HumanMessage(content=\"This is a good start!\")]}\n",
    "state1 = graph_custom_reducers.invoke(inputs1, config_reducers)\n",
    "print(f\"\\n状態1: {state1}\")\n",
    "assert state1[\"total_score\"] == 10\n",
    "assert state1[\"event_log\"] == [\"Positive event occurred\"]\n",
    "assert state1[\"last_event_description\"] == \"Positive event occurred\"\n",
    "\n",
    "# 2回目の実行 (同じスレッドIDで状態を引き継ぐ)\n",
    "inputs2 = {\"messages\": [HumanMessage(content=\"Something bad happened.\")]} \n",
    "# invokeの入力にmessagesだけ渡すと、前回の状態のmessagesに追記される\n",
    "# しかし、他のキー（total_scoreなど）は前回のものが保持される（Reducerが働くため）\n",
    "state2 = graph_custom_reducers.invoke(inputs2, config_reducers)\n",
    "print(f\"\\n状態2: {state2}\")\n",
    "assert state2[\"total_score\"] == 10 - 5 # 10 (前回) + (-5) (今回)\n",
    "assert state2[\"event_log\"] == [\"Positive event occurred\", \"Negative event occurred\"]\n",
    "assert state2[\"last_event_description\"] == \"Negative event occurred\"\n",
    "\n",
    "print(\"\\nカスタムReducerテスト成功！\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List, Optional, Sequence, Union\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage # add_messagesもReducerの一種\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import uuid\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- カスタムReducer関数の定義 ---\n",
    "def sum_values_reducer(previous_value: Optional[Union[int, float]], new_value: Union[int, float]) -> Union[int, float]:\n",
    "    \"\"\"古い値と新しい値を加算するReducer。初回(previous_valueがNone)は新しい値をそのまま使う。\"\"\"\n",
    "    print(f\"sum_values_reducer: prev={previous_value}, new={new_value}\")\n",
    "    if previous_value is None:\n",
    "        return new_value\n",
    "    return previous_value + new_value\n",
    "\n",
    "def append_str_to_list_reducer(previous_list: Optional[List[str]], new_item_or_list: Union[str, List[str]]) -> List[str]:\n",
    "    \"\"\"文字列または文字列リストを既存リストに追加するReducer。初回は新しいアイテム(リスト)で初期化。\"\"\"\n",
    "    print(f\"append_str_to_list_reducer: prev_list_len={len(previous_list) if previous_list else 0}, new_item='{new_item_or_list}'\")\n",
    "    current_list = previous_list if previous_list is not None else []\n",
    "    if isinstance(new_item_or_list, list):\n",
    "        return current_list + new_item_or_list\n",
    "    return current_list + [new_item_or_list]\n",
    "\n",
    "# --- 状態定義 (AnnotatedとReducerを使用) ---\n",
    "class CustomUpdateLogicState(TypedDict):\n",
    "    # messages は LangGraph組み込みの add_messages Reducer を使用\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    \n",
    "    # total_points はカスタムReducer sum_values_reducer を使って加算更新\n",
    "    total_points: Annotated[Optional[int], sum_values_reducer] # Optional[int]にしないと初回Noneが型エラー\n",
    "    \n",
    "    # action_history はカスタムReducer append_str_to_list_reducer を使ってリストに要素追加\n",
    "    action_history: Annotated[Optional[List[str]], append_str_to_list_reducer]\n",
    "    \n",
    "    # last_action_summary はReducer指定なし (デフォルトの上書き動作)\n",
    "    last_action_summary: Optional[str]\n",
    "\n",
    "# --- ノード定義 ---\n",
    "def process_user_action_node(state: CustomUpdateLogicState):\n",
    "    # ユーザーの最後のメッセージ内容に基づいて状態を更新するデモノード\n",
    "    last_user_msg_content = \"\"\n",
    "    if state.get(\"messages\"):\n",
    "        for msg in reversed(state[\"messages\"]):\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                last_user_msg_content = msg.content\n",
    "                break\n",
    "    \n",
    "    points_change = 0\n",
    "    action_description = \"(No specific action detected)\"\n",
    "    \n",
    "    if \"購入\" in last_user_msg_content:\n",
    "        points_change = 50\n",
    "        action_description = f\"商品購入処理 (+{points_change}pt)\"\n",
    "    elif \"ボーナス\" in last_user_msg_content:\n",
    "        points_change = 100\n",
    "        action_description = f\"ボーナスポイント獲得 (+{points_change}pt)\"\n",
    "    elif \"利用\" in last_user_msg_content:\n",
    "        points_change = -20\n",
    "        action_description = f\"ポイント利用 (-{abs(points_change)}pt)\"\n",
    "    else:\n",
    "        points_change = 1\n",
    "        action_description = f\"その他のアクティビティ (+{points_change}pt)\"\n",
    "        \n",
    "    print(f\"\\nprocess_user_action_node: ユーザー入力「{last_user_msg_content}」\")\n",
    "    print(f\"  -> ポイント変動: {points_change}, アクション: '{action_description}'\")\n",
    "    \n",
    "    return {\n",
    "        \"total_points\": points_change,          # Reducerが既存値に加算する\n",
    "        \"action_history\": action_description,   # Reducerがリストの末尾に追加する\n",
    "        \"last_action_summary\": action_description, # これは単純に上書きされる\n",
    "        \"messages\": [AIMessage(content=f\"アクション「{action_description}」を記録。ポイント変動: {points_change}\")]\n",
    "    }\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow_reducers = StateGraph(CustomUpdateLogicState)\n",
    "workflow_reducers.add_node(\"user_action_processor\", process_user_action_node)\n",
    "workflow_reducers.set_entry_point(\"user_action_processor\")\n",
    "workflow_reducers.add_edge(\"user_action_processor\", END)\n",
    "\n",
    "# MemorySaverを使って、invoke間で状態が引き継がれることを確認\n",
    "custom_reducer_checkpointer = MemorySaver()\n",
    "graph_with_custom_reducers = workflow_reducers.compile(checkpointer=custom_reducer_checkpointer)\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "try:\n",
    "    display(Image(graph_with_custom_reducers.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- 実行テスト ---\n",
    "print(\"\\n--- カスタムReducerによる状態更新の連続テスト --- \")\n",
    "test_thread_id = str(uuid.uuid4()) # このスレッドIDで状態が保存・復元される\n",
    "test_config = {\"configurable\": {\"thread_id\": test_thread_id}}\n",
    "\n",
    "print(\"\\n実行1: 「商品Aを購入」\")\n",
    "inputs_run1 = {\"messages\": [HumanMessage(content=\"商品Aを購入しました\")]}\n",
    "state_run1 = graph_with_custom_reducers.invoke(inputs_run1, test_config)\n",
    "print(f\"実行1後の状態: total_points={state_run1['total_points']}, action_history={state_run1['action_history']}, last_summary='{state_run1['last_action_summary']}'\")\n",
    "assert state_run1[\"total_points\"] == 50\n",
    "assert state_run1[\"action_history\"] == [\"商品購入処理 (+50pt)\"]\n",
    "assert state_run1[\"last_action_summary\"] == \"商品購入処理 (+50pt)\"\n",
    "\n",
    "print(\"\\n実行2: 「ボーナス獲得」 (同じスレッドIDで状態を引き継ぐ)\")\n",
    "inputs_run2 = {\"messages\": [HumanMessage(content=\"特別ボーナス獲得です！\")]}\n",
    "state_run2 = graph_with_custom_reducers.invoke(inputs_run2, test_config) # messages以外は入力不要 (checkpointerが復元)\n",
    "print(f\"実行2後の状態: total_points={state_run2['total_points']}, action_history={state_run2['action_history']}, last_summary='{state_run2['last_action_summary']}'\")\n",
    "assert state_run2[\"total_points\"] == 50 + 100 # 50 (前回) + 100 (今回)\n",
    "assert state_run2[\"action_history\"] == [\"商品購入処理 (+50pt)\", \"ボーナスポイント獲得 (+100pt)\"]\n",
    "assert state_run2[\"last_action_summary\"] == \"ボーナスポイント獲得 (+100pt)\"\n",
    "\n",
    "print(\"\\n実行3: 「ポイントを利用」\")\n",
    "inputs_run3 = {\"messages\": [HumanMessage(content=\"貯まったポイントを利用します\")]}\n",
    "state_run3 = graph_with_custom_reducers.invoke(inputs_run3, test_config)\n",
    "print(f\"実行3後の状態: total_points={state_run3['total_points']}, action_history={state_run3['action_history']}, last_summary='{state_run3['last_action_summary']}'\")\n",
    "assert state_run3[\"total_points\"] == 50 + 100 - 20 # 150 (前回) - 20 (今回)\n",
    "assert state_run3[\"action_history\"] == [\"商品購入処理 (+50pt)\", \"ボーナスポイント獲得 (+100pt)\", \"ポイント利用 (-20pt)\"]\n",
    "assert state_run3[\"last_action_summary\"] == \"ポイント利用 (-20pt)\"\n",
    "\n",
    "print(\"\\nカスタムReducerと状態永続化の連携テスト成功！\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題007: 非同期ノードによるI/O処理の効率化\n",
    "\n",
    "### 課題\n",
    "グラフのノードが外部API呼び出しやファイル読み書きのようなI/Oバウンドな処理を含む場合、同期的な実行では処理全体がブロックされてしまう可能性があります。Pythonの`asyncio`を使った非同期処理をLangGraphのノードに導入することで、I/O待ちの間に他の処理（LangGraphの内部処理や、可能であれば他の非同期タスク）を進められるようになり、全体のパフォーマンス向上が期待できます。この問題では、簡単な非同期I/O処理（`asyncio.sleep`で模倣）を含む非同期ノードを作成し、グラフがそれをどのように扱うかを確認します。\n",
    "\n",
    "*   **学習内容:** `async def`で非同期ノード関数を定義する方法、ノード内で`await`を使って非同期処理（例: `asyncio.sleep`や実際の非同期API呼び出し）を実行する方法、そして非同期ノードを含むグラフを`graph.ainvoke()`や`graph.astream()`で非同期に実行する方法を学びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "import asyncio # 非同期I/Oのため\n",
    "import time\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class AsyncIOState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    task_description: str\n",
    "    io_result: Optional[str]\n",
    "    processing_log: List[str]\n",
    "\n",
    "# --- 非同期ノード定義 (async def) ---\n",
    "async def async_io_bound_node(state: AsyncIOState):\n",
    "    task_desc = state[\"task_description\"]\n",
    "    log = state.get(\"processing_log\", [])\n",
    "    print(f\"async_io_bound_node: 「{task_desc}」の非同期I/O処理を開始します...\")\n",
    "    \n",
    "    # ダミーの非同期I/O処理 (例: 外部API呼び出しを模倣)\n",
    "    await asyncio.sleep(1) # 1秒待機\n",
    "    io_operation_result = f\"「{task_desc}」のI/O処理が完了しました。結果データABC。\"\n",
    "    print(f\"  -> I/O処理完了: {io_operation_result}\")\n",
    "    \n",
    "    return {\n",
    "        \"io_result\": io_operation_result,\n",
    "        \"processing_log\": log + [f\"Async I/O for '{task_desc}' completed.\"],\n",
    "        \"messages\": [AIMessage(content=io_operation_result, name=\"AsyncIONode\")]\n",
    "    }\n",
    "\n",
    "def synchronous_cpu_bound_node(state: AsyncIOState):\n",
    "    # I/Oノードの後に実行される同期的なCPUバウンド処理のダミー\n",
    "    io_res = state.get(\"io_result\", \"(I/O結果なし)\")\n",
    "    log = state.get(\"processing_log\", [])\n",
    "    print(f\"synchronous_cpu_bound_node: I/O結果「{io_res[:30]}...」を元にCPU処理を開始します...\")\n",
    "    \n",
    "    # ダミーのCPU処理 (例: 時間のかかる計算)\n",
    "    # time.sleep(0.5) # 同期的なのでここでブロックする\n",
    "    # 簡単な文字列操作で代替\n",
    "    processed_summary = f\"CPU処理済み: {io_res.upper()} - FINALIZED\"\n",
    "    print(f\"  -> CPU処理完了: {processed_summary}\")\n",
    "    \n",
    "    return {\n",
    "        \"processing_log\": log + [f\"Sync CPU processing of '{io_res[:20]}...' completed.\"],\n",
    "        \"messages\": [AIMessage(content=processed_summary, name=\"SyncCPUNode\")]\n",
    "    }\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow_async = StateGraph(AsyncIOState)\n",
    "workflow_async.add_node(\"async_io_task\", async_io_bound_node) # async def関数をそのまま追加\n",
    "workflow_async.add_node(\"sync_cpu_task\", synchronous_cpu_bound_node)\n",
    "\n",
    "workflow_async.set_entry_point(\"async_io_task\")\n",
    "workflow_async.add_edge(\"async_io_task\", \"sync_cpu_task\")\n",
    "workflow_async.add_edge(\"sync_cpu_task\", END)\n",
    "\n",
    "graph_with_async_node = workflow_async.compile()\n",
    "\n",
    "# --- グラフの非同期実行 ---\n",
    "async def run_async_graph_demo():\n",
    "    print(\"--- 非同期ノードによるI/O処理効率化テスト ---\")\n",
    "    task_description_for_async = \"外部データベースからの大規模データ取得シミュレーション\"\n",
    "    \n",
    "    initial_input_async = {\n",
    "        \"messages\": [HumanMessage(content=f\"タスク開始: {task_description_for_async}\")],\n",
    "        \"task_description\": task_description_for_async,\n",
    "        \"processing_log\": []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"グラフ実行開始時刻: {start_time:.2f}\")\n",
    "\n",
    "    # graph.ainvoke() を使って非同期にグラフを実行\n",
    "    final_state_async = await graph_with_async_node.ainvoke(initial_input_async, {\"recursion_limit\": 5})\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nグラフ実行終了時刻: {end_time:.2f}, 所要時間: {end_time - start_time:.2f}秒\")\n",
    "\n",
    "    print(\"\\n--- 非同期実行後の最終状態 ---\")\n",
    "    print(f\"  タスク記述: {final_state_async['task_description']}\")\n",
    "    print(f\"  I/O処理結果: {final_state_async.get('io_result')}\")\n",
    "    print(f\"  処理ログ: {final_state_async['processing_log']}\")\n",
    "    print(f\"  最後のメッセージ: {final_state_async['messages'][-1].content if final_state_async['messages'] else 'N/A'}\")\n",
    "    assert \"FINALIZED\" in final_state_async['messages'][-1].content\n",
    "    assert len(final_state_async['processing_log']) == 2\n",
    "    print(\"非同期ノードテストのアサーション成功！\")\n",
    "\n",
    "# Jupyter Notebook/Colabなど、トップレベルawaitが使える環境では await run_async_graph_demo() で実行\n",
    "# 通常のPythonスクリプトでは asyncio.run() を使用\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        asyncio.run(run_async_graph_demo())\n",
    "    except RuntimeError as e:\n",
    "        if \" asyncio.run() cannot be called from a running event loop\" in str(e) and 'google.colab' in str(type(asyncio.get_event_loop())):\n",
    "            print(\"\\nColab環境で実行します。上記の実行ログを確認してください。\")\n",
    "        else: raise\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "import asyncio # Pythonの非同期I/Oライブラリ\n",
    "import time    # 処理時間を計測するため\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableConfig # configを渡すために必要\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class AsyncWorkflowState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    initial_task_description: str\n",
    "    async_io_operation_result: Optional[str] # 非同期I/O処理の結果\n",
    "    final_processed_output: Optional[str]    # CPUバウンド処理後の最終出力\n",
    "    operation_log: List[str]                 # 各処理のログ\n",
    "\n",
    "# --- 非同期ノード定義 (async def を使用) ---\n",
    "async def perform_async_io_task_node(state: AsyncWorkflowState, config: RunnableConfig):\n",
    "    task_desc = state[\"initial_task_description\"]\n",
    "    current_log = state.get(\"operation_log\", [])\n",
    "    node_name = \"AsyncIOTaskNode\"\n",
    "    \n",
    "    print(f\"\\n{node_name}: 「{task_desc}」に関連する非同期I/O処理を開始します... (例: APIからデータフェッチ)\")\n",
    "    log_message_start = f\"{node_name} - 開始: {task_desc}\"\n",
    "    \n",
    "    # ダミーの非同期I/O処理 (例: 外部API呼び出しを asyncio.sleep で模倣)\n",
    "    await asyncio.sleep(1.5) # 1.5秒待機してI/Oバウンド処理をシミュレート\n",
    "    \n",
    "    simulated_io_result = f\"「{task_desc}」の非同期I/O処理が正常に完了しました。取得データペイロード: {{'key': 'value', 'timestamp': '{time.time()}'}}\"\n",
    "    print(f\"  -> {node_name} - I/O処理完了: {simulated_io_result}\")\n",
    "    log_message_end = f\"{node_name} - 完了: {simulated_io_result[:50]}...\"\n",
    "    \n",
    "    return {\n",
    "        \"async_io_operation_result\": simulated_io_result,\n",
    "        \"operation_log\": current_log + [log_message_start, log_message_end],\n",
    "        \"messages\": [AIMessage(content=simulated_io_result, name=node_name)]\n",
    "    }\n",
    "\n",
    "# --- 同期ノード定義 (比較のため) ---\n",
    "def perform_synchronous_cpu_task_node(state: AsyncWorkflowState):\n",
    "    # このノードは、前の非同期ノードの結果を受けて同期的に動作する想定\n",
    "    io_result_data = state.get(\"async_io_operation_result\", \"(前のI/O結果なし)\")\n",
    "    current_log = state.get(\"operation_log\", [])\n",
    "    node_name = \"SynchronousCPUTaskNode\"\n",
    "    \n",
    "    print(f\"\\n{node_name}: I/O結果「{io_result_data[:60]}...」を元に、同期的なCPUバウンド処理を開始します...\")\n",
    "    log_message_start = f\"{node_name} - 開始: I/O結果の処理\"\n",
    "    \n",
    "    # ダミーのCPUバウンド処理 (例: 時間のかかる計算やデータ変換)\n",
    "    # time.sleep(1.0) # 同期的なので、ここで実行がブロックされる\n",
    "    # 簡単な文字列処理で代替\n",
    "    processed_output_data = f\"CPU処理済み最終出力: [ {io_result_data.upper()} ] - 検証OK\"\n",
    "    print(f\"  -> {node_name} - CPU処理完了: {processed_output_data}\")\n",
    "    log_message_end = f\"{node_name} - 完了: {processed_output_data[:50]}...\"\n",
    "    \n",
    "    return {\n",
    "        \"final_processed_output\": processed_output_data,\n",
    "        \"operation_log\": current_log + [log_message_start, log_message_end],\n",
    "        \"messages\": [AIMessage(content=processed_output_data, name=node_name)]\n",
    "    }\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow_with_async = StateGraph(AsyncWorkflowState)\n",
    "\n",
    "# 非同期関数 (async def) もそのままノードとして追加可能\n",
    "workflow_with_async.add_node(\"async_io_step\", perform_async_io_task_node) \n",
    "workflow_with_async.add_node(\"sync_cpu_step\", perform_synchronous_cpu_task_node)\n",
    "\n",
    "workflow_with_async.set_entry_point(\"async_io_step\")\n",
    "workflow_with_async.add_edge(\"async_io_step\", \"sync_cpu_step\")\n",
    "workflow_with_async.add_edge(\"sync_cpu_step\", END)\n",
    "\n",
    "graph_async_demo = workflow_with_async.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "try:\n",
    "    display(Image(graph_async_demo.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}. Graphvizがインストールされているか確認してください。\")\n",
    "\n",
    "# --- グラフの非同期実行 ---\n",
    "async def main_run_async_workflow_demo():\n",
    "    print(\"\\n--- 非同期ノードを含むグラフの実行デモ --- \")\n",
    "    task_description = \"ユーザープロファイルと行動履歴に基づく推薦リストの生成\"\n",
    "    \n",
    "    initial_input_for_async_graph = {\n",
    "        \"messages\": [HumanMessage(content=f\"非同期処理タスク開始: {task_description}\")],\n",
    "        \"initial_task_description\": task_description,\n",
    "        \"operation_log\": []\n",
    "    }\n",
    "    \n",
    "    overall_start_time = time.perf_counter()\n",
    "    print(f\"グラフ全体の非同期実行を開始します... (開始時刻: {overall_start_time:.3f}s)\")\n",
    "\n",
    "    # graph.ainvoke() を使ってグラフ全体を非同期に実行\n",
    "    # (ストリーミングする場合は graph.astream() を使用)\n",
    "    final_async_state = await graph_async_demo.ainvoke(initial_input_for_async_graph, {\"recursion_limit\": 5})\n",
    "    \n",
    "    overall_end_time = time.perf_counter()\n",
    "    print(f\"\\nグラフ全体の非同期実行が完了しました。 (終了時刻: {overall_end_time:.3f}s, 所要時間: {overall_end_time - overall_start_time:.3f}s)\")\n",
    "\n",
    "    print(\"\\n--- 非同期実行後の最終状態 --- \")\n",
    "    print(f\"  初期タスク記述: {final_async_state['initial_task_description']}\")\n",
    "    print(f\"  非同期I/O処理結果(一部): {final_async_state.get('async_io_operation_result', '')[:80]}...\")\n",
    "    print(f\"  最終処理出力(一部): {final_async_state.get('final_processed_output', '')[:80]}...\")\n",
    "    print(f\"  オペレーションログ:\")\n",
    "    for log_item in final_async_state.get('operation_log', []):\n",
    "        print(f\"    - {log_item}\")\n",
    "    final_messages = final_async_state.get('messages', [])\n",
    "    print(f\"  Messagesの最後のAI応答: {final_messages[-1].content[:80] if final_messages and isinstance(final_messages[-1], AIMessage) else 'N/A'}...\")\n",
    "    \n",
    "    assert \"FINALIZED\" in final_async_state.get('final_processed_output', '')\n",
    "    assert len(final_async_state.get('operation_log', [])) == 4 # 各ノードで開始・完了の2ログずつ\n",
    "    print(\"\\n非同期ノード実行テストのアサーション成功！\")\n",
    "\n",
    "# Jupyter Notebook/Colabなどトップレベルawaitが利用可能な環境では、\n",
    "# await main_run_async_workflow_demo() のように直接呼び出せます。\n",
    "# 通常のPythonスクリプトの場合は、以下のように asyncio.run() を使います。\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        asyncio.run(main_run_async_workflow_demo())\n",
    "    except RuntimeError as e:\n",
    "        # Colab環境で `asyncio.run()` が既に実行中のイベントループ内で呼ばれるとエラーになることがある\n",
    "        if \" asyncio.run() cannot be called from a running event loop\" in str(e) and 'google.colab' in str(type(asyncio.get_event_loop())):\n",
    "            print(\"\\nColab環境を検出しました。上記の実行ログで非同期処理の動作を確認してください。\")\n",
    "            # Colab環境では、このセルが直接実行されることで `await main_run_async_workflow_demo()` が\n",
    "            # 適切に動作することが期待されるため、ここではエラーをパスします。\n",
    "        else:\n",
    "            raise # その他のRuntimeErrorはそのまま送出\n",
    "``````\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
