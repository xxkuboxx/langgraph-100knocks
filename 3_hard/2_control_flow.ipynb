{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2章: グラフの制御フロー\n",
    "\n",
    "## 準備\n",
    "\n",
    "以下のセルを順番に実行して、演習に必要な環境をセットアップします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMプロバイダーの選択\n",
    "\n",
    "このセルでは、使用するLLMプロバイダーを選択します。\n",
    "`LLM_PROVIDER` 変数に、利用したいプロバイダー名を設定してください。\n",
    "選択可能なプロバイダー: `\"openai\"`, `\"azure\"`, `\"google\"` (Vertex AI), `\"google_genai\"` (Gemini API), `\"anthropic\"`, `\"bedrock\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMプロバイダーの選択 ===\n",
    "# 利用したいLLMプロバイダーを以下の変数で指定してください。\n",
    "# \"openai\", \"azure\", \"google\" (Vertex AI), \"google_genai\" (Gemini API), \"anthropic\", \"bedrock\" のいずれかを選択できます。\n",
    "LLM_PROVIDER = \"openai\"  # 例: OpenAI を利用する場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIキー/環境変数の設定\n",
    "\n",
    "以下のセルを実行する前に、選択したLLMプロバイダーに応じたAPIキーまたは環境変数を設定する必要があります。\n",
    "\n",
    "**手順:**\n",
    "1.  `.env.sample` ファイルをコピーして `.env` ファイルを作成します。\n",
    "2.  `.env` ファイルを開き、選択したLLMプロバイダーに対応するAPIキーや必要な情報を記述します。\n",
    "    *   **OpenAI:** `OPENAI_API_KEY`\n",
    "    *   **Azure OpenAI:** `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `OPENAI_API_VERSION`, `AZURE_OPENAI_DEPLOYMENT_NAME`\n",
    "    *   **Google (Vertex AI):** `GOOGLE_CLOUD_PROJECT_ID`, `GOOGLE_CLOUD_LOCATION` (Colab環境外で実行する場合、`GOOGLE_APPLICATION_CREDENTIALS` 環境変数の設定も必要になることがあります)\n",
    "    *   **Google (Gemini API):** `GOOGLE_API_KEY`\n",
    "    *   **Anthropic:** `ANTHROPIC_API_KEY`\n",
    "    *   **AWS Bedrock:** `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION_NAME` (IAMロールを使用する場合は、これらのキー設定は不要な場合がありますが、リージョン名は必須です)\n",
    "3.  ファイルを保存します。\n",
    "\n",
    "**Google Colab を使用している場合:**\n",
    "上記の `.env` ファイルを使用する代わりに、Colabのシークレットマネージャーに必要なキーを登録してください。\n",
    "例えば、OpenAIを使用する場合は `OPENAI_API_KEY` という名前でシークレットを登録します。\n",
    "Vertex AI を利用する場合は、Colab上での認証 (`google.colab.auth.authenticate_user()`) が実行されます。\n",
    "\n",
    "このセルは、設定された情報に基づいて環境変数をロードし、LLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === APIキー/環境変数の設定 ===\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envファイルから環境変数を読み込む (存在する場合)\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# --- OpenAI ---\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY and IS_COLAB:\n",
    "        OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"OpenAI APIキーが設定されていません。環境変数 OPENAI_API_KEY を設定するか、Colab環境の場合はシークレットに OPENAI_API_KEY を設定してください。\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# --- Azure OpenAI ---\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not AZURE_OPENAI_API_KEY: AZURE_OPENAI_API_KEY = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not AZURE_OPENAI_ENDPOINT: AZURE_OPENAI_ENDPOINT = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        if not OPENAI_API_VERSION: OPENAI_API_VERSION = userdata.get(\"OPENAI_API_VERSION\") # 例: \"2023-07-01-preview\"\n",
    "        if not AZURE_OPENAI_DEPLOYMENT_NAME: AZURE_OPENAI_DEPLOYMENT_NAME = userdata.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if not AZURE_OPENAI_API_KEY: raise ValueError(\"Azure OpenAI APIキー (AZURE_OPENAI_API_KEY) が設定されていません。\")\n",
    "    if not AZURE_OPENAI_ENDPOINT: raise ValueError(\"Azure OpenAI エンドポイント (AZURE_OPENAI_ENDPOINT) が設定されていません。\")\n",
    "    if not OPENAI_API_VERSION: OPENAI_API_VERSION = \"2023-07-01-preview\" # デフォルトを設定することも可能\n",
    "    if not AZURE_OPENAI_DEPLOYMENT_NAME: raise ValueError(\"Azure OpenAI デプロイメント名 (AZURE_OPENAI_DEPLOYMENT_NAME) が設定されていません。\")\n",
    "\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "    os.environ[\"OPENAI_API_VERSION\"] = OPENAI_API_VERSION\n",
    "\n",
    "# --- Google Cloud Vertex AI (Gemini) ---\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT_ID\") # .env 用に修正\n",
    "    LOCATION = os.environ.get(\"GOOGLE_CLOUD_LOCATION\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not PROJECT_ID: PROJECT_ID = userdata.get(\"GOOGLE_CLOUD_PROJECT_ID\")\n",
    "        if not LOCATION: LOCATION = userdata.get(\"GOOGLE_CLOUD_LOCATION\") # 例: \"us-central1\"\n",
    "        from google.colab import auth as google_auth\n",
    "        google_auth.authenticate_user() # Vertex AI を使う場合は Colab での認証を推奨\n",
    "    else: # Colab外の場合、.envから読み込んだ値で環境変数を設定\n",
    "        if PROJECT_ID: os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID # Vertex AI SDKが参照する標準的な環境変数名\n",
    "        if LOCATION: os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "\n",
    "    if not PROJECT_ID: raise ValueError(\"Google Cloud Project ID が設定されていません。環境変数 GOOGLE_CLOUD_PROJECT_ID を設定するか、Colab環境の場合はシークレットに GOOGLE_CLOUD_PROJECT_ID を設定してください。\")\n",
    "    if not LOCATION: LOCATION = \"us-central1\" # デフォルトロケーション\n",
    "\n",
    "# --- Google Gemini API (langchain-google-genai) ---\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY and IS_COLAB:\n",
    "        GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"Google APIキーが設定されていません。環境変数 GOOGLE_API_KEY を設定するか、Colab環境の場合はシークレットに GOOGLE_API_KEY を設定してください。\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# --- Anthropic (Claude) ---\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY and IS_COLAB:\n",
    "        ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        raise ValueError(\"Anthropic APIキーが設定されていません。環境変数 ANTHROPIC_API_KEY を設定するか、Colab環境の場合はシークレットに ANTHROPIC_API_KEY を設定してください。\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "# --- Amazon Bedrock (Claude) ---\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    AWS_REGION_NAME = os.environ.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if IS_COLAB: \n",
    "        if not AWS_ACCESS_KEY_ID: AWS_ACCESS_KEY_ID = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
    "        if not AWS_SECRET_ACCESS_KEY: AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        if not AWS_REGION_NAME: AWS_REGION_NAME = userdata.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if not AWS_REGION_NAME:\n",
    "         raise ValueError(\"AWSリージョン名 (AWS_REGION_NAME) が設定されていません。Bedrock利用にはリージョン指定が必要です。\")\n",
    "\n",
    "    # 環境変数に設定 (boto3がこれらを自動で読み込む)\n",
    "    if AWS_ACCESS_KEY_ID: os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "    if AWS_SECRET_ACCESS_KEY: os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION_NAME # boto3が参照する標準的なリージョン環境変数名\n",
    "    os.environ[\"AWS_REGION\"] = AWS_REGION_NAME # いくつかのライブラリはこちらを参照することもある\n",
    "\n",
    "print(f\"APIキー/環境変数の設定完了 (プロバイダー: {LLM_PROVIDER})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMクライアントの初期化\n",
    "\n",
    "このセルは、上で選択・設定したLLMプロバイダーに基づいて、対応するLLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMクライアントの動的初期化 ===\n",
    "llm = None\n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), # 環境変数から取得\n",
    "        openai_api_version=os.environ.get(\"OPENAI_API_VERSION\"), # 環境変数から取得\n",
    "        temperature=0,\n",
    "    )\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    from langchain_google_vertexai import ChatVertexAI\n",
    "    # PROJECT_ID, LOCATION は前のセルで環境変数に設定済みか、Colabの場合は直接利用\n",
    "    llm = ChatVertexAI(model_name=\"gemini-2.0-flash\", temperature=0, project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"), location=os.environ.get(\"GOOGLE_CLOUD_LOCATION\"))\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    from langchain_anthropic import ChatAnthropic\n",
    "    llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    from langchain_aws import ChatBedrock # langchain_community.chat_models から langchain_aws に変更の可能性あり\n",
    "    # AWS_REGION_NAME は前のセルで環境変数 AWS_DEFAULT_REGION に設定済み\n",
    "    llm = ChatBedrock( # BedrockChat ではなく ChatBedrock が一般的\n",
    "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        # region_name=os.environ.get(\"AWS_DEFAULT_REGION\"), # 通常、boto3が環境変数から自動で読み込む\n",
    "        model_kwargs={\"temperature\": 0},\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Unsupported LLM_PROVIDER: {LLM_PROVIDER}. \"\n",
    "        \"Please choose from 'openai', 'azure', 'google', 'google_genai', 'anthropic', or 'bedrock'.\"\n",
    "    )\n",
    "\n",
    "print(f\"LLM Provider: {LLM_PROVIDER}\")\n",
    "if llm:\n",
    "    print(f\"LLM Client Type: {type(llm)}\")\n",
    "    # モデル名取得の試行を汎用的に\n",
    "    model_attr = (\n",
    "                 getattr(llm, 'model', None) or\n",
    "                 getattr(llm, 'model_name', None) or\n",
    "                 getattr(llm, 'model_id', None) or\n",
    "                 (hasattr(llm, 'llm') and getattr(llm.llm, 'model', None)) # 一部のLLMクライアントのネスト構造に対応\n",
    "    )\n",
    "    if hasattr(llm, 'azure_deployment') and not model_attr: # Azure特有の属性\n",
    "        model_attr = llm.azure_deployment\n",
    "        \n",
    "    if model_attr:\n",
    "        print(f\"LLM Model: {model_attr}\")\n",
    "    else:\n",
    "        print(\"LLM Model: (Could not determine model name from client attributes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題001: 条件付きエッジによる分岐の導入\n",
    "\n",
    "LangGraphの強力な機能の一つである条件付きエッジを導入し、グラフの実行パスを動的に制御する方法を学びましょう。ここでは、入力された数値が偶数か奇数かによって、異なる処理を行うグラフを作成します。\n",
    "\n",
    "*   **学習内容:** `add_conditional_edges`を使用して、グラフの実行パスを動的に制御する方法を学びます。ルーター関数がどのように次のノードを決定するのか、そして状態が分岐間でどのように共有されるかを理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001 - グラフ構築\n",
    "____ typing ____ ____, ____\n",
    "____ langgraph.graph ____ ____, ____\n",
    "____ langchain_core.messages ____ ____ # AIMessageは解答例で使うのでここでは不要かも\n",
    "\n",
    "# 状態定義\n",
    "____ ConditionalState(____):\n",
    "    number: int\n",
    "    message: str\n",
    "\n",
    "# ノード定義\n",
    "____ check_number_node(state: ConditionalState):\n",
    "    print(f\"入力された数値: {state['number']}\")\n",
    "    ____ {} # 何も更新しない\n",
    "\n",
    "____ even_node(state: ConditionalState):\n",
    "    msg = f\"{state['number']} は偶数です。\"\n",
    "    print(msg)\n",
    "    ____ {\"message\": msg}\n",
    "\n",
    "____ odd_node(state: ConditionalState):\n",
    "    msg = f\"{state['number']} は奇数です。\"\n",
    "    print(msg)\n",
    "    ____ {\"message\": msg}\n",
    "\n",
    "# ルーター関数\n",
    "____ route_by_parity(state: ConditionalState):\n",
    "    if state[\"number\"] % 2 == 0:\n",
    "        return \"to_even\" # 偶数ならこの名前を返す (解答例より)\n",
    "    else:\n",
    "        return \"to_odd\" # 奇数ならこの名前を返す (解答例より)\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(ConditionalState)\n",
    "workflow.add_node(\"check\", check_number_node)\n",
    "workflow.add_node(\"even\", even_node)\n",
    "workflow.add_node(\"odd\", odd_node)\n",
    "workflow.set_entry_point(\"check\")\n",
    "\n",
    "# 条件付きエッジの追加\n",
    "workflow.add_conditional_edges(\n",
    "    \"check\",\n",
    "    route_by_parity,\n",
    "    {\n",
    "        \"to_even\": \"even\",\n",
    "        \"to_odd\": \"odd\" # 解答例より\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"even\", END)\n",
    "workflow.add_edge(\"odd\", END)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001 - グラフ可視化\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001 - グラフ実行\n",
    "print(\"--- 偶数のテスト ---\")\n",
    "print(f\"最終結果: {graph.invoke({'number': 42, 'message': ''})}\") # messageを初期化\n",
    "print(\"--- 奇数のテスト ---\")\n",
    "print(f\"最終結果: {graph.invoke({'number': 77, 'message': ''})}\") # messageを初期化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答001</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# 状態定義\n",
    "class ConditionalState(TypedDict):\n",
    "    number: int\n",
    "    message: str\n",
    "\n",
    "# ノード定義\n",
    "def check_number_node(state: ConditionalState):\n",
    "    print(f\"入力された数値: {state['number']}\")\n",
    "    return {} # 何も更新しない\n",
    "\n",
    "def even_node(state: ConditionalState):\n",
    "    msg = f\"{state['number']} は偶数です。\"\n",
    "    print(msg)\n",
    "    return {\"message\": msg}\n",
    "\n",
    "def odd_node(state: ConditionalState):\n",
    "    msg = f\"{state['number']} は奇数です。\"\n",
    "    print(msg)\n",
    "    return {\"message\": msg}\n",
    "\n",
    "# ルーター関数\n",
    "def route_by_parity(state: ConditionalState):\n",
    "    if state[\"number\"] % 2 == 0:\n",
    "        return \"to_even\" # 偶数ならこの名前を返す\n",
    "    else:\n",
    "        return \"to_odd\" # 奇数ならこの名前を返す\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(ConditionalState)\n",
    "workflow.add_node(\"check\", check_number_node)\n",
    "workflow.add_node(\"even\", even_node)\n",
    "workflow.add_node(\"odd\", odd_node)\n",
    "workflow.set_entry_point(\"check\")\n",
    "\n",
    "# 条件付きエッジの追加\n",
    "workflow.add_conditional_edges(\n",
    "    \"check\",\n",
    "    route_by_parity,\n",
    "    {\n",
    "        \"to_even\": \"even\",\n",
    "        \"to_odd\": \"odd\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"even\", END)\n",
    "workflow.add_edge(\"odd\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "# 実行\n",
    "print(\"--- 偶数のテスト ---\")\n",
    "print(f\"最終結果: {graph.invoke({'number': 42, 'message': ''})}\")\n",
    "print(\"--- 奇数のテスト ---\")\n",
    "print(f\"最終結果: {graph.invoke({'number': 77, 'message': ''})}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説001</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **ルーター関数:** `route_by_parity` 関数は、現在のグラフの状態 `state` を受け取り、次に進むべきエッジの名前（文字列）を返します。この戻り値に基づいて、`add_conditional_edges` で定義したマッピングに従い、処理が分岐します。\n",
    "*   **`add_conditional_edges`:** 3つの主要な引数を取ります。\n",
    "    1.  **始点ノード (`\"check\"`)**: 分岐を開始するノード。\n",
    "    2.  **ルーター関数 (`route_by_parity`)**: どのパスに進むかを決定する関数。\n",
    "    3.  **パスのマッピング (辞書)**: ルーター関数の戻り値（キー）と、次に遷移するノード名（バリュー）の対応を定義します。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題002: 状態を使ったループ（シンプルなカウンター制御）\n",
    "*   **学習内容:** 状態と条件付きエッジを組み合わせて、カウンターが上限に達するまで処理を繰り返すループ構造を実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002 - グラフ構築\n",
    "____ typing ____ ____, ____\n",
    "____ langgraph.graph ____ ____, ____\n",
    "____ langchain_core.messages ____ ____ # AIMessageは解答例で使用\n",
    "\n",
    "# 状態定義\n",
    "____ CounterLoopState(____):\n",
    "    count: int\n",
    "    max_count: int\n",
    "    log: list[str]\n",
    "\n",
    "# ノード定義\n",
    "____ increment_node(state: CounterLoopState):\n",
    "    new_count = state[\"count\"] + 1\n",
    "    log_message = f\"カウンターが {new_count} になりました。\"\n",
    "    print(log_message)\n",
    "    ____ {\"count\": new_count, \"log\": state[\"log\"] + [log_message]}\n",
    "\n",
    "____ final_message_node(state: CounterLoopState):\n",
    "    final_msg = f\"ループ終了。最終カウント: {state['count']}\"\n",
    "    print(final_msg)\n",
    "    ____ {\"log\": state[\"log\"] + [final_msg]}\n",
    "\n",
    "# ルーター関数\n",
    "____ should_continue(state: CounterLoopState):\n",
    "    ____ state[\"count\"] < state[\"max_count\"]:\n",
    "        ____ \"continue_loop\" # ループ継続のキー (解答例より)\n",
    "    else:\n",
    "        return \"exit_loop\" # ループ終了のキー (解答例より)\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(CounterLoopState)\n",
    "workflow.add_node(\"increment\", increment_node)\n",
    "workflow.add_node(\"final_message\", final_message_node)\n",
    "\n",
    "workflow.set_entry_point(\"increment\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"increment\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue_loop\": \"increment\",\n",
    "        \"exit_loop\": \"final_message\" # 解答例より\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"final_message\", END)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002 - グラフ可視化\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002 - グラフ実行\n",
    "initial_state = {\"count\": 0, \"max_count\": 3, \"log\": []}\n",
    "print(f\"--- カウンター上限 {initial_state['max_count']} のループテスト --- \")\n",
    "final_result = graph.invoke(initial_state)\n",
    "print(f\"最終状態のログ: {final_result['log']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答002</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# 状態定義\n",
    "class CounterLoopState(TypedDict):\n",
    "    count: int\n",
    "    max_count: int\n",
    "    log: list[str]\n",
    "\n",
    "# ノード定義\n",
    "def increment_node(state: CounterLoopState):\n",
    "    new_count = state[\"count\"] + 1\n",
    "    log_message = f\"カウンターが {new_count} になりました。\"\n",
    "    print(log_message)\n",
    "    return {\"count\": new_count, \"log\": state[\"log\"] + [log_message]}\n",
    "\n",
    "def final_message_node(state: CounterLoopState):\n",
    "    final_msg = f\"ループ終了。最終カウント: {state['count']}\"\n",
    "    print(final_msg)\n",
    "    return {\"log\": state[\"log\"] + [final_msg]}\n",
    "\n",
    "# ルーター関数\n",
    "def should_continue(state: CounterLoopState):\n",
    "    if state[\"count\"] < state[\"max_count\"]:\n",
    "        return \"continue_loop\"\n",
    "    else:\n",
    "        return \"exit_loop\"\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(CounterLoopState)\n",
    "workflow.add_node(\"increment\", increment_node)\n",
    "workflow.add_node(\"final_message\", final_message_node)\n",
    "\n",
    "workflow.set_entry_point(\"increment\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"increment\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue_loop\": \"increment\",\n",
    "        \"exit_loop\": \"final_message\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"final_message\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "# 実行\n",
    "initial_state = {\"count\": 0, \"max_count\": 3, \"log\": []}\n",
    "print(f\"--- カウンター上限 {initial_state['max_count']} のループテスト --- \")\n",
    "final_result = graph.invoke(initial_state)\n",
    "print(f\"最終状態のログ: {final_result['log']}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説002</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **ループの実現:** `increment_node` の処理後、`should_continue` ルーターが呼び出されます。このルーターが `\"continue_loop\"` を返すと、処理は再び `increment_node` に戻ります。これがループバックエッジとなり、繰り返し処理を実現します。\n",
    "*   **状態による制御:** ループを継続するかどうかは、状態 `CounterLoopState` の `count` と `max_count` の比較によって決定されます。状態がループの振る舞いを制御する中心的な役割を果たします。\n",
    "*   **終了条件:** `count` が `max_count` に達すると、ルーターは `\"exit_loop\"` を返し、処理は `final_message_node` に移り、その後 `END` でグラフが終了します。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題003: LLMによるループ継続判断（自己反省ループ）\n",
    "*   **学習内容:** LLM自身が生成内容を評価し、特定の品質基準を満たすまで、あるいは最大試行回数に達するまで再生成を繰り返す、より高度なループを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003 - グラフ構築\n",
    "____ typing ____ ____, ____, Optional\n",
    "____ langgraph.graph ____ ____, ____\n",
    "____ langchain_core.messages ____ ____, ____\n",
    "\n",
    "# 状態定義\n",
    "____ ReflectionLoopState(____):\n",
    "    topic: str\n",
    "    current_summary: Optional[str]\n",
    "    critique: Optional[str]\n",
    "    attempts_left: int\n",
    "    max_attempts: int\n",
    "    log: list[str]\n",
    "\n",
    "# ノード定義\n",
    "____ generate_summary_node(state: ReflectionLoopState):\n",
    "    prompt = f\"以下のトピックについて、非常に短い（1文程度の）要約を作成してください: {state['topic']}\"\n",
    "    ____ state.get(\"critique\") and \"完璧\" not in state[\"critique\"].lower(): # 「完璧」でない批判がある場合のみ考慮 (解答例より)\n",
    "        prompt += f\"\n",
    "前回の要約への批判: {state['critique']}\n",
    "この批判を踏まえて改善してください。\"\n",
    "    \n",
    "    print(f\"\n",
    "[要約生成ノード] プロンプト: {prompt}\")\n",
    "    response = llm.____(prompt) # llmは準備セルで初期化済み\n",
    "    summary = response.content.strip()\n",
    "    log_msg = f\"生成された要約: '{summary}'\"\n",
    "    print(log_msg)\n",
    "    ____ {\"current_summary\": summary, \"log\": state[\"log\"] + [log_msg], \"critique\": None} # critiqueをリセット (解答例より)\n",
    "\n",
    "____ critique_summary_node(state: ReflectionLoopState):\n",
    "    summary = state[\"current_summary\"]\n",
    "    prompt = f\"以下の要約を評価してください。改善点があれば具体的に指摘し、もし完璧だと思えば「完璧」とだけ答えてください。要約: '{summary}'\"\n",
    "    print(f\"\n",
    "[評価ノード] プロンプト: {prompt}\")\n",
    "    response = llm.____(prompt)\n",
    "    critique_text = response.content.strip()\n",
    "    log_msg = f\"評価結果: '{critique_text}'\"\n",
    "    print(log_msg)\n",
    "    new_attempts_left = state[\"attempts_left\"] - 1\n",
    "    return {\"critique\": critique_text, \"log\": state[\"log\"] + [log_msg], \"attempts_left\": new_attempts_left}\n",
    "\n",
    "def final_result_node(state: ReflectionLoopState):\n",
    "    final_log = f\"\n",
    "[最終結果ノード] 最終的な要約: '{state['current_summary']}', 残り試行回数: {state['attempts_left']}\"\n",
    "    print(final_log)\n",
    "    return {\"log\": state[\"log\"] + [final_log]}\n",
    "\n",
    "# ルーター関数\n",
    "def should_reflect_or_finish(state: ReflectionLoopState):\n",
    "    critique = state.get(\"critique\", \"\")\n",
    "    if \"完璧\" in critique.lower(): # 解答例より\n",
    "        print(\"  -> 評価が「完璧」のため終了します。\")\n",
    "        return \"finish\"\n",
    "    elif state[\"attempts_left\"] <= 0:\n",
    "        print(\"  -> 最大試行回数に達したため終了します。\")\n",
    "        return \"finish\"\n",
    "    else:\n",
    "        print(\"  -> 改善の余地あり、かつ試行回数が残っているため再生成します。\")\n",
    "        return \"regenerate\" # 再生成のキー (解答例より)\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(ReflectionLoopState)\n",
    "workflow.add_node(\"generate_summary\", generate_summary_node)\n",
    "workflow.add_node(\"critique_summary\", critique_summary_node)\n",
    "workflow.add_node(\"final_result\", final_result_node)\n",
    "\n",
    "workflow.set_entry_point(\"generate_summary\")\n",
    "workflow.add_edge(\"generate_summary\", \"critique_summary\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"critique_summary\",\n",
    "    should_reflect_or_finish,\n",
    "    {\n",
    "        \"regenerate\": \"generate_summary\", # 再生成の場合はこのノードへ (解答例より)\n",
    "        \"finish\": \"final_result\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"final_result\", END)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003 - グラフ可視化\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003 - グラフ実行\n",
    "topic_to_summarize = \"大規模言語モデルの最新の進展について\"\n",
    "max_reflection_attempts = 2\n",
    "initial_reflection_state = {\n",
    "    \"topic\": topic_to_summarize,\n",
    "    \"current_summary\": None,\n",
    "    \"critique\": None,\n",
    "    \"attempts_left\": max_reflection_attempts,\n",
    "    \"max_attempts\": max_reflection_attempts,\n",
    "    \"log\": []\n",
    "}\n",
    "print(f\"--- 自己反省ループテスト (トピック: {topic_to_summarize}, 最大試行回数: {max_reflection_attempts}) ---\")\n",
    "final_reflection_result = graph.invoke(initial_reflection_state, {\"recursion_limit\": 10}) # recursion_limitを適切に設定\n",
    "print(f\"\n",
    "最終状態のログ:\")\n",
    "for log_entry in final_reflection_result['log']:\n",
    "    print(f\"  - {log_entry}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答003</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage # AIMessageは直接は使わないが、llm応答がAIMessageなのでimportは維持\n",
    "\n",
    "# 状態定義\n",
    "class ReflectionLoopState(TypedDict):\n",
    "    topic: str\n",
    "    current_summary: Optional[str]\n",
    "    critique: Optional[str]\n",
    "    attempts_left: int\n",
    "    max_attempts: int\n",
    "    log: list[str]\n",
    "\n",
    "# ノード定義\n",
    "def generate_summary_node(state: ReflectionLoopState):\n",
    "    prompt = f\"以下のトピックについて、非常に短い（1文程度の）要約を作成してください: {state['topic']}\"\n",
    "    if state.get(\"critique\") and \"完璧\" not in state[\"critique\"].lower(): # 「完璧」でない批判がある場合のみ考慮\n",
    "        prompt += f\"\n",
    "前回の要約への批判: {state['critique']}\n",
    "この批判を踏まえて改善してください。\"\n",
    "    \n",
    "    print(f\"\n",
    "[要約生成ノード] プロンプト: {prompt}\")\n",
    "    response = llm.invoke(prompt)\n",
    "    summary = response.content.strip()\n",
    "    log_msg = f\"生成された要約: '{summary}'\"\n",
    "    print(log_msg)\n",
    "    return {\"current_summary\": summary, \"log\": state[\"log\"] + [log_msg], \"critique\": None} \n",
    "\n",
    "def critique_summary_node(state: ReflectionLoopState):\n",
    "    summary = state[\"current_summary\"]\n",
    "    prompt = f\"以下の要約を評価してください。改善点があれば具体的に指摘し、もし完璧だと思えば「完璧」とだけ答えてください。要約: '{summary}'\"\n",
    "    print(f\"\n",
    "[評価ノード] プロンプト: {prompt}\")\n",
    "    response = llm.invoke(prompt)\n",
    "    critique_text = response.content.strip()\n",
    "    log_msg = f\"評価結果: '{critique_text}'\"\n",
    "    print(log_msg)\n",
    "    new_attempts_left = state[\"attempts_left\"] - 1\n",
    "    return {\"critique\": critique_text, \"log\": state[\"log\"] + [log_msg], \"attempts_left\": new_attempts_left}\n",
    "\n",
    "def final_result_node(state: ReflectionLoopState):\n",
    "    final_log = f\"\n",
    "[最終結果ノード] 最終的な要約: '{state['current_summary']}', 残り試行回数: {state['attempts_left']}\"\n",
    "    print(final_log)\n",
    "    return {\"log\": state[\"log\"] + [final_log]}\n",
    "\n",
    "# ルーター関数\n",
    "def should_reflect_or_finish(state: ReflectionLoopState):\n",
    "    critique = state.get(\"critique\", \"\")\n",
    "    if \"完璧\" in critique.lower():\n",
    "        print(\"  -> 評価が「完璧」のため終了します。\")\n",
    "        return \"finish\"\n",
    "    elif state[\"attempts_left\"] <= 0:\n",
    "        print(\"  -> 最大試行回数に達したため終了します。\")\n",
    "        return \"finish\"\n",
    "    else:\n",
    "        print(\"  -> 改善の余地あり、かつ試行回数が残っているため再生成します。\")\n",
    "        return \"regenerate\"\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(ReflectionLoopState)\n",
    "workflow.add_node(\"generate_summary\", generate_summary_node)\n",
    "workflow.add_node(\"critique_summary\", critique_summary_node)\n",
    "workflow.add_node(\"final_result\", final_result_node)\n",
    "\n",
    "workflow.set_entry_point(\"generate_summary\")\n",
    "workflow.add_edge(\"generate_summary\", \"critique_summary\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"critique_summary\",\n",
    "    should_reflect_or_finish,\n",
    "    {\n",
    "        \"regenerate\": \"generate_summary\",\n",
    "        \"finish\": \"final_result\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"final_result\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "# 実行\n",
    "topic_to_summarize = \"LangGraphライブラリの主な機能と利点について\"\n",
    "max_reflection_attempts = 2 \n",
    "initial_reflection_state = {\n",
    "    \"topic\": topic_to_summarize,\n",
    "    \"current_summary\": None,\n",
    "    \"critique\": None,\n",
    "    \"attempts_left\": max_reflection_attempts,\n",
    "    \"max_attempts\": max_reflection_attempts,\n",
    "    \"log\": []\n",
    "}\n",
    "print(f\"--- 自己反省ループテスト (トピック: {topic_to_summarize}, 最大試行回数: {max_reflection_attempts}) ---\")\n",
    "final_reflection_result = graph.invoke(initial_reflection_state, {\"recursion_limit\": 15}) # ループがあるのでrecursion_limitを増やす\n",
    "print(f\"\n",
    "最終状態のログ:\")\n",
    "for log_entry in final_reflection_result['log']:\n",
    "    print(f\"  - {log_entry}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説003</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **LLMによる評価と改善指示:** `critique_summary_node` がLLMを使って現在の要約を評価し、改善点を指摘します（または「完璧」と判断します）。`generate_summary_node` は、この批判（`critique`）を次のプロンプトに含めることで、より良い要約を生成しようと試みます。\n",
    "*   **ループ制御:** `should_reflect_or_finish` ルーターが、評価結果（`critique`）と残り試行回数（`attempts_left`）に基づいてループを継続するか（`\"regenerate\"` で `generate_summary_node` に戻る）、終了するか（`\"finish\"` で `final_result_node` に進む）を決定します。\n",
    "*   **状態の役割:** `current_summary`（最新の生成物）、`critique`（LLMからのフィードバック）、`attempts_left`（ループ終了条件）といった状態が、この自己反省ループの中心的な役割を担います。\n",
    "*   **`recursion_limit`:** ループ構造を持つグラフを実行する場合、`invoke` メソッドの `config` 引数で `recursion_limit` を適切に設定することが重要です。デフォルト値ではループの深さが足りずにエラーになることがあります。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題004: 並列処理（ファンアウト・ファンイン）による複数タスクの同時実行\n",
    "*   **学習内容:** 1つの始点ノードから複数のノードへ処理を分岐（ファンアウト）させ、それらの完了を待ってから結果を統合（ファンイン）する方法を学びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004 - グラフ構築\n",
    "____ typing ____ ____, List, Optional\n",
    "____ langgraph.graph ____ ____, ____\n",
    "\n",
    "# 状態定義\n",
    "____ FanOutFanInState(____):\n",
    "    input_data: str\n",
    "    processed_a: Optional[str]\n",
    "    processed_b: Optional[str]\n",
    "    final_output: Optional[str]\n",
    "\n",
    "# ノード定義\n",
    "____ start_node(state: FanOutFanInState):\n",
    "    print(f\"[開始ノード] 入力データ: {state['input_data']}\")\n",
    "    ____ {}\n",
    "\n",
    "____ process_a_node(state: FanOutFanInState):\n",
    "    # ここではダミー処理として入力に \"_A\" を追加\n",
    "    result_a = state[\"input_data\"] + \"_ProcessedA\"\n",
    "    print(f\"  [処理Aノード] 結果: {result_a}\")\n",
    "    ____ {\"processed_a\": result_a}\n",
    "\n",
    "____ process_b_node(state: FanOutFanInState):\n",
    "    # ここではダミー処理として入力に \"_B\" を追加\n",
    "    result_b = state[\"input_data\"] + \"_ProcessedB\"\n",
    "    print(f\"  [処理Bノード] 結果: {result_b}\")\n",
    "    ____ {\"processed_b\": result_b}\n",
    "\n",
    "____ aggregate_node(state: FanOutFanInState):\n",
    "    # process_a_node と process_b_node の両方の完了を待って実行される\n",
    "    result = f\"集約結果: A='{state.get('processed_a', 'N/A')}', B='{state.get('processed_b', 'N/A')}'\"\n",
    "    print(f\"[集約ノード] {result}\")\n",
    "    ____ {\"final_output\": result}\n",
    "\n",
    "# グラフ構築\n",
    "workflow = ____(FanOutFanInState)\n",
    "workflow.____(\"start\", start_node)\n",
    "workflow.____(\"process_a\", process_a_node)\n",
    "workflow.add_node(\"process_b\", process_b_node)\n",
    "workflow.add_node(\"aggregate\", aggregate_node)\n",
    "\n",
    "workflow.set_entry_point(\"start\")\n",
    "\n",
    "# ファンアウト: start から process_a と process_b へ\n",
    "workflow.add_edge(\"start\", \"process_a\")\n",
    "workflow.add_edge(\"start\", \"process_b\") # 解答例より\n",
    "\n",
    "# ファンイン: process_a と process_b の両方が完了したら aggregate へ\n",
    "workflow.add_edge([\"process_a\", \"process_b\"], \"aggregate\") # 解答例より\n",
    "\n",
    "workflow.add_edge(\"aggregate\", END)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004 - グラフ可視化\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004 - グラフ実行\n",
    "initial_fan_state = {\"input_data\": \"初期データ\", \"processed_a\":None, \"processed_b\":None, \"final_output\":None} # Optionalキーも初期化 (解答例より)\n",
    "print(f\"--- ファンアウト・ファンイン テスト (入力: {initial_fan_state['input_data']}) ---\")\n",
    "final_fan_result = graph.invoke(initial_fan_state)\n",
    "print(f\"最終出力: {final_fan_result.get('final_output')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答004</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# 状態定義\n",
    "class FanOutFanInState(TypedDict):\n",
    "    input_data: str\n",
    "    processed_a: Optional[str]\n",
    "    processed_b: Optional[str]\n",
    "    final_output: Optional[str]\n",
    "\n",
    "# ノード定義\n",
    "def start_node(state: FanOutFanInState):\n",
    "    print(f\"[開始ノード] 入力データ: {state['input_data']}\")\n",
    "    return {}\n",
    "\n",
    "def process_a_node(state: FanOutFanInState):\n",
    "    result_a = state[\"input_data\"] + \"_ProcessedA\"\n",
    "    print(f\"  [処理Aノード] 結果: {result_a}\")\n",
    "    return {\"processed_a\": result_a}\n",
    "\n",
    "def process_b_node(state: FanOutFanInState):\n",
    "    result_b = state[\"input_data\"] + \"_ProcessedB\"\n",
    "    print(f\"  [処理Bノード] 結果: {result_b}\")\n",
    "    return {\"processed_b\": result_b}\n",
    "\n",
    "def aggregate_node(state: FanOutFanInState):\n",
    "    result = f\"集約結果: A='{state.get('processed_a', 'N/A')}', B='{state.get('processed_b', 'N/A')}'\"\n",
    "    print(f\"[集約ノード] {result}\")\n",
    "    return {\"final_output\": result}\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(FanOutFanInState)\n",
    "workflow.add_node(\"start\", start_node)\n",
    "workflow.add_node(\"process_a\", process_a_node)\n",
    "workflow.add_node(\"process_b\", process_b_node)\n",
    "workflow.add_node(\"aggregate\", aggregate_node)\n",
    "\n",
    "workflow.set_entry_point(\"start\")\n",
    "\n",
    "workflow.add_edge(\"start\", \"process_a\")\n",
    "workflow.add_edge(\"start\", \"process_b\") # ファンアウト\n",
    "\n",
    "workflow.add_edge([\"process_a\", \"process_b\"], \"aggregate\") # ファンイン\n",
    "\n",
    "workflow.add_edge(\"aggregate\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "# 実行\n",
    "initial_fan_state = {\"input_data\": \"初期データ\", \"processed_a\":None, \"processed_b\":None, \"final_output\":None} # Optionalキーも初期化\n",
    "print(f\"--- ファンアウト・ファンイン テスト (入力: {initial_fan_state['input_data']}) ---\")\n",
    "final_fan_result = graph.invoke(initial_fan_state)\n",
    "print(f\"最終出力: {final_fan_result.get('final_output')}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説004</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **ファンアウト:** `start_node` から `process_a_node` と `process_b_node` の両方にエッジを接続することで、`start_node` の処理完了後、これら2つのノードが（理論上）並列に実行されます。\n",
    "*   **ファンイン:** `add_edge` の第1引数にノード名のリスト `[\"process_a\", \"process_b\"]` を指定し、第2引数に `aggregate_node` を指定することで、`process_a_node` と `process_b_node` の**両方の処理が完了するのを待ってから** `aggregate_node` が実行されます。\n",
    "*   これにより、複数の独立した処理を並行して行い、その結果を後で統合する、といったワークフローを効率的に構築できます。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題005: Human-in-the-Loop (人間による介在と承認)\n",
    "*   **学習内容:** `Interrupt` を使用してグラフの実行を一時停止し、人間の確認・承認を得てから処理を再開または分岐させる方法を学びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005 - グラフ構築\n",
    "____ typing ____ ____, ____, Optional\n",
    "____ langgraph.graph ____ ____, ____, ____\n",
    "____ uuid ____ uuid4\n",
    "____ langgraph.checkpoint.memory ____ ____ # MemorySaverを使う場合 (解答例より)\n",
    "\n",
    "# 状態定義\n",
    "____ HumanApprovalState(____):\n",
    "    task_description: str\n",
    "    generated_plan: Optional[str]\n",
    "    human_feedback: Optional[str] # \"approve\" or \"reject\" or other comments\n",
    "    final_result: Optional[str]\n",
    "    log: list[str]\n",
    "\n",
    "# ノード定義\n",
    "____ plan_generation_node(state: HumanApprovalState):\n",
    "    # ここではダミーの計画生成\n",
    "    plan = f\"タスク「{state['task_description']}」に対する計画案: ステップ1 -> ステップ2 -> ステップ3\"\n",
    "    print(f\"[計画生成ノード] 生成された計画: {plan}\")\n",
    "    ____ {\"generated_plan\": plan, \"log\": state[\"log\"] + [f\"計画生成: {plan}\"]}\n",
    "\n",
    "____ wait_for_human_approval_node(state: HumanApprovalState):\n",
    "    print(f\"\n",
    "[人間承認待機ノード] 以下の計画について承認またはフィードバックを待っています:\")\n",
    "    print(f\"  計画: {state['generated_plan']}\")\n",
    "    # Interruptを発生させて処理を一時停止 (解答例ではcompile時に指定)\n",
    "    # raise ____() # ここでraiseするか、compile時にinterrupt_beforeを指定\n",
    "    return {} # 何も状態を更新しない (解答例より)\n",
    "\n",
    "def process_approved_plan_node(state: HumanApprovalState):\n",
    "    approved_plan = state[\"generated_plan\"]\n",
    "    # 承認された計画を実行するダミー処理\n",
    "    result = f\"計画「{approved_plan}」が承認され、正常に実行されました。\"\n",
    "    print(f\"[計画実行ノード] {result}\")\n",
    "    return {\"final_result\": result, \"log\": state[\"log\"] + [result]}\n",
    "\n",
    "def process_rejected_plan_node(state: HumanApprovalState):\n",
    "    rejected_plan = state[\"generated_plan\"]\n",
    "    feedback = state.get(\"human_feedback\", \"(フィードバックなし)\")\n",
    "    result = f\"計画「{rejected_plan}」は拒否されました。フィードバック: {feedback}\"\n",
    "    print(f\"[計画拒否処理ノード] {result}\")\n",
    "    return {\"final_result\": result, \"log\": state[\"log\"] + [result]}\n",
    "\n",
    "# ルーター関数\n",
    "def route_after_human_feedback(state: HumanApprovalState):\n",
    "    feedback = state.get(\"human_feedback\", \"\").lower()\n",
    "    if \"approve\" in feedback:\n",
    "        print(\"  -> 人間が承認したため、計画を実行します。\")\n",
    "        return \"approved\"\n",
    "    else:\n",
    "        print(\"  -> 人間が拒否または他のフィードバックを与えたため、拒否処理を行います。\")\n",
    "        return \"rejected\"\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(HumanApprovalState)\n",
    "workflow.add_node(\"plan_generator\", plan_generation_node)\n",
    "workflow.add_node(\"human_approval_step\", wait_for_human_approval_node)\n",
    "workflow.add_node(\"execute_approved\", process_approved_plan_node)\n",
    "workflow.add_node(\"handle_rejection\", process_rejected_plan_node)\n",
    "\n",
    "workflow.set_entry_point(\"plan_generator\")\n",
    "workflow.add_edge(\"plan_generator\", \"human_approval_step\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"human_approval_step\",\n",
    "    route_after_human_feedback,\n",
    "    {\n",
    "        \"approved\": \"execute_approved\",\n",
    "        \"rejected\": \"handle_rejection\" # 解答例より\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"execute_approved\", END)\n",
    "workflow.add_edge(\"handle_rejection\", END)\n",
    "\n",
    "memory = MemorySaver() # 解答例より\n",
    "graph = workflow.compile(checkpointer=memory, interrupt_before=[\"human_approval_step\"]) # Interrupt設定 (解答例より)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005 - グラフ可視化\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005 - グラフ実行\n",
    "task = \"新しいマーケティングキャンペーンを開始する\"\n",
    "initial_approval_state = {\n",
    "    \"task_description\": task,\n",
    "    \"log\": [],\n",
    "    \"generated_plan\": None, # Optionalキーも初期化 (解答例より)\n",
    "    \"human_feedback\": None,\n",
    "    \"final_result\": None\n",
    "}\n",
    "\n",
    "print(f\"--- Human-in-the-Loopテスト (タスク: {task}) ---\")\n",
    "thread_id = str(uuid4()) # 各実行にユニークなIDを割り当てる\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "print(\"グラフを初期状態で実行します...\")\n",
    "graph.invoke(initial_approval_state, config=config)\n",
    "\n",
    "print(\"\n",
    "--- 中断後のグラフの状態 --- (Interruptが発生したはず)\")\n",
    "current_graph_state = graph.get_state(config)\n",
    "if current_graph_state: # 解答例よりNoneチェック追加\n",
    "    print(f\"現在の計画: {current_graph_state.values.get('generated_plan')}\")\n",
    "    print(f\"現在のログ: {current_graph_state.values.get('log')}\")\n",
    "else:\n",
    "    print(\"中断状態が取得できませんでした。\")\n",
    "\n",
    "# ユーザーからのフィードバックをシミュレート\n",
    "user_choice = input(\"計画を承認しますか？ (approve / reject [フィードバック]): \").strip()\n",
    "\n",
    "feedback_to_inject = {}\n",
    "if user_choice.lower().startswith(\"approve\"):\n",
    "    feedback_to_inject = {\"human_feedback\": \"approve\"}\n",
    "elif user_choice.lower().startswith(\"reject\"):\n",
    "    feedback_to_inject = {\"human_feedback\": user_choice} # reject [フィードバック] 全体を渡す\n",
    "else:\n",
    "    feedback_to_inject = {\"human_feedback\": f\"other: {user_choice}\"}\n",
    "\n",
    "print(f\"\n",
    "注入するフィードバック: {feedback_to_inject}\")\n",
    "final_approval_result = graph.invoke(feedback_to_inject, config=config) # 中断したところから再開\n",
    "\n",
    "print(\"\n",
    "--- 最終結果 ---\")\n",
    "if final_approval_result: # 解答例よりNoneチェック追加\n",
    "    print(f\"最終的な結果メッセージ: {final_approval_result.get('final_result')}\")\n",
    "    print(f\"最終ログ: {final_approval_result.get('log')}\")\n",
    "else:\n",
    "    print(\"最終状態が取得できませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答005</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "from langgraph.graph import StateGraph, END, Interrupt\n",
    "from uuid import uuid4\n",
    "from langgraph.checkpoint.memory import MemorySaver # MemorySaverを使う場合\n",
    "\n",
    "# 状態定義\n",
    "class HumanApprovalState(TypedDict):\n",
    "    task_description: str\n",
    "    generated_plan: Optional[str]\n",
    "    human_feedback: Optional[str]\n",
    "    final_result: Optional[str]\n",
    "    log: list[str]\n",
    "\n",
    "# ノード定義\n",
    "def plan_generation_node(state: HumanApprovalState):\n",
    "    plan = f\"タスク「{state['task_description']}」に対する計画案: ステップ1 -> ステップ2 -> ステップ3\"\n",
    "    print(f\"[計画生成ノード] 生成された計画: {plan}\")\n",
    "    return {\"generated_plan\": plan, \"log\": state[\"log\"] + [f\"計画生成: {plan}\"]}\n",
    "\n",
    "def wait_for_human_approval_node(state: HumanApprovalState):\n",
    "    print(f\"\n",
    "[人間承認待機ノード] 以下の計画について承認またはフィードバックを待っています:\")\n",
    "    print(f\"  計画: {state['generated_plan']}\")\n",
    "    # このノードが interrupt_before に指定されているため、実行前に Interrupt が発生\n",
    "    # ここで raise Interrupt() を書く必要はない (compile時に指定するため)\n",
    "    # もしノードの処理の途中で中断したい場合は raise Interrupt() を使う\n",
    "    return {} # 何も状態を更新しないが、Interrupt後に再開されることを示す\n",
    "\n",
    "def process_approved_plan_node(state: HumanApprovalState):\n",
    "    approved_plan = state[\"generated_plan\"]\n",
    "    result = f\"計画「{approved_plan}」が承認され、正常に実行されました。\"\n",
    "    print(f\"[計画実行ノード] {result}\")\n",
    "    return {\"final_result\": result, \"log\": state[\"log\"] + [result]}\n",
    "\n",
    "def process_rejected_plan_node(state: HumanApprovalState):\n",
    "    rejected_plan = state[\"generated_plan\"]\n",
    "    feedback = state.get(\"human_feedback\", \"(フィードバックなし)\")\n",
    "    result = f\"計画「{rejected_plan}」は拒否されました。フィードバック: {feedback}\"\n",
    "    print(f\"[計画拒否処理ノード] {result}\")\n",
    "    return {\"final_result\": result, \"log\": state[\"log\"] + [result]}\n",
    "\n",
    "# ルーター関数\n",
    "def route_after_human_feedback(state: HumanApprovalState):\n",
    "    feedback = state.get(\"human_feedback\", \"\").lower()\n",
    "    if \"approve\" in feedback:\n",
    "        print(\"  -> 人間が承認したため、計画を実行します。\")\n",
    "        return \"approved\"\n",
    "    else:\n",
    "        print(\"  -> 人間が拒否または他のフィードバックを与えたため、拒否処理を行います。\")\n",
    "        return \"rejected\"\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(HumanApprovalState)\n",
    "workflow.add_node(\"plan_generator\", plan_generation_node)\n",
    "workflow.add_node(\"human_approval_step\", wait_for_human_approval_node)\n",
    "workflow.add_node(\"execute_approved\", process_approved_plan_node)\n",
    "workflow.add_node(\"handle_rejection\", process_rejected_plan_node)\n",
    "\n",
    "workflow.set_entry_point(\"plan_generator\")\n",
    "workflow.add_edge(\"plan_generator\", \"human_approval_step\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"human_approval_step\",\n",
    "    route_after_human_feedback,\n",
    "    {\n",
    "        \"approved\": \"execute_approved\",\n",
    "        \"rejected\": \"handle_rejection\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"execute_approved\", END)\n",
    "workflow.add_edge(\"handle_rejection\", END)\n",
    "\n",
    "# MemorySaverインスタンスを作成\n",
    "memory = MemorySaver()\n",
    "\n",
    "# interrupt_beforeにノード名を指定し、checkpointerにMemorySaverインスタンスを渡す\n",
    "graph = workflow.compile(checkpointer=memory, interrupt_before=[\"human_approval_step\"])\n",
    "\n",
    "# 実行 (インタラクティブ)\n",
    "task = \"新しいマーケティングキャンペーンを開始する\"\n",
    "initial_approval_state = {\n",
    "    \"task_description\": task,\n",
    "    \"log\": [],\n",
    "    \"generated_plan\": None, # Optionalキーも初期化\n",
    "    \"human_feedback\": None,\n",
    "    \"final_result\": None\n",
    "}\n",
    "\n",
    "print(f\"--- Human-in-the-Loopテスト (タスク: {task}) ---\")\n",
    "thread_id = str(uuid4()) \n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "print(\"グラフを初期状態で実行します...\")\n",
    "# invokeを呼び出すと、interrupt_beforeで指定したノードの手前でInterruptが発生する\n",
    "graph.invoke(initial_approval_state, config=config)\n",
    "\n",
    "print(\"\n",
    "--- 中断後のグラフの状態 --- (Interruptが発生しました)\")\n",
    "current_graph_state = graph.get_state(config)\n",
    "if current_graph_state:\n",
    "    print(f\"現在の計画: {current_graph_state.values.get('generated_plan')}\")\n",
    "    print(f\"現在のログ: {current_graph_state.values.get('log')}\")\n",
    "else:\n",
    "    print(\"中断状態が取得できませんでした。\")\n",
    "\n",
    "user_choice = input(\"計画を承認しますか？ (approve / reject [フィードバック]): \").strip()\n",
    "\n",
    "feedback_to_inject = {}\n",
    "if user_choice.lower().startswith(\"approve\"):\n",
    "    feedback_to_inject = {\"human_feedback\": \"approve\"}\n",
    "elif user_choice.lower().startswith(\"reject\"):\n",
    "    feedback_to_inject = {\"human_feedback\": user_choice}\n",
    "else:\n",
    "    feedback_to_inject = {\"human_feedback\": f\"other: {user_choice}\"}\n",
    "\n",
    "print(f\"\n",
    "注入するフィードバック: {feedback_to_inject}\")\n",
    "# 中断したグラフにフィードバックを与えて再開\n",
    "# invokeの第一引数はNoneでも良いが、更新する状態を渡すのが一般的\n",
    "final_approval_result_state = graph.invoke(feedback_to_inject, config=config)\n",
    "\n",
    "print(\"\n",
    "--- 最終結果 ---\")\n",
    "if final_approval_result_state:\n",
    "    print(f\"最終的な結果メッセージ: {final_approval_result_state.get('final_result')}\")\n",
    "    print(f\"最終ログ: {final_approval_result_state.get('log')}\")\n",
    "else:\n",
    "    print(\"最終状態が取得できませんでした。\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説005</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **`Interrupt` と `checkpointer`:** `graph.compile()`時に `interrupt_before=[\"node_name\"]` を指定すると、指定されたノードの実行直前でグラフの実行が一時停止（`Interrupt`）します。この機能を利用するには、`checkpointer`（例: `MemorySaver`）の設定が必須です。`checkpointer`はグラフの状態を保存・復元する役割を担います。\n",
    "*   **グラフの実行と中断:** `graph.invoke(initial_state, config)` を実行すると、`human_approval_step` ノードの手前で中断します。`config` には `{\"configurable\": {\"thread_id\": \"unique_id\"}}` のようにスレッドIDを指定し、中断と再開を同じ会話（状態）コンテキストで行えるようにします。\n",
    "*   **状態の取得と更新:** 中断後、`graph.get_state(config)` で現在のグラフの状態を取得できます。ユーザーからのフィードバック（承認/拒否）を新しい状態（`feedback_to_inject`）として用意し、再び `graph.invoke(feedback_to_inject, config)` を呼び出すことで、中断した箇所から処理を再開します。この際、`human_approval_step` ノード自体は中断前に実行されていないため、再開後に実行され、その後 `route_after_human_feedback` ルーターが新しいフィードバックに基づいて処理を分岐させます。\n",
    "*   **`wait_for_human_approval_node` の役割:** このノードは、`interrupt_before` で指定されているため、実際にはその中身が実行される前に中断が発生します。再開後、このノードが（もし何か処理があれば）実行され、その後エッジとルーターが評価されます。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題006: エラーハンドリングとリトライ処理\n",
    "*   **学習内容:** `try-except`ブロックでエラーを捕捉し、状態に記録します。さらに、そのエラー情報に基づいて、指定回数だけ処理をリトライするループを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄006 - グラフ構築\n",
    "____ typing ____ ____, Optional, ____\n",
    "____ random\n",
    "____ langgraph.graph ____ ____, ____\n",
    "\n",
    "# 状態定義\n",
    "____ RetryState(____):\n",
    "    data_to_process: str\n",
    "    processed_result: Optional[str]\n",
    "    error_message: Optional[str]\n",
    "    retry_count: int\n",
    "    max_retries: int\n",
    "    log: list[str]\n",
    "\n",
    "# ノード定義\n",
    "____ potentially_failing_node(state: RetryState):\n",
    "    print(f\"\n",
    "[処理試行ノード] データ: '{state['data_to_process']}', 試行回数: {state['retry_count'] + 1}\")\n",
    "    # 最初の (max_retries - 1) 回までは高確率で失敗させるための調整 (解答例より)\n",
    "    should_fail_this_time = random.random() < 0.8 # 80%の確率で失敗を試みる (解答例より)\n",
    "    is_last_retry_attempt = (state['retry_count'] + 1) >= state['max_retries']\n",
    "\n",
    "    ____ should_fail_this_time and not is_last_retry_attempt:\n",
    "        error_msg = \"ランダムエラー発生！処理に失敗しました。\"\n",
    "        print(f\"  -> エラー: {error_msg}\")\n",
    "        ____ {\n",
    "            \"error_message\": error_msg, \n",
    "            \"retry_count\": state[\"retry_count\"] + 1,\n",
    "            \"log\": state[\"log\"] + [f\"試行 {state['retry_count'] + 1}: エラー - {error_msg}\"]\n",
    "        }\n",
    "    ____:\n",
    "        success_reason = \"成功\" ____ not is_last_retry_attempt ____ \"最後のリトライで成功扱い\" # 解答例より\n",
    "        result = f\"データ「{state['data_to_process']}」の処理{success_reason} (試行 {state['retry_count'] + 1} 回目)\"\n",
    "        print(f\"  -> {success_reason}: {result}\")\n",
    "        ____ {\n",
    "            \"processed_result\": result, \n",
    "            \"error_message\": None, # エラー情報をクリア (解答例より)\n",
    "            \"retry_count\": state[\"retry_count\"] + 1,\n",
    "            \"log\": state[\"log\"] + [f\"試行 {state['retry_count'] + 1}: {success_reason} - {result}\"]\n",
    "        }\n",
    "\n",
    "____ success_node(state: RetryState):\n",
    "    msg = f\"[成功処理ノード] 最終結果: {state['processed_result']}\"\n",
    "    print(msg)\n",
    "    ____ {\"log\": state[\"log\"] + [msg]}\n",
    "\n",
    "def failure_node(state: RetryState):\n",
    "    msg = f\"[失敗処理ノード] 最大リトライ回数 ({state['max_retries']}) に達しました。最終エラー: {state['error_message']}\"\n",
    "    print(msg)\n",
    "    return {\"log\": state[\"log\"] + [msg]}\n",
    "\n",
    "# ルーター関数\n",
    "def should_retry_or_fail(state: RetryState):\n",
    "    if state.get(\"error_message\") and state[\"retry_count\"] < state[\"max_retries\"]:\n",
    "        print(\"  -> エラー発生、リトライします。\")\n",
    "        return \"retry\"\n",
    "    elif state.get(\"error_message\"):\n",
    "        print(\"  -> エラー発生、最大リトライ回数超過。失敗処理へ。\")\n",
    "        return \"fail\"\n",
    "    else: # エラーなし (成功)\n",
    "        print(\"  -> 処理成功。成功処理へ。\")\n",
    "        return \"succeed\" # 成功時のキー (解答例より)\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(RetryState)\n",
    "workflow.add_node(\"process_data\", potentially_failing_node)\n",
    "workflow.add_node(\"handle_success\", success_node)\n",
    "workflow.add_node(\"handle_failure\", failure_node)\n",
    "\n",
    "workflow.set_entry_point(\"process_data\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"process_data\",\n",
    "    should_retry_or_fail,\n",
    "    {\n",
    "        \"retry\": \"process_data\",\n",
    "        \"succeed\": \"handle_success\", # 成功時はこのノードへ (解答例より)\n",
    "        \"fail\": \"handle_failure\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"handle_success\", END)\n",
    "workflow.add_edge(\"handle_failure\", END)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄006 - グラフ可視化\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄006 - グラフ実行\n",
    "data = \"重要なデータ\"\n",
    "retries = 3\n",
    "initial_retry_state = {\n",
    "    \"data_to_process\": data,\n",
    "    \"retry_count\": 0,\n",
    "    \"max_retries\": retries,\n",
    "    \"log\": [],\n",
    "    \"processed_result\": None, # Optionalキーも初期化\n",
    "    \"error_message\": None\n",
    "}\n",
    "print(f\"--- エラーハンドリングとリトライテスト (データ: {data}, 最大リトライ: {retries}) ---\")\n",
    "final_retry_result = graph.invoke(initial_retry_state, {\"recursion_limit\": 15})\n",
    "print(\"\n",
    "最終ログ:\")\n",
    "for entry in final_retry_result['log']:\n",
    "    print(f\"  - {entry}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答006</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Optional, Annotated\n",
    "import random\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# 状態定義\n",
    "class RetryState(TypedDict):\n",
    "    data_to_process: str\n",
    "    processed_result: Optional[str]\n",
    "    error_message: Optional[str]\n",
    "    retry_count: int\n",
    "    max_retries: int\n",
    "    log: list[str]\n",
    "\n",
    "# ノード定義\n",
    "def potentially_failing_node(state: RetryState):\n",
    "    print(f\"\n",
    "[処理試行ノード] データ: '{state['data_to_process']}', 試行回数: {state['retry_count'] + 1}\")\n",
    "    # 最初の (max_retries - 1) 回までは高確率で失敗させるための調整\n",
    "    # これにより、少なくとも数回のリトライが試みられる可能性を高める\n",
    "    # 例: max_retries = 3 の場合、retry_count が 0, 1 の時は失敗しやすく、2 (最後のリトライ)では成功しやすくする\n",
    "    should_fail_this_time = random.random() < 0.8 # 80%の確率で失敗を試みる\n",
    "    is_last_retry_attempt = (state['retry_count'] + 1) >= state['max_retries']\n",
    "\n",
    "    if should_fail_this_time and not is_last_retry_attempt:\n",
    "        error_msg = \"ランダムエラー発生！処理に失敗しました。\"\n",
    "        print(f\"  -> エラー: {error_msg}\")\n",
    "        return {\n",
    "            \"error_message\": error_msg, \n",
    "            \"retry_count\": state[\"retry_count\"] + 1,\n",
    "            \"log\": state[\"log\"] + [f\"試行 {state['retry_count'] + 1}: エラー - {error_msg}\"]\n",
    "        }\n",
    "    else:\n",
    "        # 成功、または最後のリトライでは強制的に成功させる（デモのため）\n",
    "        success_reason = \"成功\" if not is_last_retry_attempt else \"最後のリトライで成功扱い\"\n",
    "        result = f\"データ「{state['data_to_process']}」の処理{success_reason} (試行 {state['retry_count'] + 1} 回目)\"\n",
    "        print(f\"  -> {success_reason}: {result}\")\n",
    "        return {\n",
    "            \"processed_result\": result, \n",
    "            \"error_message\": None, \n",
    "            \"retry_count\": state[\"retry_count\"] + 1,\n",
    "            \"log\": state[\"log\"] + [f\"試行 {state['retry_count'] + 1}: {success_reason} - {result}\"]\n",
    "        }\n",
    "\n",
    "def success_node(state: RetryState):\n",
    "    msg = f\"[成功処理ノード] 最終結果: {state['processed_result']}\"\n",
    "    print(msg)\n",
    "    return {\"log\": state[\"log\"] + [msg]}\n",
    "\n",
    "def failure_node(state: RetryState):\n",
    "    msg = f\"[失敗処理ノード] 最大リトライ回数 ({state['max_retries']}) に達しました。最終エラー: {state['error_message']}\"\n",
    "    print(msg)\n",
    "    return {\"log\": state[\"log\"] + [msg]}\n",
    "\n",
    "# ルーター関数\n",
    "def should_retry_or_fail(state: RetryState):\n",
    "    if state.get(\"error_message\") and state[\"retry_count\"] < state[\"max_retries\"]:\n",
    "        print(\"  -> エラー発生、リトライします。\")\n",
    "        return \"retry\"\n",
    "    elif state.get(\"error_message\"):\n",
    "        print(\"  -> エラー発生、最大リトライ回数超過。失敗処理へ。\")\n",
    "        return \"fail\"\n",
    "    else: \n",
    "        print(\"  -> 処理成功。成功処理へ。\")\n",
    "        return \"succeed\"\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(RetryState)\n",
    "workflow.add_node(\"process_data\", potentially_failing_node)\n",
    "workflow.add_node(\"handle_success\", success_node)\n",
    "workflow.add_node(\"handle_failure\", failure_node)\n",
    "\n",
    "workflow.set_entry_point(\"process_data\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"process_data\",\n",
    "    should_retry_or_fail,\n",
    "    {\n",
    "        \"retry\": \"process_data\",\n",
    "        \"succeed\": \"handle_success\",\n",
    "        \"fail\": \"handle_failure\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"handle_success\", END)\n",
    "workflow.add_edge(\"handle_failure\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "# 実行\n",
    "data = \"重要なデータ\"\n",
    "retries = 3\n",
    "initial_retry_state = {\n",
    "    \"data_to_process\": data,\n",
    "    \"retry_count\": 0,\n",
    "    \"max_retries\": retries,\n",
    "    \"log\": [],\n",
    "    \"processed_result\": None,\n",
    "    \"error_message\": None\n",
    "}\n",
    "print(f\"--- エラーハンドリングとリトライテスト (データ: {data}, 最大リトライ: {retries}) ---\")\n",
    "final_retry_result = graph.invoke(initial_retry_state, {\"recursion_limit\": 2 * retries + 5}) # リトライ回数に応じて調整\n",
    "print(\"\n",
    "最終ログ:\")\n",
    "for entry in final_retry_result['log']:\n",
    "    print(f\"  - {entry}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説006</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **エラーの捕捉と状態への記録:** `potentially_failing_node` 内で、ダミーの処理が失敗すると（ここでは`random.random()`でシミュレート）、`error_message` 状態キーにエラー情報が記録され、`retry_count` がインクリメントされます。成功した場合は `processed_result` が設定され、`error_message` はクリア（または `None` に）されます。\n",
    "*   **リトライ制御ルーター:** `should_retry_or_fail` ルーターが、`error_message` の有無と `retry_count` が `max_retries` に達しているかどうかに基づいて、次の遷移先を決定します。\n",
    "    *   エラーがあり、かつリトライ回数が上限未満なら `\"retry\"` を返し、再び `process_data` ノードを実行します（リトライ）。\n",
    "    *   エラーがあり、かつリトライ回数が上限に達していたら `\"fail\"` を返し、`handle_failure` ノードに進みます。\n",
    "    *   エラーがなければ `\"succeed\"` を返し、`handle_success` ノードに進みます。\n",
    "*   **ループによるリトライ:** `\"retry\"` の場合に `process_data` ノード自身に戻るエッジが、リトライのループを形成します。\n",
    "*   **`recursion_limit` の調整:** リトライ処理のようにループが発生する可能性があるグラフでは、`graph.invoke` や `graph.stream` の `config` で `recursion_limit` を十分に大きな値に設定する必要があります。デフォルトのままだと、多数回のリトライで再帰深度エラーが発生することがあります。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題007: 第2章のまとめ - 反復的な改善と最終承認フローの構築\n",
    "*   **学習内容:** 第2章で学んだ要素（LLMによる改善ループ、エラーハンドリング、人間による最終承認）を組み合わせ、実践的な多段階プロセスを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄007 - グラフ構築\n",
    "____ typing ____ ____, Optional, ____, List\n",
    "____ random\n",
    "____ uuid ____ uuid4\n",
    "____ langgraph.graph ____ ____, ____, ____\n",
    "____ langgraph.checkpoint.memory ____ ____ # MemorySaverが必要\n",
    "\n",
    "# 状態定義\n",
    "____ IterativeApprovalState(____):\n",
    "    original_request: str\n",
    "    current_draft: Optional[str]\n",
    "    llm_critique: Optional[str]\n",
    "    human_feedback: Optional[str]\n",
    "    iteration_count: int\n",
    "    max_iterations: int\n",
    "    error_details: Optional[str]\n",
    "    final_document: Optional[str]\n",
    "    log: List[str]\n",
    "\n",
    "# ノード定義\n",
    "____ initial_draft_node(state: IterativeApprovalState):\n",
    "    print(f\"\n",
    "[初期ドラフト生成] リクエスト: {state['original_request']}\")\n",
    "    current_iter = state.get(\"iteration_count\", 0) # 解答例より\n",
    "    draft = f\"「{state['original_request']}」に関するドラフトです。バージョン{current_iter + 1}。\"\n",
    "    ____ state.get(\"human_feedback\") and \"revise\" in state[\"human_feedback\"].lower(): # 解答例より\n",
    "        draft += f\" (フィードバック「{state['human_feedback']}」を反映しました)\"\n",
    "    ____ state.get(\"llm_critique\") and \"完璧\" not in state[\"llm_critique\"].lower(): # 解答例より\n",
    "        draft += f\" (LLMレビュー「{state['llm_critique'][:30]}...」を反映しました)\"\n",
    "\n",
    "    if random.random() < 0.1 and current_iter == 0: # 低確率で初回のみエラー (解答例より)\n",
    "        err = \"初期ドラフト生成中に予期せぬAPIエラーが発生しました。\"\n",
    "        print(f\"  -> エラー: {err}\")\n",
    "        return {\"error_details\": err, \"iteration_count\": current_iter + 1, \"log\": state[\"log\"] + [f\"ドラフト(v{current_iter+1})エラー: {err}\"]}\n",
    "    print(f\"  -> 生成ドラフト: {draft}\")\n",
    "    return {\"current_draft\": draft, \"iteration_count\": current_iter + 1, \"log\": state[\"log\"] + [f\"ドラフト生成(v{current_iter+1}): {draft}\"]\n",
    "            , \"error_details\": None, \"llm_critique\": None, \"human_feedback\": None} # 解答例よりリセット\n",
    "\n",
    "def llm_review_node(state: IterativeApprovalState):\n",
    "    if state.get(\"error_details\"): return {} # 前のノードでエラーなら何もしない\n",
    "    draft = state[\"current_draft\"]\n",
    "    print(f\"\n",
    "[LLMレビュー] ドラフト: {draft}\")\n",
    "    # ダミーLLM評価\n",
    "    if state[\"iteration_count\"] < state[\"max_iterations\"] / 2: # 初期は厳しめにレビュー (解答例より)\n",
    "        crit = f\"改善点あり: もっと詳細な情報が必要です (反復 {state['iteration_count']})。\"\n",
    "    elif state[\"iteration_count\"] < state[\"max_iterations\"]: # 解答例より\n",
    "        crit = f\"ほぼ良いですが、結論部分を強調してください (反復 {state['iteration_count']})。\"\n",
    "    else:\n",
    "        crit = \"完璧です。これ以上改善の必要はありません。\"\n",
    "    print(f\"  -> LLM評価: {crit}\")\n",
    "    return {\"llm_critique\": crit, \"log\": state[\"log\"] + [f\"LLMレビュー: {crit}\"]}\n",
    "\n",
    "def human_approval_gateway_node(state: IterativeApprovalState):\n",
    "    if state.get(\"error_details\"): return {} # エラー時は承認ステップをスキップ\n",
    "    print(f\"\n",
    "[人間による承認ゲートウェイ] ドラフト: {state['current_draft']}\")\n",
    "    print(f\"  LLMレビュー: {state.get('llm_critique', 'N/A')}\")\n",
    "    return {}\n",
    "\n",
    "def finalize_document_node(state: IterativeApprovalState):\n",
    "    final_doc = state[\"current_draft\"]\n",
    "    feedback = state.get(\"human_feedback\", \"(フィードバックなし)\")\n",
    "    msg = f\"[最終化] ドキュメント「{final_doc}」が人間によって承認されました。フィードバック: {feedback}\"\n",
    "    print(msg)\n",
    "    return {\"final_document\": final_doc, \"log\": state[\"log\"] + [msg]}\n",
    "\n",
    "def revision_or_rejection_node(state: IterativeApprovalState):\n",
    "    feedback = state.get(\"human_feedback\", \"\")\n",
    "    action = \"改訂\" if \"revise\" in feedback.lower() else \"最終拒否\" # 解答例より\n",
    "    msg = f\"[{action}処理] 人間のフィードバック: 「{feedback}」。\"\n",
    "    print(msg)\n",
    "    return {\"log\": state[\"log\"] + [msg]} # current_draftの更新はinitial_draft_nodeで行う (解答例より)\n",
    "\n",
    "def error_processing_node(state: IterativeApprovalState):\n",
    "    err_msg = f\"[エラー処理] エラー発生: {state['error_details']}\"\n",
    "    print(err_msg)\n",
    "    return {\"log\": state[\"log\"] + [err_msg], \"final_document\": \"エラーのため処理中断\"} # 解答例より\n",
    "\n",
    "# ルーター関数\n",
    "def route_after_llm_review(state: IterativeApprovalState):\n",
    "    if state.get(\"error_details\"): return \"error_handler\"\n",
    "    critique = state.get(\"llm_critique\", \"\")\n",
    "    if \"完璧\" in critique.lower() or state[\"iteration_count\"] >= state[\"max_iterations\"]:\n",
    "        print(\"  -> LLMレビューOKまたは最大反復。人間承認へ。\")\n",
    "        return \"human_approval\"\n",
    "    else:\n",
    "        print(\"  -> LLMレビューで改善点あり。再ドラフトへ。\")\n",
    "        return \"regenerate_draft\" # 再ドラフトのキー (解答例より)\n",
    "\n",
    "def route_after_human_feedback(state: IterativeApprovalState):\n",
    "    feedback = state.get(\"human_feedback\", \"\").lower()\n",
    "    if \"approve\" in feedback:\n",
    "        print(\"  -> 人間が承認。最終化へ。\")\n",
    "        return \"approved_by_human\"\n",
    "    elif \"revise\" in feedback and state[\"iteration_count\"] < state[\"max_iterations\"]: # 解答例より\n",
    "        print(\"  -> 人間が改訂を要求。revision_handler経由で再ドラフトへ。\")\n",
    "        return \"needs_revision\"\n",
    "    else: # reject or revise but max_iterations reached (解答例より)\n",
    "        print(\"  -> 人間が拒否、または改訂上限。revision_handlerで終了。\")\n",
    "        return \"rejected_by_human\"\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(IterativeApprovalState)\n",
    "workflow.add_node(\"draft\", initial_draft_node)\n",
    "workflow.add_node(\"llm_review\", llm_review_node)\n",
    "workflow.add_node(\"human_approval\", human_approval_gateway_node)\n",
    "workflow.add_node(\"finalize\", finalize_document_node)\n",
    "workflow.add_node(\"revision_handler\", revision_or_rejection_node) \n",
    "workflow.add_node(\"error_handler\", error_processing_node)\n",
    "\n",
    "workflow.set_entry_point(\"draft\")\n",
    "workflow.add_edge(\"draft\", \"llm_review\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"llm_review\", route_after_llm_review,\n",
    "    {\n",
    "        \"regenerate_draft\": \"draft\", \n",
    "        \"human_approval\": \"human_approval\",\n",
    "        \"error_handler\": \"error_handler\"\n",
    "    }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"human_approval\", route_after_human_feedback,\n",
    "    {\n",
    "        \"approved_by_human\": \"finalize\",\n",
    "        \"needs_revision\": \"revision_handler\", # 解答例より revision_handler を経由\n",
    "        \"rejected_by_human\": \"revision_handler\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"revision_handler\", \"draft\") # 改訂指示の場合、revision_handlerの後に再度draftへ (解答例より)\n",
    "workflow.add_edge(\"finalize\", END)\n",
    "workflow.add_edge(\"error_handler\", END)   # エラー処理後終了\n",
    "\n",
    "chapter2_summary_memory = MemorySaver() # 解答例より\n",
    "graph = workflow.compile(checkpointer=chapter2_summary_memory, interrupt_before=[\"human_approval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄007 - グラフ可視化\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄007 - グラフ実行\n",
    "request = \"LangGraphを使った新しいAIエージェントの提案書\"\n",
    "iterations = 2 # LLMによる自動反復の上限回数（人間による改訂要求はこの回数に含まれない）(解答例より)\n",
    "ch2_thread_id = str(uuid4())\n",
    "ch2_config = {\"configurable\": {\"thread_id\": ch2_thread_id}}\n",
    "initial_ch2_state = {\n",
    "    \"original_request\": request,\n",
    "    \"iteration_count\": 0,\n",
    "    \"max_iterations\": iterations,\n",
    "    \"log\": [],\n",
    "    \"current_draft\":None, \"llm_critique\":None, \"human_feedback\":None, \"error_details\":None, \"final_document\":None\n",
    "}\n",
    "print(f\"--- 第2章まとめテスト (リクエスト: {request}, LLM最大反復: {iterations}) ---\")\n",
    "\n",
    "current_graph_run = graph.invoke(initial_ch2_state, config=ch2_config) # 解答例の実行ロジックを適用\n",
    "\n",
    "if graph.get_state(ch2_config).next: # Check if graph is interrupted (解答例より)\n",
    "    current_ch2_state_values = graph.get_state(ch2_config).values\n",
    "    print(f\"\n",
    "--- 人間による確認 --- (現在のドラフト: {current_ch2_state_values.get('current_draft')})\")\n",
    "    human_input = input(\"ドラフトを承認しますか？ (approve / revise [指示] / reject): \").strip()\n",
    "    \n",
    "    current_graph_run = graph.invoke({\"human_feedback\": human_input}, config=ch2_config)\n",
    "    while \"revise\" in human_input.lower() and graph.get_state(ch2_config).next and current_graph_run.get(\"iteration_count\", 0) < current_graph_run.get(\"max_iterations\", iterations) +1: # 解答例のループ条件\n",
    "        print(\"人間による改訂要求後、再度LLMレビューと承認ゲートウェイに到達しました。\")\n",
    "        current_ch2_state_values = graph.get_state(ch2_config).values\n",
    "        print(f\"  改訂後ドラフト: {current_ch2_state_values.get('current_draft')}\")\n",
    "        print(f\"  LLMレビュー: {current_ch2_state_values.get('llm_critique')}\")\n",
    "        human_input = input(\"改訂版ドラフトを承認しますか？ (approve / revise [指示] / reject): \").strip()\n",
    "        current_graph_run = graph.invoke({\"human_feedback\": human_input}, config=ch2_config)\n",
    "        if \"approve\" in human_input.lower() or \"reject\" in human_input.lower():\n",
    "            break\n",
    "\n",
    "print(\"\n",
    "--- 最終結果 --- \")\n",
    "if current_graph_run.get(\"final_document\"):\n",
    "    print(f\"最終ドキュメント: {current_graph_run['final_document']}\")\n",
    "elif current_graph_run.get(\"error_details\"):\n",
    "    print(f\"エラー終了: {current_graph_run['error_details']}\")\n",
    "else:\n",
    "    print(f\"処理完了。最終ドラフト: {current_graph_run.get('current_draft')}\")\n",
    "    if current_graph_run.get(\"human_feedback\"):\n",
    "        print(f\"  最後の人間フィードバック: {current_graph_run.get('human_feedback')}\")\n",
    "\n",
    "print(\"\n",
    "全ログ:\")\n",
    "for entry in current_graph_run.get('log', []):\n",
    "    print(f\"  - {entry}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答007</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Optional, Annotated, List\n",
    "import random\n",
    "from uuid import uuid4\n",
    "from langgraph.graph import StateGraph, END, Interrupt\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 状態定義\n",
    "class IterativeApprovalState(TypedDict):\n",
    "    original_request: str\n",
    "    current_draft: Optional[str]\n",
    "    llm_critique: Optional[str]\n",
    "    human_feedback: Optional[str]\n",
    "    iteration_count: int\n",
    "    max_iterations: int\n",
    "    error_details: Optional[str]\n",
    "    final_document: Optional[str]\n",
    "    log: List[str]\n",
    "\n",
    "# ノード定義\n",
    "def initial_draft_node(state: IterativeApprovalState):\n",
    "    print(f\"\n",
    "[初期ドラフト生成] リクエスト: {state['original_request']}\")\n",
    "    current_iter = state.get(\"iteration_count\", 0)\n",
    "    draft = f\"「{state['original_request']}」に関するドラフトです。バージョン{current_iter + 1}。\"\n",
    "    # 前回のフィードバックがあればそれを反映する（ダミー処理）\n",
    "    if state.get(\"human_feedback\") and \"revise\" in state[\"human_feedback\"].lower():\n",
    "        draft += f\" (フィードバック「{state['human_feedback']}」を反映しました)\"\n",
    "    elif state.get(\"llm_critique\") and \"完璧\" not in state[\"llm_critique\"].lower():\n",
    "        draft += f\" (LLMレビュー「{state['llm_critique'][:30]}...」を反映しました)\"\n",
    "\n",
    "    if random.random() < 0.1 and current_iter == 0: # 低確率で初回のみエラー\n",
    "        err = \"初期ドラフト生成中に予期せぬAPIエラーが発生しました。\"\n",
    "        print(f\"  -> エラー: {err}\")\n",
    "        return {\"error_details\": err, \"iteration_count\": current_iter + 1, \"log\": state[\"log\"] + [f\"ドラフト(v{current_iter+1})エラー: {err}\"]}\n",
    "    print(f\"  -> 生成ドラフト: {draft}\")\n",
    "    return {\"current_draft\": draft, \"iteration_count\": current_iter + 1, \"log\": state[\"log\"] + [f\"ドラフト生成(v{current_iter+1}): {draft}\"]\n",
    "            , \"error_details\": None, \"llm_critique\": None, \"human_feedback\": None}\n",
    "\n",
    "def llm_review_node(state: IterativeApprovalState):\n",
    "    if state.get(\"error_details\"): return {} \n",
    "    draft = state[\"current_draft\"]\n",
    "    print(f\"\n",
    "[LLMレビュー] ドラフト: {draft}\")\n",
    "    if state[\"iteration_count\"] < state[\"max_iterations\"] / 2: # 初期は厳しめにレビュー\n",
    "        crit = f\"改善点あり: もっと詳細な情報が必要です (反復 {state['iteration_count']})。\"\n",
    "    elif state[\"iteration_count\"] < state[\"max_iterations\"]:\n",
    "        crit = f\"ほぼ良いですが、結論部分を強調してください (反復 {state['iteration_count']})。\"\n",
    "    else:\n",
    "        crit = \"完璧です。これ以上改善の必要はありません。\"\n",
    "    print(f\"  -> LLM評価: {crit}\")\n",
    "    return {\"llm_critique\": crit, \"log\": state[\"log\"] + [f\"LLMレビュー: {crit}\"]}\n",
    "\n",
    "def human_approval_gateway_node(state: IterativeApprovalState):\n",
    "    if state.get(\"error_details\"): return {} \n",
    "    print(f\"\n",
    "[人間による承認ゲートウェイ] ドラフト: {state['current_draft']}\")\n",
    "    print(f\"  LLMレビュー: {state.get('llm_critique', 'N/A')}\")\n",
    "    return {}\n",
    "\n",
    "def finalize_document_node(state: IterativeApprovalState):\n",
    "    final_doc = state[\"current_draft\"]\n",
    "    feedback = state.get(\"human_feedback\", \"(フィードバックなし)\")\n",
    "    msg = f\"[最終化] ドキュメント「{final_doc}」が人間によって承認されました。フィードバック: {feedback}\"\n",
    "    print(msg)\n",
    "    return {\"final_document\": final_doc, \"log\": state[\"log\"] + [msg]}\n",
    "\n",
    "def revision_or_rejection_node(state: IterativeApprovalState):\n",
    "    feedback = state.get(\"human_feedback\", \"\")\n",
    "    action = \"改訂\" if \"revise\" in feedback.lower() else \"最終拒否\"\n",
    "    msg = f\"[{action}処理] 人間のフィードバック: 「{feedback}」。\"\n",
    "    print(msg)\n",
    "    # 改訂の場合、iteration_countは次のループで増えるのでここでは何もしない\n",
    "    # current_draftは次のinitial_draft_nodeでフィードバックを元に更新される想定\n",
    "    return {\"log\": state[\"log\"] + [msg]}\n",
    "\n",
    "def error_processing_node(state: IterativeApprovalState):\n",
    "    err_msg = f\"[エラー処理] エラー発生: {state['error_details']}\"\n",
    "    print(err_msg)\n",
    "    return {\"log\": state[\"log\"] + [err_msg], \"final_document\": \"エラーのため処理中断\"}\n",
    "\n",
    "# ルーター関数\n",
    "def route_after_llm_review(state: IterativeApprovalState):\n",
    "    if state.get(\"error_details\"): return \"error_handler\"\n",
    "    critique = state.get(\"llm_critique\", \"\")\n",
    "    if \"完璧\" in critique.lower() or state[\"iteration_count\"] >= state[\"max_iterations\"]:\n",
    "        print(\"  -> LLMレビューOKまたは最大反復。人間承認へ。\")\n",
    "        return \"human_approval\"\n",
    "    else:\n",
    "        print(\"  -> LLMレビューで改善点あり。再ドラフトへ。\")\n",
    "        return \"regenerate_draft\"\n",
    "\n",
    "def route_after_human_feedback(state: IterativeApprovalState):\n",
    "    feedback = state.get(\"human_feedback\", \"\").lower()\n",
    "    if \"approve\" in feedback:\n",
    "        print(\"  -> 人間が承認。最終化へ。\")\n",
    "        return \"approved_by_human\"\n",
    "    elif \"revise\" in feedback and state[\"iteration_count\"] < state[\"max_iterations\"]:\n",
    "        print(\"  -> 人間が改訂を要求。revision_handler経由で再ドラフトへ。\")\n",
    "        return \"needs_revision\"\n",
    "    else: # reject or revise but max_iterations reached\n",
    "        print(\"  -> 人間が拒否、または改訂上限。revision_handlerで終了。\")\n",
    "        return \"rejected_by_human\"\n",
    "\n",
    "# グラフ構築\n",
    "workflow = StateGraph(IterativeApprovalState)\n",
    "workflow.add_node(\"draft\", initial_draft_node)\n",
    "workflow.add_node(\"llm_review\", llm_review_node)\n",
    "workflow.add_node(\"human_approval\", human_approval_gateway_node)\n",
    "workflow.add_node(\"finalize\", finalize_document_node)\n",
    "workflow.add_node(\"revision_handler\", revision_or_rejection_node) \n",
    "workflow.add_node(\"error_handler\", error_processing_node)\n",
    "\n",
    "workflow.set_entry_point(\"draft\")\n",
    "workflow.add_edge(\"draft\", \"llm_review\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"llm_review\", route_after_llm_review,\n",
    "    {\n",
    "        \"regenerate_draft\": \"draft\", \n",
    "        \"human_approval\": \"human_approval\",\n",
    "        \"error_handler\": \"error_handler\"\n",
    "    }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"human_approval\", route_after_human_feedback,\n",
    "    {\n",
    "        \"approved_by_human\": \"finalize\",\n",
    "        \"needs_revision\": \"revision_handler\", \n",
    "        \"rejected_by_human\": \"revision_handler\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"revision_handler\", \"draft\") # 改訂指示の場合、revision_handlerの後に再度draftへ\n",
    "workflow.add_edge(\"finalize\", END)\n",
    "workflow.add_edge(\"error_handler\", END)\n",
    "\n",
    "chapter2_summary_memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=chapter2_summary_memory, interrupt_before=[\"human_approval\"])\n",
    "\n",
    "# 実行\n",
    "request = \"LangGraphの高度な機能に関する技術ブログ記事の草案\"\n",
    "iterations = 2 # LLMによる自動反復の上限回数（人間による改訂要求はこの回数に含まれない）\n",
    "ch2_thread_id = str(uuid4())\n",
    "ch2_config = {\"configurable\": {\"thread_id\": ch2_thread_id}}\n",
    "initial_ch2_state = {\n",
    "    \"original_request\": request,\n",
    "    \"iteration_count\": 0,\n",
    "    \"max_iterations\": iterations,\n",
    "    \"log\": [],\n",
    "    \"current_draft\":None, \"llm_critique\":None, \"human_feedback\":None, \"error_details\":None, \"final_document\":None\n",
    "}\n",
    "print(f\"--- 第2章まとめテスト (リクエスト: {request}, LLM最大反復: {iterations}) ---\")\n",
    "\n",
    "current_graph_run = graph.invoke(initial_ch2_state, config=ch2_config)\n",
    "\n",
    "if graph.get_state(ch2_config).next: # Check if graph is interrupted\n",
    "    current_ch2_state_values = graph.get_state(ch2_config).values\n",
    "    print(f\"\n",
    "--- 人間による確認 --- (現在のドラフト: {current_ch2_state_values.get('current_draft')})\")\n",
    "    human_input = input(\"ドラフトを承認しますか？ (approve / revise [指示] / reject): \").strip()\n",
    "    \n",
    "    # 人間のフィードバックを注入して再開\n",
    "    current_graph_run = graph.invoke({\"human_feedback\": human_input}, config=ch2_config)\n",
    "    # 人間がreviseを選び、まだiteration_count < max_iterations の場合、再度中断する可能性がある\n",
    "    while \"revise\" in human_input.lower() and graph.get_state(ch2_config).next and current_graph_run.get(\"iteration_count\", 0) < current_graph_run.get(\"max_iterations\", iterations) +1: # +1 for human revise loop\n",
    "        print(\"人間による改訂要求後、再度LLMレビューと承認ゲートウェイに到達しました。\")\n",
    "        current_ch2_state_values = graph.get_state(ch2_config).values\n",
    "        print(f\"  改訂後ドラフト: {current_ch2_state_values.get('current_draft')}\")\n",
    "        print(f\"  LLMレビュー: {current_ch2_state_values.get('llm_critique')}\")\n",
    "        human_input = input(\"改訂版ドラフトを承認しますか？ (approve / revise [指示] / reject): \").strip()\n",
    "        current_graph_run = graph.invoke({\"human_feedback\": human_input}, config=ch2_config)\n",
    "        if \"approve\" in human_input.lower() or \"reject\" in human_input.lower():\n",
    "            break\n",
    "\n",
    "print(\"\n",
    "--- 最終結果 --- \")\n",
    "if current_graph_run.get(\"final_document\"):\n",
    "    print(f\"最終ドキュメント: {current_graph_run['final_document']}\")\n",
    "elif current_graph_run.get(\"error_details\"):\n",
    "    print(f\"エラー終了: {current_graph_run['error_details']}\")\n",
    "else:\n",
    "    print(f\"処理完了。最終ドラフト: {current_graph_run.get('current_draft')}\")\n",
    "    if current_graph_run.get(\"human_feedback\"):\n",
    "        print(f\"  最後の人間フィードバック: {current_graph_run.get('human_feedback')}\")\n",
    "\n",
    "print(\"\n",
    "全ログ:\")\n",
    "for entry in current_graph_run.get('log', []):\n",
    "    print(f\"  - {entry}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説007</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **複合的なフロー:** このグラフは、初期ドラフト生成、LLMによるレビュー、人間による承認という複数のステージを経ます。\n",
    "*   **LLM改善ループ:** `draft` -> `llm_review` -> (改善点あれば) `draft` というループが形成され、LLMが自己（または他のLLM）の生成物を反復的に改善しようとします。`max_iterations` でループ回数を制限しています。\n",
    "*   **エラーハンドリング:** `initial_draft_node` で発生しうるエラーを捕捉し、`error_processing_node` に処理を分岐させています。\n",
    "*   **人間による介入:** `human_approval_gateway_node` の手前で `Interrupt` が発生し、人間の入力を待ちます。入力 (`human_feedback`) に応じて、承認なら `finalize_document_node` へ、改訂要求なら `revision_or_rejection_node` を経由して再度 `draft` へ（改訂ループ）、拒否なら `revision_or_rejection_node` を経由して終了します。\n",
    "*   **状態遷移の複雑さ:** 複数の条件分岐とループが組み合わさることで、状態遷移のパスが複雑になります。このような場合にグラフの可視化が非常に役立ちます。\n",
    "*   **`MemorySaver` と `config`:** `Interrupt` を使用する際は `checkpointer` が必須です。ここでは `MemorySaver` を使用し、`invoke` 時に `config` でスレッドIDを指定することで、中断と再開の状態を正しく管理しています。\n",
    "\n",
    "この問題は第2章で学んだ多くの要素を組み合わせた実践的な例であり、LangGraphの柔軟性と表現力を示しています。\n",
    "\n",
    "---</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}