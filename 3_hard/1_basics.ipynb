{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1章: グラフの基本要素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "以下のセルを順番に実行して、演習に必要な環境をセットアップします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMプロバイダーの選択\n",
    "\n",
    "このセルでは、使用するLLMプロバイダーを選択します。\n",
    "`LLM_PROVIDER` 変数に、利用したいプロバイダー名を設定してください。\n",
    "選択可能なプロバイダー: `\"openai\"`, `\"azure\"`, `\"google\"` (Vertex AI), `\"google_genai\"` (Gemini API), `\"anthropic\"`, `\"bedrock\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMプロバイダーの選択 ===\n",
    "# 利用したいLLMプロバイダーを以下の変数で指定してください。\n",
    "# \"openai\", \"azure\", \"google\" (Vertex AI), \"google_genai\" (Gemini API), \"anthropic\", \"bedrock\" のいずれかを選択できます。\n",
    "LLM_PROVIDER = \"openai\"  # 例: OpenAI を利用する場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIキー/環境変数の設定\n",
    "\n",
    "以下のセルを実行する前に、選択したLLMプロバイダーに応じたAPIキーまたは環境変数を設定する必要があります。\n",
    "\n",
    "**手順:**\n",
    "1.  `.env.sample` ファイルをコピーして `.env` ファイルを作成します。\n",
    "2.  `.env` ファイルを開き、選択したLLMプロバイダーに対応するAPIキーや必要な情報を記述します。\n",
    "    *   **OpenAI:** `OPENAI_API_KEY`\n",
    "    *   **Azure OpenAI:** `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `OPENAI_API_VERSION`, `AZURE_OPENAI_DEPLOYMENT_NAME`\n",
    "    *   **Google (Vertex AI):** `GOOGLE_CLOUD_PROJECT_ID`, `GOOGLE_CLOUD_LOCATION` (Colab環境外で実行する場合、`GOOGLE_APPLICATION_CREDENTIALS` 環境変数の設定も必要になることがあります)\n",
    "    *   **Google (Gemini API):** `GOOGLE_API_KEY`\n",
    "    *   **Anthropic:** `ANTHROPIC_API_KEY`\n",
    "    *   **AWS Bedrock:** `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION_NAME` (IAMロールを使用する場合は、これらのキー設定は不要な場合がありますが、リージョン名は必須です)\n",
    "3.  ファイルを保存します。\n",
    "\n",
    "**Google Colab を使用している場合:**\n",
    "上記の `.env` ファイルを使用する代わりに、Colabのシークレットマネージャーに必要なキーを登録してください。\n",
    "例えば、OpenAIを使用する場合は `OPENAI_API_KEY` という名前でシークレットを登録します。\n",
    "Vertex AI を利用する場合は、Colab上での認証 (`google.colab.auth.authenticate_user()`) が実行されます。\n",
    "\n",
    "このセルは、設定された情報に基づいて環境変数をロードし、LLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === APIキー/環境変数の設定 ===\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envファイルから環境変数を読み込む (存在する場合)\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# --- OpenAI ---\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY and IS_COLAB:\n",
    "        OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"OpenAI APIキーが設定されていません。環境変数 OPENAI_API_KEY を設定するか、Colab環境の場合はシークレットに OPENAI_API_KEY を設定してください。\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# --- Azure OpenAI ---\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not AZURE_OPENAI_API_KEY: AZURE_OPENAI_API_KEY = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not AZURE_OPENAI_ENDPOINT: AZURE_OPENAI_ENDPOINT = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        if not OPENAI_API_VERSION: OPENAI_API_VERSION = userdata.get(\"OPENAI_API_VERSION\") # 例: \"2023-07-01-preview\"\n",
    "        if not AZURE_OPENAI_DEPLOYMENT_NAME: AZURE_OPENAI_DEPLOYMENT_NAME = userdata.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if not AZURE_OPENAI_API_KEY: raise ValueError(\"Azure OpenAI APIキー (AZURE_OPENAI_API_KEY) が設定されていません。\")\n",
    "    if not AZURE_OPENAI_ENDPOINT: raise ValueError(\"Azure OpenAI エンドポイント (AZURE_OPENAI_ENDPOINT) が設定されていません。\")\n",
    "    if not OPENAI_API_VERSION: OPENAI_API_VERSION = \"2023-07-01-preview\" # デフォルトを設定することも可能\n",
    "    if not AZURE_OPENAI_DEPLOYMENT_NAME: raise ValueError(\"Azure OpenAI デプロイメント名 (AZURE_OPENAI_DEPLOYMENT_NAME) が設定されていません。\")\n",
    "\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "    os.environ[\"OPENAI_API_VERSION\"] = OPENAI_API_VERSION\n",
    "\n",
    "# --- Google Cloud Vertex AI (Gemini) ---\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT_ID\") # .env 用に修正\n",
    "    LOCATION = os.environ.get(\"GOOGLE_CLOUD_LOCATION\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not PROJECT_ID: PROJECT_ID = userdata.get(\"GOOGLE_CLOUD_PROJECT_ID\")\n",
    "        if not LOCATION: LOCATION = userdata.get(\"GOOGLE_CLOUD_LOCATION\") # 例: \"us-central1\"\n",
    "        from google.colab import auth as google_auth\n",
    "        google_auth.authenticate_user() # Vertex AI を使う場合は Colab での認証を推奨\n",
    "    else: # Colab外の場合、.envから読み込んだ値で環境変数を設定\n",
    "        if PROJECT_ID: os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID # Vertex AI SDKが参照する標準的な環境変数名\n",
    "        if LOCATION: os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "\n",
    "    if not PROJECT_ID: raise ValueError(\"Google Cloud Project ID が設定されていません。環境変数 GOOGLE_CLOUD_PROJECT_ID を設定するか、Colab環境の場合はシークレットに GOOGLE_CLOUD_PROJECT_ID を設定してください。\")\n",
    "    if not LOCATION: LOCATION = \"us-central1\" # デフォルトロケーション\n",
    "\n",
    "# --- Google Gemini API (langchain-google-genai) ---\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY and IS_COLAB:\n",
    "        GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"Google APIキーが設定されていません。環境変数 GOOGLE_API_KEY を設定するか、Colab環境の場合はシークレットに GOOGLE_API_KEY を設定してください。\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# --- Anthropic (Claude) ---\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY and IS_COLAB:\n",
    "        ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        raise ValueError(\"Anthropic APIキーが設定されていません。環境変数 ANTHROPIC_API_KEY を設定するか、Colab環境の場合はシークレットに ANTHROPIC_API_KEY を設定してください。\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "# --- Amazon Bedrock (Claude) ---\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    AWS_REGION_NAME = os.environ.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if IS_COLAB: \n",
    "        if not AWS_ACCESS_KEY_ID: AWS_ACCESS_KEY_ID = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
    "        if not AWS_SECRET_ACCESS_KEY: AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        if not AWS_REGION_NAME: AWS_REGION_NAME = userdata.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if not AWS_REGION_NAME:\n",
    "         raise ValueError(\"AWSリージョン名 (AWS_REGION_NAME) が設定されていません。Bedrock利用にはリージョン指定が必要です。\")\n",
    "\n",
    "    # 環境変数に設定 (boto3がこれらを自動で読み込む)\n",
    "    if AWS_ACCESS_KEY_ID: os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "    if AWS_SECRET_ACCESS_KEY: os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION_NAME # boto3が参照する標準的なリージョン環境変数名\n",
    "    os.environ[\"AWS_REGION\"] = AWS_REGION_NAME # いくつかのライブラリはこちらを参照することもある\n",
    "\n",
    "print(f\"APIキー/環境変数の設定完了 (プロバイダー: {LLM_PROVIDER})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMクライアントの初期化\n",
    "\n",
    "このセルは、上で選択・設定したLLMプロバイダーに基づいて、対応するLLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMクライアントの動的初期化 ===\n",
    "llm = None\n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), # 環境変数から取得\n",
    "        openai_api_version=os.environ.get(\"OPENAI_API_VERSION\"), # 環境変数から取得\n",
    "        temperature=0,\n",
    "    )\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    from langchain_google_vertexai import ChatVertexAI\n",
    "    # PROJECT_ID, LOCATION は前のセルで環境変数に設定済みか、Colabの場合は直接利用\n",
    "    llm = ChatVertexAI(model_name=\"gemini-2.0-flash\", temperature=0, project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"), location=os.environ.get(\"GOOGLE_CLOUD_LOCATION\"))\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    from langchain_anthropic import ChatAnthropic\n",
    "    llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    from langchain_aws import ChatBedrock # langchain_community.chat_models から langchain_aws に変更の可能性あり\n",
    "    # AWS_REGION_NAME は前のセルで環境変数 AWS_DEFAULT_REGION に設定済み\n",
    "    llm = ChatBedrock( # BedrockChat ではなく ChatBedrock が一般的\n",
    "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        # region_name=os.environ.get(\"AWS_DEFAULT_REGION\"), # 通常、boto3が環境変数から自動で読み込む\n",
    "        model_kwargs={\"temperature\": 0},\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Unsupported LLM_PROVIDER: {LLM_PROVIDER}. \"\n",
    "        \"Please choose from 'openai', 'azure', 'google', 'google_genai', 'anthropic', or 'bedrock'.\"\n",
    "    )\n",
    "\n",
    "print(f\"LLM Provider: {LLM_PROVIDER}\")\n",
    "if llm:\n",
    "    print(f\"LLM Client Type: {type(llm)}\")\n",
    "    # モデル名取得の試行を汎用的に\n",
    "    model_attr = (\n",
    "                 getattr(llm, 'model', None) or\n",
    "                 getattr(llm, 'model_name', None) or\n",
    "                 getattr(llm, 'model_id', None) or\n",
    "                 (hasattr(llm, 'llm') and getattr(llm.llm, 'model', None)) # 一部のLLMクライアントのネスト構造に対応\n",
    "    )\n",
    "    if hasattr(llm, 'azure_deployment') and not model_attr: # Azure特有の属性\n",
    "        model_attr = llm.azure_deployment\n",
    "        \n",
    "    if model_attr:\n",
    "        print(f\"LLM Model: {model_attr}\")\n",
    "    else:\n",
    "        print(\"LLM Model: (Could not determine model name from client attributes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題001: 最小構成のLangGraphグラフの構築\n",
    "\n",
    "LangGraphの最も基本的な構成要素である`StateGraph`と`State`を理解し、シンプルなグラフを構築してみましょう。この問題では、入力された文字列をそのまま出力するだけの、単一のノードを持つグラフを作成します。\n",
    "\n",
    "*   **学習内容:** この問題では、`StateGraph`、`TypedDict`を用いた`State`の定義、`add_node`、`set_entry_point`、`add_edge`、`END`といったLangGraphの最も基本的なAPIを学びます。また、`Annotated`と`add_messages`を使ってメッセージ履歴を管理する方法も理解します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001\n",
    "\n",
    "from ____ import ____, ____\n",
    "from ____.____ import ____, ____\n",
    "from ____.____.____ import ____\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ____(____):\n",
    "    ____: ____[____, ____]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def ____(____: GraphState):\n",
    "    print(f'simple_node: {____[____][____].____}')\n",
    "    return {____: [____[____][____]]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "____ = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(____, simple_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(____)\n",
    "\n",
    "# 終了ポイントの設定\n",
    "workflow.add_edge(____, ____)\n",
    "\n",
    "# グラフのコンパイル\n",
    "____ = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "____ = {\"messages\": [(\"user\", \"Hello, LangGraph!\")]}\n",
    "\n",
    "# 最終結果の確認\n",
    "____ = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答001</summary>\n",
    "\n",
    "``````python\n",
    "# 解答001\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    # グラフの状態を保持する辞書\n",
    "    # ここでは、入力メッセージを保持する\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def simple_node(state: GraphState):\n",
    "    # 入力されたメッセージをそのまま返すノード\n",
    "    print(f'simple_node: {state[\"messages\"][-1].content}')\n",
    "    return {\"messages\": [state[\"messages\"][-1]]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"simple_node\", simple_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"simple_node\")\n",
    "\n",
    "# 終了ポイントの設定\n",
    "workflow.add_edge(\"simple_node\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs = {\"messages\": [(\"user\", \"Hello, LangGraph!\")]}\n",
    "\n",
    "# 最終結果の確認\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説001</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **コード解説:**\n",
    "    *   `GraphState`は、グラフ全体で共有される状態を定義します。`TypedDict`を使うことで、状態のスキーマを明確にできます。`messages: Annotated[list, add_messages]`は、LangChainのメッセージ形式のリストを状態として持ち、新しいメッセージが追加されるたびに自動的にリストの末尾に追加されるように設定しています。\n",
    "    *   `simple_node`関数は、グラフのノードとして機能します。`state`引数として現在のグラフの状態を受け取り、新しい状態を辞書として返します。ここでは、入力された最後のメッセージをそのまま返しています。\n",
    "    *   `StateGraph(GraphState)`でグラフのインスタンスを作成し、`GraphState`で定義した状態スキーマを渡します。\n",
    "    *   `workflow.add_node(\"simple_node\", simple_node)`で、`simple_node`関数を`simple_node`という名前のノードとしてグラフに追加します。\n",
    "    *   `workflow.set_entry_point(\"simple_node\")`は、グラフの実行が開始される最初のノードを指定します。\n",
    "    *   `workflow.add_edge(\"simple_node\", END)`は、`simple_node`の実行が完了したらグラフを終了することを示します。`END`はLangGraphが提供する特別な終了ノードです。\n",
    "    *   `graph = workflow.compile()`で、定義したワークフローを実行可能なアプリケーションにコンパイルします。\n",
    "    *   `graph.stream(inputs)`は、グラフの実行過程をストリーミングで受け取ることができます。`graph.invoke(inputs)`は、グラフの実行が完了した最終状態を返します。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題002: グラフの可視化\n",
    "\n",
    "問題001で構築した最小構成のグラフの構造を、視覚的に確認する方法を学びましょう。\n",
    "\n",
    "*   **学習内容:** `graph.get_graph().draw_png()` を使用して、コンパイル済みのLangGraphグラフ構造をPNG画像として描画し、Jupyter Notebook上に表示する方法を学びます。これにより、グラフのノードとエッジの接続関係を直感的に理解できるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002\n",
    "\n",
    "# 問題001のコードを再掲（このセルでグラフを定義・コンパイルします）\n",
    "from typing import ____, ____\n",
    "from langgraph.____ import ____, ____\n",
    "from langgraph.graph.____ import ____\n",
    "from langchain_core.____ import ____\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ____(____):\n",
    "    ____: ____[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def ____(state: GraphState):\n",
    "    # このノードは状態を更新せず、最後のメッセージをログに出力するだけ\n",
    "    print(f\"simple_node: Received message -> {____[____][____].____}\")\n",
    "    # LangGraphでは、ノードがNoneまたは空の辞書を返すと、状態は更新されない\n",
    "    ____\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "____ = StateGraph(GraphState)\n",
    "____.add_node(\"simple_node\", simple_node)\n",
    "____.set_entry_point(____)\n",
    "____.add_edge(____, ____)\n",
    "____ = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの可視化 ---\n",
    "from IPython.____ import ____, ____\n",
    "\n",
    "try:\n",
    "    ____ = graph.get_graph().draw_png()\n",
    "    ____(Image(png_data))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの動作確認 ---\n",
    "# 可視化したグラフが問題001と同様に動作することを確認します。\n",
    "try:\n",
    "    inputs = {\"messages\": [HumanMessage(content=\"Hello, this is a test.\")]}\n",
    "    final_state = graph.invoke(inputs)\n",
    "    print(\"\\nグラフの実行が完了しました。\")\n",
    "    print(f\"最終的なmessagesの状態: {final_state['messages']}\")\n",
    "except NameError:\n",
    "    print(\"グラフが定義されていません。前のセルを先に実行してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答002</summary>\n",
    "\n",
    "``````python\n",
    "# 解答002\n",
    "\n",
    "# 問題001のコードを再掲（このセルでグラフを定義・コンパイルします）\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def simple_node(state: GraphState):\n",
    "    # このノードは状態を更新せず、最後のメッセージをログに出力するだけ\n",
    "    print(f\"simple_node: Received message -> {state['messages'][-1].content}\")\n",
    "    # LangGraphでは、ノードがNoneまたは空の辞書を返すと、状態は更新されない\n",
    "    return\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"simple_node\", simple_node)\n",
    "workflow.set_entry_point(\"simple_node\")\n",
    "workflow.add_edge(\"simple_node\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    png_data = graph.get_graph().draw_png()\n",
    "    display(Image(png_data))\n",
    "    print(\"グラフが正常に可視化されました。\")\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの動作確認 ---\n",
    "# 可視化したグラフが問題001と同様に動作することを確認します。\n",
    "try:\n",
    "    inputs = {\"messages\": [HumanMessage(content=\"Hello, this is a test.\")]}\n",
    "    final_state = graph.invoke(inputs)\n",
    "    print(\"\\nグラフの実行が完了しました。\")\n",
    "    print(f\"最終的なmessagesの状態: {final_state['messages']}\")\n",
    "except NameError:\n",
    "    print(\"グラフが定義されていません。前のセルを先に実行してください。\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説002</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **コード解説:**\n",
    "    *   この問題では、まず問題001で作成したグラフ定義のコードを再利用して、`graph`オブジェクトを準備します。\n",
    "    *   グラフを可視化するための中心的なメソッドが `graph.get_graph().draw_png()` です。\n",
    "        *   `graph.get_graph()`: コンパイル済みの`graph`オブジェクトから、可視化や解析が可能な内部グラフ表現を取得します。\n",
    "        *   `.draw_png()`: 取得したグラフ表現をPNG形式の画像データ（バイト列）として描画します。\n",
    "    *   `IPython.display.Image` は、画像データをJupyter NotebookなどのIPython環境で表示可能なオブジェクトに変換します。\n",
    "    *   `IPython.display.display()` 関数を使って、`Image`オブジェクトをセルに表示します。\n",
    "    *   **重要:** `.draw_png()` メソッドを使用するには、**Graphviz**というグラフ可視化ソフトウェアがシステムにインストールされている必要があります。また、Pythonライブラリの`pygraphviz`も必要です（これらは準備セクションでインストール済みです）。もし`ExecutableNotFound`のようなエラーが出る場合は、Graphviz本体がOSに正しくインストールされ、パスが通っているかを確認してください。\n",
    "\n",
    "*   **なぜ可視化が重要か:**\n",
    "    *   グラフが単純なうちはコードを読むだけで構造を理解できますが、ノードやエッジが増え、特に条件分岐やループが絡んでくると、全体の流れを把握するのが難しくなります。\n",
    "    *   グラフを可視化することで、設計した通りの構造になっているかを一目で確認でき、意図しない接続やループのデバッグに非常に役立ちます。\n",
    "\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題003: 複数のノードを持つシーケンシャルグラフの構築\n",
    "\n",
    "前の問題で学んだ基本的なグラフ構築に加えて、複数のノードを直列に接続し、データがノード間をどのように流れるかを理解しましょう。ここでは、入力された文字列を加工する2つのノード（例：大文字化、逆順化）を持つグラフを作成します。\n",
    "\n",
    "*   **学習内容:** 複数のノードを`add_edge`で直列に接続する方法と、ノード間で状態がどのように引き継がれるかを学びます。`HumanMessage`と`AIMessage`を使って、メッセージの送信元を明示する方法も理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003\n",
    "\n",
    "from ____ import ____, ____\n",
    "from ____.____ import ____, ____\n",
    "from ____.____.____ import ____\n",
    "from ____.____.____ import ____, ____\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ____(____):\n",
    "    ____: ____[____, ____]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def ____(state: GraphState):\n",
    "    last_message_content = ____[____][____].____\n",
    "    print(f\"uppercase_node: {last_message_content}\")\n",
    "    return {____: [____(content=last_message_content.upper())]}\n",
    "\n",
    "def ____(state: GraphState):\n",
    "    last_message_content = ____[____][____].____\n",
    "    print(f\"reverse_node: {last_message_content}\")\n",
    "    return {____: [____(content=last_message_content[::-1])]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "____ = ____(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "____.____(\"uppercase\", uppercase_node)\n",
    "____.____(\"reverse\", reverse_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "____.____(\"uppercase\")\n",
    "\n",
    "# エッジの追加 (直列接続)\n",
    "____.____(\"uppercase\", \"reverse\")\n",
    "____.____(\"reverse\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "____ = ____.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの実行と結果表示 ---\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Hello LangGraph\")]}\n",
    "\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答003</summary>\n",
    "\n",
    "``````python\n",
    "# 解答003\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def uppercase_node(state: GraphState):\n",
    "    # 最新のメッセージを大文字に変換するノード\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"uppercase_node: {last_message_content}\")\n",
    "    return {\"messages\": [AIMessage(content=last_message_content.upper())]}\n",
    "\n",
    "def reverse_node(state: GraphState):\n",
    "    # 最新のメッセージを逆順にするノード\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"reverse_node: {last_message_content}\")\n",
    "    return {\"messages\": [AIMessage(content=last_message_content[::-1])]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"uppercase\", uppercase_node)\n",
    "workflow.add_node(\"reverse\", reverse_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"uppercase\")\n",
    "\n",
    "# エッジの追加 (直列接続)\n",
    "workflow.add_edge(\"uppercase\", \"reverse\")\n",
    "workflow.add_edge(\"reverse\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Hello LangGraph\")]}\n",
    "\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説003</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **コード解説:**\n",
    "    *   `uppercase_node`と`reverse_node`は、それぞれ入力メッセージを大文字化、逆順化する処理を行います。重要なのは、各ノードが新しい`AIMessage`を作成して状態に返す点です。これにより、次のノードは前のノードの処理結果を`state[\"messages\"][-1]`で取得できます。\n",
    "    *   `workflow.add_edge(\"uppercase\", \"reverse\")`は、`uppercase`ノードの実行が完了したら、次に`reverse`ノードを実行するように指示します。このようにして、処理の流れを定義します。\n",
    "    *   入力メッセージを`HumanMessage`として渡すことで、ユーザーからの入力であることを明示しています。ノードからの出力は`AIMessage`として返され、メッセージ履歴にAIの応答として記録されます。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題004: グラフ内でのLLMの利用（シンプルなチャットボット）\n",
    "\n",
    "LangGraphのノード内で大規模言語モデル（LLM）を呼び出す方法を学び、シンプルなチャットボットを構築しましょう。ここでは、ユーザーからの入力に対してLLMが応答を生成し、その応答を返すグラフを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004\n",
    "from typing import ____, ____\n",
    "from langgraph.____ import ____, ____\n",
    "from langgraph.____.____ import ____\n",
    "from langchain_core.____ import HumanMessage, AIMessage\n",
    "import ____\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "# (from langchain_openai import ChatOpenAI や llm = ChatOpenAI(...) といった行はここには不要)\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ____(TypedDict):\n",
    "    ____: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def ____(state: GraphState):\n",
    "    # LLMを呼び出し、応答を生成するノード\n",
    "    # ノートブック冒頭で初期化された共通の `llm` 変数を使用します。\n",
    "    print(f\"llm_node: Calling LLM with messages: {____[____]}\")\n",
    "    ____ = llm.invoke(state[\"messages\"]) \n",
    "    print(f\"llm_node: LLM response: {____.____}\")\n",
    "    return {____: [response]} # responseはAIMessageオブジェクトを期待 (ここは歯抜けにしない)\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "____ = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "____.____(\"llm_responder\", llm_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "____.____(\"llm_responder\")\n",
    "\n",
    "# 終了ポイントの設定\n",
    "____.____(\"llm_responder\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "____ = ____.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- チャットボットのテスト ---\")\n",
    "# 最初のメッセージはHumanMessageであると想定\n",
    "inputs = {\"messages\": [HumanMessage(content=\"こんにちは、あなたの名前は何ですか？\")]}\n",
    "\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n--- 別の質問 ---\")\n",
    "inputs2 = {\"messages\": [HumanMessage(content=\"今日の天気は？\")]}\n",
    "\n",
    "final_state2 = graph.invoke(inputs2)\n",
    "print(f\"最終的な応答: {final_state2['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答004</summary>\n",
    "\n",
    "``````python\n",
    "# 解答004\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import os # osはAPIキー設定のコメントアウト部分で使われているので残しても良いが、直接は不要になる\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def llm_node(state: GraphState):\n",
    "    # LLMを呼び出し、応答を生成するノード\n",
    "    # ノートブック冒頭で初期化された共通の `llm` 変数を使用します。\n",
    "    print(f\"llm_node: Calling LLM with messages: {state['messages']}\")\n",
    "    response = llm.invoke(state[\"messages\"]) # 共通llmを使用\n",
    "    print(f\"llm_node: LLM response: {response.content}\")\n",
    "    return {\"messages\": [response]} # responseはAIMessageオブジェクトを期待\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"llm_responder\", llm_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"llm_responder\")\n",
    "\n",
    "# 終了ポイントの設定\n",
    "workflow.add_edge(\"llm_responder\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- チャットボットのテスト ---\")\n",
    "# 最初のメッセージはHumanMessageであると想定\n",
    "inputs = {\"messages\": [HumanMessage(content=\"こんにちは、あなたの名前は何ですか？\")]}\n",
    "\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n--- 別の質問 ---\")\n",
    "inputs2 = {\"messages\": [HumanMessage(content=\"今日の天気は？\")]}\n",
    "\n",
    "final_state2 = graph.invoke(inputs2)\n",
    "print(f\"最終的な応答: {final_state2['messages'][-1].content}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説004</summary>\n",
    "\n",
    "このノートブックでは、様々なLLMプラットフォーム（OpenAI, Azure OpenAI, Google Cloud Vertex AI, Google Gemini (Gemini API), Anthropic Claude, Amazon Bedrockなど）を簡単に切り替えて試せるように設計されています。\n",
    "ノートブックの冒頭にある `LLM_PROVIDER` 変数で使用したいLLMを選択し、対応するAPIキーや環境変数を設定するだけで、この問題を含む全てのLLM呼び出し箇所で選択したLLMが利用されます。\n",
    "選択した `LLM_PROVIDER` に応じて、必要なAPIキーが設定されているか（環境変数またはGoogle Colabのシークレット経由）、ノートブック起動時にチェックされます。\n",
    "\n",
    "ここでは、ノートブックの先頭で設定・初期化された共通の `llm` 変数を使用して、LLMに質問をしています。\n",
    "`llm.invoke()` という統一されたインターフェースで、どのLLMプロバイダーを利用しているかに関わらず、同じようにLLMを呼び出すことができます。\n",
    "これにより、特定のLLMサービスに依存しない、より汎用的なコードを作成するメリットを手軽に体験できます。\n",
    "\n",
    "もしエラーが発生した場合は、ノートブック冒頭の `LLM_PROVIDER` の設定、および選択したプロバイダーに応じたAPIキーや環境変数の設定（例: `OPENAI_API_KEY`, `GOOGLE_API_KEY`, `AZURE_OPENAI_ENDPOINT`など）が正しく行われているかを確認してください。\n",
    "各プロバイダー固有の設定項目（例えばVertex AIのProject ID、AzureのDeployment Name、Bedrockのリージョンなど）も見直してください。\n",
    "プロバイダーによっては、`pip install` コマンドで対応するライブラリ (例: `langchain-google-genai`) がインストールされているかも確認点です。\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題005: 状態の更新 - 特定キーの値を上書きする\n",
    "\n",
    "`add_messages` によるメッセージ履歴の追加だけでなく、グラフの状態(`State`)内の特定のキーの値を直接更新する方法を学びましょう。ここでは、カウンター値を保持する状態キーを定義し、ノードでその値をインクリメントするグラフを作成します。\n",
    "\n",
    "*   **学習内容:** `TypedDict`で定義する状態クラスに、`messages`以外のカスタムキー（ここでは`counter: int`）を追加し、ノード関数内でその値を直接読み書きする方法を学びます。これにより、メッセージ履歴だけでなく、数値や文字列、ブール値など、より多様なデータをグラフ全体で管理できるようになります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005\n",
    "\n",
    "from ____ import ____, ____\n",
    "from ____.____ import ____, ____\n",
    "from ____.____.____ import ____\n",
    "from ____.____.____ import ____\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ____(____):\n",
    "    ____: ____[____, ____]\n",
    "    ____: ____ \n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def ____(____: CounterState):\n",
    "    # counterの値を1増やすノード\n",
    "    ____ = ____.get(\"counter\", 0) \n",
    "    ____ = current_count + 1\n",
    "    print(f\"increment_counter: Current count: {current_count}, New count: {new_count}\")\n",
    "    return {____: new_count, ____: [HumanMessage(content=f\"Counter incremented to {new_count}\")]}\n",
    "\n",
    "def ____(state: CounterState):\n",
    "    # counterの最終値を表示するノード (実際にはmessagesに追加されたもので確認)\n",
    "    print(f\"display_count: Final counter value is {state['counter']}\")\n",
    "    # このノードは状態を更新しないが、メッセージを追加しても良い\n",
    "    return {\"messages\": [HumanMessage(content=f\"Final count: {state['counter']}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "____ = ____(CounterState)\n",
    "\n",
    "# ノードの追加\n",
    "____.____(\"increment\", increment_counter)\n",
    "____.____(\"display\", display_count)\n",
    "\n",
    "# エントリポイントの設定\n",
    "____.____(\"increment\")\n",
    "\n",
    "# エッジの追加\n",
    "____.____(\"increment\", \"display\")\n",
    "____.____(\"display\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "____ = ____.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- カウンターテスト (初期値0から) ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Start counting\")], \"counter\": 0} # 初期カウンター値を設定\n",
    "\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n--- カウンターテスト (初期値5から) ---\")\n",
    "inputs_2 = {\"messages\": [HumanMessage(content=\"Start counting from 5\")], \"counter\": 5} # 初期カウンター値を設定\n",
    "\n",
    "final_state_2 = graph.invoke(inputs_2)\n",
    "print(f\"最終的な応答: {final_state_2['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答005</summary>\n",
    "\n",
    "``````python\n",
    "# 解答005\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class CounterState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    counter: int # 新しくカウンター用の状態キーを定義\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def increment_counter(state: CounterState):\n",
    "    # counterの値を1増やすノード\n",
    "    current_count = state.get(\"counter\", 0) # stateからcounterの値を取得、なければ0\n",
    "    new_count = current_count + 1\n",
    "    print(f\"increment_counter: Current count: {current_count}, New count: {new_count}\")\n",
    "    return {\"counter\": new_count, \"messages\": [HumanMessage(content=f\"Counter incremented to {new_count}\")]}\n",
    "\n",
    "def display_count(state: CounterState):\n",
    "    # counterの最終値を表示するノード (実際にはmessagesに追加されたもので確認)\n",
    "    print(f\"display_count: Final counter value is {state['counter']}\")\n",
    "    # このノードは状態を更新しないが、メッセージを追加しても良い\n",
    "    return {\"messages\": [HumanMessage(content=f\"Final count: {state['counter']}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(CounterState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"increment\", increment_counter)\n",
    "workflow.add_node(\"display\", display_count)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"increment\")\n",
    "\n",
    "# エッジの追加\n",
    "workflow.add_edge(\"increment\", \"display\")\n",
    "workflow.add_edge(\"display\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- カウンターテスト (初期値0から) ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Start counting\")], \"counter\": 0} # 初期カウンター値を設定\n",
    "\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n--- カウンターテスト (初期値5から) ---\")\n",
    "inputs_2 = {\"messages\": [HumanMessage(content=\"Start counting from 5\")], \"counter\": 5} # 初期カウンター値を設定\n",
    "\n",
    "final_state_2 = graph.invoke(inputs_2)\n",
    "print(f\"最終的な応答: {final_state_2['messages'][-1].content}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説005</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **コード解説:**\n",
    "    *   `CounterState`に`counter: int`を追加しました。これにより、グラフの状態はメッセージリストと整数型のカウンターを持つことになります。\n",
    "    *   `increment_counter`ノードでは、`state.get(\"counter\", 0)`を使って現在のカウンター値を取得しています。`.get()`メソッドを使うことで、キーが存在しない場合のデフォルト値を指定できます（ここでは初回実行時を想定して0）。その後、値をインクリメントし、更新後の値を`{\"counter\": new_count}`という辞書形式で返しています。LangGraphは、ノードが返す辞書のキーに基づいて対応する状態を更新します。\n",
    "    *   `messages`キーも同時に返すことで、状態更新のログや情報をメッセージ履歴に残すことができます。\n",
    "    *   グラフ実行時 (`graph.invoke`や`graph.stream`) に、`inputs`辞書に`\"counter\": 0`（または任意の値）を含めることで、`counter`キーの初期値を設定できます。\n",
    "    *   このように、ノードは状態の一部または全部を更新する辞書を返すことで、グラフの状態を変化させます。`add_messages`はメッセージリストの更新に特化した便利な方法ですが、他のキーは直接値を指定して更新します。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題006: 状態の更新 - 複数のキーを一度に更新する\n",
    "\n",
    "一つのノードから、状態(`State`)の複数のキーを同時に更新する方法を学びましょう。ここでは、ユーザーからのメッセージ内容に応じて、応答メッセージと共に「応答タイプ」という別の状態キーも更新するグラフを作成します。\n",
    "\n",
    "*   **学習内容:** 一つのノード関数から返す辞書に複数のキーと値のペアを含めることで、グラフの状態(`State`)の複数の属性を一度に更新する方法を学びます。また、`typing.Literal`を使って、状態キーが取りうる値を制限する方法も示します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄006\n",
    "from ____ import ____, ____, ____\n",
    "from ____.____ import ____, ____\n",
    "from ____.____.____ import ____\n",
    "from ____.____.____ import ____, ____\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "____ = Literal[\"greeting\", \"question\", \"other\", \"unknown\"]\n",
    "\n",
    "class ____(____):\n",
    "    ____: Annotated[list, add_messages]\n",
    "    ____: ResponseType\n",
    "    ____: str \n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def ____(state: MultiUpdateState):\n",
    "    user_message = ____[____][____].content.lower()\n",
    "    response_text = \"\"\n",
    "    resp_type: ResponseType = \"unknown\"\n",
    "\n",
    "    if \"こんにちは\" in user_message or \"こんばんは\" in user_message:\n",
    "        response_text = \"こんにちは！何かお手伝いできますか？\"\n",
    "        resp_type = \"greeting\" \n",
    "    elif \"?\" in user_message or \"教えて\" in user_message:\n",
    "        response_text = \"ご質問ありがとうございます。それについては現在調べています。\"\n",
    "        resp_type = \"question\" \n",
    "    else:\n",
    "        response_text = \"メッセージありがとうございます。\"\n",
    "        resp_type = \"other\" \n",
    "    \n",
    "    print(f\"process_input: User: '{user_message}', AI: '{response_text}', Type: '{resp_type}'\")\n",
    "    # 複数のキーを同時に更新して返す\n",
    "    return { \n",
    "        \"messages\": [____(content=response_text)],\n",
    "        \"response_type\": ____,\n",
    "        \"last_user_message\": ____\n",
    "    }\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "____ = ____(MultiUpdateState)\n",
    "\n",
    "____.____(\"processor\", process_input)\n",
    "____.____(\"processor\")\n",
    "____.____(\"processor\", END)\n",
    "\n",
    "____ = ____.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの実行と結果表示 ---\n",
    "test_inputs = [\n",
    "    {\"messages\": [HumanMessage(content=\"こんにちは\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"LangGraphについて教えてください\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"今日の天気は晴れですね\")]}\n",
    "]\n",
    "\n",
    "for i, inputs in enumerate(test_inputs):\n",
    "    print(f\"\\n--- テスト実行 {i+1} ---\")\n",
    "\n",
    "    final_state = graph.invoke(inputs, {\"recursion_limit\": 3})\n",
    "    print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "    assert \"response_type\" in final_state\n",
    "    assert \"last_user_message\" in final_state\n",
    "    print(f\"Response Type: {final_state['response_type']}\")\n",
    "    print(f\"Last User Message: {final_state['last_user_message']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答006</summary>\n",
    "\n",
    "``````python\n",
    "# 解答006\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "ResponseType = Literal[\"greeting\", \"question\", \"other\", \"unknown\"]\n",
    "\n",
    "class MultiUpdateState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    response_type: ResponseType # 応答の種類を保持するキー\n",
    "    last_user_message: str # 最後に入力されたユーザーメッセージ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def process_input(state: MultiUpdateState):\n",
    "    user_message = state[\"messages\"][-1].content.lower()\n",
    "    response_text = \"\"\n",
    "    resp_type: ResponseType = \"unknown\"\n",
    "\n",
    "    if \"こんにちは\" in user_message or \"こんばんは\" in user_message:\n",
    "        response_text = \"こんにちは！何かお手伝いできますか？\"\n",
    "        resp_type = \"greeting\"\n",
    "    elif \"?\" in user_message or \"教えて\" in user_message:\n",
    "        response_text = \"ご質問ありがとうございます。それについては現在調べています。\"\n",
    "        resp_type = \"question\"\n",
    "    else:\n",
    "        response_text = \"メッセージありがとうございます。\"\n",
    "        resp_type = \"other\"\n",
    "    \n",
    "    print(f\"process_input: User: '{user_message}', AI: '{response_text}', Type: '{resp_type}'\")\n",
    "    # 複数のキーを同時に更新して返す\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response_text)],\n",
    "        \"response_type\": resp_type,\n",
    "        \"last_user_message\": user_message # 元のユーザーメッセージを保存\n",
    "    }\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(MultiUpdateState)\n",
    "\n",
    "workflow.add_node(\"processor\", process_input)\n",
    "workflow.set_entry_point(\"processor\")\n",
    "workflow.add_edge(\"processor\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "test_inputs = [\n",
    "    {\"messages\": [HumanMessage(content=\"こんにちは\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"LangGraphについて教えてください\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"今日の天気は晴れですね\")]}\n",
    "]\n",
    "\n",
    "for i, inputs in enumerate(test_inputs):\n",
    "    print(f\"\\n--- テスト実行 {i+1} ---\")\n",
    "    # 初期状態としてresponse_typeやlast_user_messageを渡すことも可能だが、\n",
    "    # この問題ではノード内でこれらが設定されることを確認する\n",
    "    initial_state = inputs.copy()\n",
    "    # 必要であれば、初期値を設定\n",
    "    # initial_state.setdefault(\"response_type\", \"unknown\") \n",
    "    # initial_state.setdefault(\"last_user_message\", \"\")\n",
    "\n",
    "    final_state = graph.invoke(initial_state, {\"recursion_limit\": 3})\n",
    "    print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "    assert \"response_type\" in final_state\n",
    "    assert \"last_user_message\" in final_state\n",
    "    print(f\"Response Type: {final_state['response_type']}\")\n",
    "    print(f\"Last User Message: {final_state['last_user_message']}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説006</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **コード解説:**\n",
    "    *   `MultiUpdateState`に、`response_type: ResponseType` と `last_user_message: str` という2つの新しいキーを追加しました。`ResponseType`は`Literal[\"greeting\", \"question\", \"other\", \"unknown\"]`で定義され、`response_type`キーがこれらの文字列のうちのいずれかの値を取ることを示します（型ヒントであり、実行時の厳密な強制ではありませんが、開発時の可読性や静的解析に役立ちます）。\n",
    "    *   `process_input`ノードは、ユーザーのメッセージ内容に基づいて応答メッセージを生成し、それと同時に`response_type`（挨拶、質問、その他など）と`last_user_message`（処理対象となった元のユーザーメッセージ）も決定します。\n",
    "    *   ノードが返す辞書は `{\"messages\": ..., \"response_type\": ..., \"last_user_message\": ...}` のようになります。LangGraphは、この辞書に含まれる各キーに対応する状態を更新します。\n",
    "    *   実行時には、`messages`キーだけでなく、`response_type`と`last_user_message`も最終状態に含まれていることを確認できます。\n",
    "    *   このように、ノードはグラフの状態を柔軟に更新する役割を担います。返す辞書に含めるキーと値によって、どの状態属性をどのように変更するかを制御できます。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題007: `END` 以外の終了ノードの指定（概念理解）\n",
    "\n",
    "LangGraphでは、グラフの終点は通常、特別な `END` ノードに接続することで示されます。しかし、概念的には、あるノードが処理の最終ステップであり、それ以上後続のノードが存在しない場合、そのノードが事実上の「終了ノード」として機能すると考えることもできます。この問題では、特定のノードを実行した後、明示的に `END` に接続せず、グラフがそこで停止することを確認します。ただし、LangGraphのベストプラクティスとしては、可能な限り `END` を使用することが推奨されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄007\n",
    "from ____ import ____, ____\n",
    "from ____.____ import ____, ____ \n",
    "from ____.____.____ import ____\n",
    "from ____.____.____ import ____, ____\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ____(____):\n",
    "    ____: ____[____, ____]\n",
    "    ____: str\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def ____(state: FinalNodeState):\n",
    "    print(\"start_process: Process started.\")\n",
    "    return {____: \"Processing\", ____: [AIMessage(content=\"Process initiated.\")]}\n",
    "\n",
    "def ____(state: FinalNodeState):\n",
    "    # このノードが処理の最後とする\n",
    "    ____ = \"Process completed at final_processing_node.\"\n",
    "    print(f\"final_processing_node: {final_message}\")\n",
    "    return {____: \"Completed\", ____: [AIMessage(content=final_message)]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "____ = ____(FinalNodeState)\n",
    "\n",
    "____.____(\"start\", start_process)\n",
    "____.____(\"final_step\", final_processing_node)\n",
    "\n",
    "____.____(\"start\")\n",
    "\n",
    "# startノードからfinal_stepノードへのエッジ\n",
    "____.____(\"start\", \"final_step\")\n",
    "\n",
    "# final_stepノードからENDへのエッジを意図的に作成しない\n",
    "# ____.____(\"final_step\", END) \n",
    "\n",
    "# グラフのコンパイル\n",
    "# check_interruptions=True をつけると、ENDに到達しない場合にエラーになるため、\n",
    "# この例では明示的にENDに繋がないことを示すために compile() の引数なし、\n",
    "# または check_interruptions=False (デフォルト) を利用します。\n",
    "# Langfuseなどのトレーシングツールと連携する場合、ENDに到達しないとトレースが終了しないことがあるため注意。\n",
    "____ = ____.____()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- 最終ノードテスト ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Begin process\")], \"status\": \"Initial\"}\n",
    "final_state_from_stream = None\n",
    "\n",
    "print(\"Streaming execution:\")\n",
    "for chunk in graph.stream(inputs, {\"recursion_limit\": 5}):\n",
    "    print(f\"  Stream chunk: {chunk}\")\n",
    "    final_state_from_stream = chunk # Capture the last chunk which contains the state of the last executed node\n",
    "\n",
    "print(f\"Final State from stream: {final_state_from_stream}\")\n",
    "\n",
    "# invokeの挙動確認\n",
    "invoked_state = None\n",
    "try:\n",
    "    invoked_state = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"最終的な応答 (invoke): {invoked_state['messages'][-1].content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Invoke call resulted in an error or unexpected behavior: {e}\")\n",
    "    print(\"This might be expected if the graph doesn't explicitly reach END.\")\n",
    "\n",
    "assert final_state_from_stream is not None, \"Final state was not captured from stream\"\n",
    "final_node_key = list(final_state_from_stream.keys())[0] # Get the key of the last node's state\n",
    "assert final_state_from_stream[final_node_key][\"status\"] == \"Completed\"\n",
    "assert \"Process completed at final_processing_node.\" in final_state_from_stream[final_node_key][\"messages\"][-1].content\n",
    "print(\"Assertion for final_state_from_stream passed.\")\n",
    "\n",
    "if invoked_state:\n",
    "    print(f\"Invoked state status: {invoked_state.get('status')}\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "    print(\"Graph visualized. Note that 'final_step' does not point to END.\")\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答007</summary>\n",
    "\n",
    "``````python\n",
    "# 解答007\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class FinalNodeState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    status: str\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def start_process(state: FinalNodeState):\n",
    "    print(\"start_process: Process started.\")\n",
    "    return {\"status\": \"Processing\", \"messages\": [AIMessage(content=\"Process initiated.\")]}\n",
    "\n",
    "def final_processing_node(state: FinalNodeState):\n",
    "    # このノードが処理の最後とする\n",
    "    final_message = \"Process completed at final_processing_node.\"\n",
    "    print(f\"final_processing_node: {final_message}\")\n",
    "    return {\"status\": \"Completed\", \"messages\": [AIMessage(content=final_message)]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(FinalNodeState)\n",
    "\n",
    "workflow.add_node(\"start\", start_process)\n",
    "workflow.add_node(\"final_step\", final_processing_node)\n",
    "\n",
    "workflow.set_entry_point(\"start\")\n",
    "\n",
    "# startノードからfinal_stepノードへのエッジ\n",
    "workflow.add_edge(\"start\", \"final_step\")\n",
    "\n",
    "# final_stepノードからENDへのエッジを意図的に作成しない\n",
    "# workflow.add_edge(\"final_step\", END) # ← コメントアウトまたは削除\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- 最終ノードテスト ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Begin process\")], \"status\": \"Initial\"}\n",
    "final_state_from_stream = None\n",
    "\n",
    "print(\"Streaming execution:\")\n",
    "for chunk in graph.stream(inputs, {\"recursion_limit\": 5}):\n",
    "    print(f\"  Stream chunk: {chunk}\")\n",
    "    final_state_from_stream = chunk\n",
    "\n",
    "print(f\"Final State from stream: {final_state_from_stream}\")\n",
    "\n",
    "invoked_state = None\n",
    "try:\n",
    "    invoked_state = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"最終的な応答 (invoke): {invoked_state['messages'][-1].content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Invoke call resulted in an error or unexpected behavior: {e}\")\n",
    "    print(\"This might be expected if the graph doesn't explicitly reach END.\")\n",
    "\n",
    "assert final_state_from_stream is not None, \"Final state was not captured from stream\"\n",
    "final_node_key = list(final_state_from_stream.keys())[0]\n",
    "assert final_state_from_stream[final_node_key][\"status\"] == \"Completed\"\n",
    "assert \"Process completed at final_processing_node.\" in final_state_from_stream[final_node_key][\"messages\"][-1].content\n",
    "print(\"Assertion for final_state_from_stream passed.\")\n",
    "\n",
    "if invoked_state:\n",
    "    print(f\"Invoked state status: {invoked_state.get('status')}\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "    print(\"Graph visualized. Note that 'final_step' does not point to END.\")\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説007</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** グラフの終点として必ずしも `END` を明示的に指定する必要はなく、あるノードから先に遷移するエッジがなければ、そのノードの処理が終わった時点でグラフの実行が停止し、その時点での状態が最終状態となることを理解します。ただし、これはLangGraphの挙動の一つであり、デバッグや可視化、他のツールとの連携（例: Langfuse）を考慮すると、可能な限りグラフの終点を `END` に接続することが推奨されます。\n",
    "*   **コード解説:**\n",
    "    *   `final_processing_node`を作成し、このノードから `END` へのエッジ（`workflow.add_edge(\"final_step\", END)`）を定義していません。\n",
    "    *   `graph.stream()` を使ってグラフを実行すると、`final_processing_node` が実行された後、それ以上進むべきノードがないため、処理が停止します。`stream()` の最後の出力（この場合は `final_processing_node` の出力）が、その実行における最終的な状態を示します。\n",
    "    *   `graph.invoke()` の場合、グラフが明示的に `END` に到達しないと、バージョンや設定によってはエラーが発生したり、予期しない挙動をしたりする可能性があります。一般的に `invoke()` はグラフが `END` に到達し、完全な最終状態が確定することを期待します。この問題では、主に `stream()` での挙動を確認し、`invoke()` は参考として示しています。\n",
    "    *   可視化すると、`final_step` ノードから `END` (または他のノード) への矢印がないことが確認できます。\n",
    "*   **重要な注意点:**\n",
    "    *   **`END` の使用推奨:** LangGraphでは、グラフの論理的な終了点を明確にするために `END` を使用することが強く推奨されます。これにより、グラフの構造が理解しやすくなり、デバッグも容易になります。また、LangSmith/Langfuseのようなトレースツールは、`END` への到達をもって一連の処理の完了とみなすことが多いため、連携時にも重要です。\n",
    "    *   **`compile(check_interruptions=True)`:** グラフをコンパイルする際に `check_interruptions=True` を指定すると、中断（Interrupt）が発生しない限り、グラフが必ず `END` に到達することを強制できます。`END` に到達しないパスがある場合、コンパイル時または実行時にエラーが発生します。\n",
    "    *   この問題は、`END` を使わない場合の挙動を理解するためのものであり、実際の開発では `END` を適切に配置する設計を心がけてください。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題008: LLMノードと非LLMノードの連携強化\n",
    "\n",
    "LLM（大規模言語モデル）を組み込んだノードと、LLM以外の処理を行うノード（例: 文字列操作、データ抽出など）を連携させる方法を学びましょう。ここでは、LLMに質問を投げて得られた応答（文字列）から、特定の情報を抽出・加工して状態を更新する、より実践的なグラフを作成します。\n",
    "\n",
    "*   **学習内容:** LLMを呼び出すノードと、その出力を処理する非LLMノードを組み合わせることで、より高度な情報処理パイプラインを構築する方法を学びます。具体的には、LLMの自然言語応答から正規表現などを用いて構造化された情報を抽出し、グラフの状態を更新します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄008\n",
    "from ____ import ____, ____\n",
    "import ____ \n",
    "from ____.____ import ____, ____\n",
    "from ____.____.____ import ____\n",
    "from ____.____.____ import ____, ____\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ____(____):\n",
    "    ____: ____[____, ____]\n",
    "    ____: str \n",
    "    ____: str \n",
    "    ____: str | ____ \n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def ____(state: ExtractionState):\n",
    "    # ユーザーの質問を状態に保存\n",
    "    ____ = state[\"messages\"][-1].content\n",
    "    print(f\"get_user_question: Received question: '{last_message_content}'\")\n",
    "    return {____: last_message_content}\n",
    "\n",
    "def ____(state: ExtractionState):\n",
    "    # LLMに質問を投げるノード\n",
    "    ____ = state[\"user_question\"]\n",
    "    print(f\"llm_responder_node: Asking LLM: '{question}'\")\n",
    "    # LLMに渡すメッセージは、過去の履歴全体でも、最新の質問だけでも良い\n",
    "    # ここでは簡単のため、最新の質問のみをHumanMessageとして渡す\n",
    "    ____ = ____.invoke([HumanMessage(content=question)]) \n",
    "    ____ = response.content\n",
    "    print(f\"llm_responder_node: LLM raw response: '{response_content}'\")\n",
    "    return {____: [response], ____: response_content}\n",
    "\n",
    "def ____(state: ExtractionState):\n",
    "    # LLMの応答から情報を抽出するノード\n",
    "    ____ = state[\"llm_response_text\"]\n",
    "    # 例: LLMが「日本の首都は東京です。」と答えたら「東京」を抽出\n",
    "    # ここでは簡単な正規表現で「XXはYYです」のYY部分を抽出試行\n",
    "    ____ = None\n",
    "    ____ = re.search(r\"(?:は|is)\\s*([^。.]+)[.。]?\", raw_response) \n",
    "    if match:\n",
    "        ____ = match.group(1).strip()\n",
    "    \n",
    "    print(f\"data_extractor_node: Raw response: '{raw_response}', Extracted: '{extracted}'\")\n",
    "    return {____: extracted, ____: [AIMessage(content=f\"Extracted: {extracted}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "____ = ____(ExtractionState)\n",
    "\n",
    "____.____(\"capture_question\", get_user_question)\n",
    "____.____(\"ask_llm\", llm_responder_node)\n",
    "____.____(\"extract_data\", data_extractor_node)\n",
    "\n",
    "____.____(\"capture_question\")\n",
    "\n",
    "____.____(\"capture_question\", \"ask_llm\")\n",
    "____.____(\"ask_llm\", \"extract_data\")\n",
    "____.____(\"extract_data\", END)\n",
    "\n",
    "____ = ____.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの実行と結果表示 ---\n",
    "questions = [\n",
    "    \"日本の首都は何ですか？\",\n",
    "    \"フランスの有名な画家の名前を一人教えてください。\",\n",
    "    \"今日の天気は？\" # これは抽出パターンにマッチしない可能性\n",
    "]\n",
    "\n",
    "for q_text in questions:\n",
    "    print(f\"\\n--- LLM連携と情報抽出テスト (質問: {q_text}) ---\")\n",
    "    inputs = {\"messages\": [HumanMessage(content=q_text)]}\n",
    "    # final_state = None # This was problematic, invoke directly\n",
    "\n",
    "    # if final_state: # This block was never reached\n",
    "    #     print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "    #     print(f\"  User Question: {final_state.get('user_question')}\")\n",
    "    #     print(f\"  LLM Response: {final_state.get('llm_response_text')}\")\n",
    "    #     print(f\"  Extracted Info: {final_state.get('extracted_info')}\")\n",
    "    # else:\n",
    "    final_state_invoked = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"最終的な応答: {final_state_invoked['messages'][-1].content}\")\n",
    "    print(f\"  User Question: {final_state_invoked.get('user_question')}\")\n",
    "    print(f\"  LLM Response: {final_state_invoked.get('llm_response_text')}\")\n",
    "    print(f\"  Extracted Info: {final_state_invoked.get('extracted_info')}\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答008</summary>\n",
    "\n",
    "``````python\n",
    "# 解答008\n",
    "from typing import TypedDict, Annotated\n",
    "import re # 正規表現モジュールをインポート\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ExtractionState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_question: str # ユーザーの元の質問\n",
    "    llm_response_text: str # LLMの生の応答テキスト\n",
    "    extracted_info: str | None # LLMの応答から抽出された情報\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def get_user_question(state: ExtractionState):\n",
    "    # ユーザーの質問を状態に保存\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"get_user_question: Received question: '{last_message_content}'\")\n",
    "    return {\"user_question\": last_message_content}\n",
    "\n",
    "def llm_responder_node(state: ExtractionState):\n",
    "    # LLMに質問を投げるノード\n",
    "    question = state[\"user_question\"]\n",
    "    print(f\"llm_responder_node: Asking LLM: '{question}'\")\n",
    "    response = llm.invoke([HumanMessage(content=question)])\n",
    "    response_content = response.content\n",
    "    print(f\"llm_responder_node: LLM raw response: '{response_content}'\")\n",
    "    return {\"messages\": [response], \"llm_response_text\": response_content}\n",
    "\n",
    "def data_extractor_node(state: ExtractionState):\n",
    "    # LLMの応答から情報を抽出するノード\n",
    "    raw_response = state[\"llm_response_text\"]\n",
    "    extracted = None\n",
    "    # 改善された正規表現: 「XXはYYです」「XX is YY」のようなパターンや、単に「YYです」のような応答にも対応試行\n",
    "    # 質問が「日本の首都は何ですか？」で応答が「東京です。」の場合「東京」を抽出\n",
    "    # 質問が「日本の首都は？」で応答が「東京」の場合「東京」を抽出\n",
    "    patterns = [\n",
    "        r\"(?:.+は|.+\\s*is)\\s*(.+?)(?:です|。|\\.|\\s*for|$)\", #「～はXです」「～ is X」\n",
    "        r\"^([^。.]+?)(?:です|。|\\.|\\s*for|$)\" # 文頭から「Xです」\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, raw_response)\n",
    "        if match:\n",
    "            extracted = match.group(1).strip()\n",
    "            if extracted.lower() == state[\"user_question\"].lower().replace(\"何ですか\",\"\").replace(\"何\",\"зиру\").strip(\"?？\") : # 質問自体が答えになるような場合を除外\n",
    "                 extracted = None # 例：「天気は？」->「晴れです」はOKだが、「天気は？」->「天気」はNG\n",
    "                 continue\n",
    "            break\n",
    "    \n",
    "    # もし上記で抽出できなかった場合、LLMの応答が単語やフレーズそのものである可能性を考慮\n",
    "    if not extracted and len(raw_response.split()) < 5 and not state[\"user_question\"].startswith(raw_response): # 短い応答で、質問の繰り返しでない場合\n",
    "        extracted = raw_response.strip()\n",
    "\n",
    "    print(f\"data_extractor_node: Raw response: '{raw_response}', Extracted: '{extracted}'\")\n",
    "    return {\"extracted_info\": extracted, \"messages\": [AIMessage(content=f\"Extracted info: {extracted if extracted else 'N/A'}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(ExtractionState)\n",
    "\n",
    "workflow.add_node(\"capture_question\", get_user_question)\n",
    "workflow.add_node(\"ask_llm\", llm_responder_node)\n",
    "workflow.add_node(\"extract_data\", data_extractor_node)\n",
    "\n",
    "workflow.set_entry_point(\"capture_question\")\n",
    "\n",
    "workflow.add_edge(\"capture_question\", \"ask_llm\")\n",
    "workflow.add_edge(\"ask_llm\", \"extract_data\")\n",
    "workflow.add_edge(\"extract_data\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "questions = [\n",
    "    \"日本の首都は何ですか？\",\n",
    "    \"フランスの有名な画家の名前を一人教えてください。\", # LLMの回答次第で抽出成功/失敗が変わる\n",
    "    \"1+1は何？\", # LLMが「2です」と答えれば抽出できるかも\n",
    "    \"今日の天気は？\" # 「晴れです」なら「晴れ」を抽出期待\n",
    "]\n",
    "\n",
    "for q_text in questions:\n",
    "    print(f\"\\n--- LLM連携と情報抽出テスト (質問: {q_text}) ---\")\n",
    "    inputs = {\"messages\": [HumanMessage(content=q_text)], \"llm_response_text\": \"\", \"extracted_info\": None} # 初期値を設定\n",
    "    \n",
    "    final_state_data = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "\n",
    "    if final_state_data:\n",
    "        print(f\"最終的な応答: {final_state_data['messages'][-1].content}\")\n",
    "        print(f\"  User Question: {final_state_data.get('user_question')}\")\n",
    "        print(f\"  LLM Response: {final_state_data.get('llm_response_text')}\")\n",
    "        print(f\"  Extracted Info: {final_state_data.get('extracted_info')}\")\n",
    "    else:\n",
    "        print(\"Could not retrieve final state.\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説008</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **コード解説:**\n",
    "    *   `ExtractionState`には、ユーザーの質問 (`user_question`)、LLMの生の応答テキスト (`llm_response_text`)、そして抽出された情報 (`extracted_info`) を保持するためのキーが定義されています。\n",
    "    *   `get_user_question`ノード: ユーザーからの最初のメッセージを `user_question` として状態に保存します。\n",
    "    *   `llm_responder_node`: 保存された `user_question` を使ってLLMに問い合わせを行い、得られた応答を `messages` (AIMessageとして) と `llm_response_text` (生の文字列として) 状態に保存します。\n",
    "    *   `data_extractor_node`: `llm_response_text` から情報を抽出します。この例では、簡単な正規表現 `re.search(r\"(?:は|is)\\s*([^。.]+)[.。]?\", raw_response)` を使用して、「AはBです」や「A is B」といった形式の文からBの部分を抽出しようと試みています。抽出結果は `extracted_info` として状態に保存されます。正規表現は完璧ではなく、LLMの応答形式によってはうまく抽出できない場合もありますが、ここではLLMの出力後処理の一例として示しています。\n",
    "    *   グラフは `capture_question` -> `ask_llm` -> `extract_data` -> `END` というシーケンシャルな流れで定義されています。\n",
    "    *   実行時には、異なる質問を投げ、LLMの応答とそこから抽出された情報（または抽出できなかった場合は `None` や `N/A`）が最終状態に含まれることを確認します。\n",
    "*   **発展:**\n",
    "    *   情報抽出の方法は正規表現に限らず、より高度なNLPライブラリ（例: spaCy）や、LangChainが提供するOutput Parsers、あるいは別のLLMコール（Function Calling/Tool Calling対応モデルならより高精度）を使って行うことも考えられます。\n",
    "    *   抽出に失敗した場合のフォールバック処理（例: ユーザーに再確認を求める、デフォルト値を設定するなど）をグラフに追加することもできます。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題009: グラフの入力と出力のカスタマイズと明確化\n",
    "\n",
    "LangGraphのグラフを実行する際、`invoke()` や `stream()` メソッドに渡す初期状態の構造と、グラフ全体の最終的な出力状態の構造を意識することが重要です。`StateGraph` に渡す状態クラス（例: `TypedDict`）の定義が、実質的にグラフの入力と出力のスキーマ（型定義）となります。この問題では、入力として複数の情報を受け取り、それらを処理して特定の構造で出力するグラフを作成し、入出力の対応関係を明確に意識します。\n",
    "\n",
    "*   **学習内容:** `TypedDict` を使ってグラフの状態スキーマを定義する際、どのキーがグラフへの主要な「入力」として期待され、どのキーがグラフからの主要な「出力」として扱われるのかを明確に意識することの重要性を学びます。また、`Optional`型やデフォルト値の扱い方についても触れます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄009\n",
    "from ____ import ____, ____, ____, ____\n",
    "from ____.____ import ____, ____\n",
    "from ____.____.____ import ____\n",
    "from ____.____.____ import ____, ____\n",
    "\n",
    "# --- 状態定義 (入力と出力のスキーマを兼ねる) ---\n",
    "class ____(____):\n",
    "    ____: str\n",
    "    ____: str\n",
    "    ____: bool\n",
    "\n",
    "class ____(____):\n",
    "    # 入力として期待されるキー\n",
    "    ____: str\n",
    "    ____: List[str]\n",
    "    ____: Optional[int]\n",
    "\n",
    "    # 処理中に使われるキー\n",
    "    ____: Annotated[list, add_messages]\n",
    "    ____: int\n",
    "\n",
    "    # 出力として期待される主要なキー\n",
    "    ____: Optional[ProcessedData] \n",
    "    ____: Optional[str] \n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def ____(state: ComplexIOState):\n",
    "    print(f\"initialize_processing: Received item '{state['raw_item_name']}' with details {state['raw_item_details']}\")\n",
    "    return { \n",
    "        \"messages\": [AIMessage(content=f\"Initializing processing for {state['raw_item_name']}\")],\n",
    "        \"internal_counter\": 0,\n",
    "        \"processed_data\": None, \n",
    "        \"error_message\": None   \n",
    "    }\n",
    "\n",
    "def ____(state: ComplexIOState):\n",
    "    name = state[____]\n",
    "    details_count = len(state[____])\n",
    "    counter = state[____] + 1\n",
    "    threshold = state.get(____, 0)\n",
    "    \n",
    "    log_msg = f\"Processing '{name}', detail count: {details_count}, attempt: {counter}, threshold: {threshold}\"\n",
    "    print(f\"main_processor: {log_msg}\")\n",
    "\n",
    "    if details_count == 0:\n",
    "        err_msg = \"No details provided.\"\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {err_msg}\")],\n",
    "            \"error_message\": err_msg,\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "    \n",
    "    if counter > threshold: \n",
    "        processed_item = ProcessedData(\n",
    "            item_id=f\"PROC_{name.upper()}_{counter}\",\n",
    "            description=f\"Successfully processed {name} with {details_count} details after {counter} attempts.\",\n",
    "            is_processed=True\n",
    "        )\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Successfully processed {name}\")],\n",
    "            \"processed_data\": processed_item,\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Attempt {counter} for {name} did not meet threshold.\")],\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def ____(state: ComplexIOState):\n",
    "    if state.get(____):\n",
    "        print(\"check_status: Error detected, routing to END.\")\n",
    "        return \"__end__\" \n",
    "    if state.get(____) and state[____][____]:\n",
    "        print(\"check_status: Successfully processed, routing to END.\")\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        print(\"check_status: Not yet processed or error, routing back to main_processor.\")\n",
    "        return \"retry_processing\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "____ = ____(ComplexIOState)\n",
    "\n",
    "____.____(\"initializer\", initialize_processing)\n",
    "____.____(\"processor\", main_processor)\n",
    "\n",
    "____.____(\"initializer\")\n",
    "____.____(\"initializer\", \"processor\")\n",
    "\n",
    "____.____(\n",
    "    \"processor\",\n",
    "    check_status,\n",
    "    {\n",
    "        \"__end__\": END,\n",
    "        \"retry_processing\": \"processor\"\n",
    "    }\n",
    ")\n",
    "\n",
    "____ = ____.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの実行と結果表示 ---\n",
    "inputs_success = {\n",
    "    \"raw_item_name\": \"TestItem1\",\n",
    "    \"raw_item_details\": [\"detail A\", \"detail B\"],\n",
    "    \"processing_threshold\": 2 \n",
    "}\n",
    "\n",
    "inputs_fail_no_details = {\n",
    "    \"raw_item_name\": \"TestItem2\",\n",
    "    \"raw_item_details\": [], \n",
    "    \"processing_threshold\": 1\n",
    "}\n",
    "\n",
    "inputs_optional_threshold_not_provided = {\n",
    "    \"raw_item_name\": \"TestItem3\",\n",
    "    \"raw_item_details\": [\"detail C\"]\n",
    "    # processing_threshold is not provided (Optional)\n",
    "}\n",
    "\n",
    "test_cases = {\n",
    "    \"Success Case\": inputs_success, \n",
    "    \"Failure Case (No Details)\": inputs_fail_no_details,\n",
    "    \"Success Case (Threshold Not Provided)\": inputs_optional_threshold_not_provided\n",
    "}\n",
    "\n",
    "for case_name, inputs_data in test_cases.items():\n",
    "    print(f\"\\n--- I/Oカスタマイズテスト: {case_name} ---\")\n",
    "    current_inputs = inputs_data.copy()\n",
    "    current_inputs.setdefault(\"messages\", []) \n",
    "\n",
    "    final_output_state = graph.invoke(current_inputs, {\"recursion_limit\": 10})\n",
    "    print(f\"最終的な応答: {final_output_state['messages'][-1].content}\")\n",
    "\n",
    "    if final_output_state.get(\"processed_data\"):\n",
    "        print(f\"  Processed Item ID: {final_output_state['processed_data']['item_id']}\")\n",
    "        print(f\"  Processed: {final_output_state['processed_data']['is_processed']}\")\n",
    "    if final_output_state.get(\"error_message\"):\n",
    "        print(f\"  Error: {final_output_state['error_message']}\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答009</summary>\n",
    "\n",
    "``````python\n",
    "# 解答009\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (入力と出力のスキーマを兼ねる) ---\n",
    "class ProcessedData(TypedDict):\n",
    "    item_id: str\n",
    "    description: str\n",
    "    is_processed: bool\n",
    "\n",
    "class ComplexIOState(TypedDict):\n",
    "    # 入力として期待されるキー\n",
    "    raw_item_name: str\n",
    "    raw_item_details: List[str]\n",
    "    processing_threshold: Optional[int]\n",
    "\n",
    "    # 処理中に使われるキー\n",
    "    messages: Annotated[list, add_messages] # 処理ログ用\n",
    "    internal_counter: int\n",
    "\n",
    "    # 出力として期待される主要なキー\n",
    "    processed_data: Optional[ProcessedData] # 処理結果\n",
    "    error_message: Optional[str] # エラー発生時のメッセージ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def initialize_processing(state: ComplexIOState):\n",
    "    print(f\"initialize_processing: Received item '{state['raw_item_name']}' with details {state['raw_item_details']}\")\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Initializing processing for {state['raw_item_name']}\")],\n",
    "        \"internal_counter\": 0,\n",
    "        \"processed_data\": None, \n",
    "        \"error_message\": None   \n",
    "    }\n",
    "\n",
    "def main_processor(state: ComplexIOState):\n",
    "    name = state[\"raw_item_name\"]\n",
    "    details_count = len(state[\"raw_item_details\"])\n",
    "    counter = state[\"internal_counter\"] + 1\n",
    "    threshold = state.get(\"processing_threshold\") if state.get(\"processing_threshold\") is not None else 0\n",
    "    \n",
    "    log_msg = f\"Processing '{name}', detail count: {details_count}, attempt: {counter}, threshold: {threshold}\"\n",
    "    print(f\"main_processor: {log_msg}\")\n",
    "\n",
    "    if details_count == 0:\n",
    "        err_msg = \"No details provided.\"\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {err_msg}\")],\n",
    "            \"error_message\": err_msg,\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "    \n",
    "    if counter > threshold:\n",
    "        processed_item = ProcessedData(\n",
    "            item_id=f\"PROC_{name.upper()}_{counter}\",\n",
    "            description=f\"Successfully processed {name} with {details_count} details after {counter} attempts.\",\n",
    "            is_processed=True\n",
    "        )\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Successfully processed {name}\")],\n",
    "            \"processed_data\": processed_item,\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Attempt {counter} for {name} did not meet threshold.\")],\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def check_status(state: ComplexIOState):\n",
    "    if state.get(\"error_message\"):\n",
    "        print(\"check_status: Error detected, routing to END.\")\n",
    "        return \"__end__\" \n",
    "    if state.get(\"processed_data\") and state[\"processed_data\"][\"is_processed\"]:\n",
    "        print(\"check_status: Successfully processed, routing to END.\")\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        print(\"check_status: Not yet processed or error, routing back to main_processor.\")\n",
    "        return \"retry_processing\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(ComplexIOState)\n",
    "\n",
    "workflow.add_node(\"initializer\", initialize_processing)\n",
    "workflow.add_node(\"processor\", main_processor)\n",
    "\n",
    "workflow.set_entry_point(\"initializer\")\n",
    "workflow.add_edge(\"initializer\", \"processor\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"processor\",\n",
    "    check_status,\n",
    "    {\n",
    "        \"__end__\": END,\n",
    "        \"retry_processing\": \"processor\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs_success = {\n",
    "    \"raw_item_name\": \"TestItem1\",\n",
    "    \"raw_item_details\": [\"detail A\", \"detail B\"],\n",
    "    \"processing_threshold\": 2 \n",
    "}\n",
    "\n",
    "inputs_fail_no_details = {\n",
    "    \"raw_item_name\": \"TestItem2\",\n",
    "    \"raw_item_details\": [], \n",
    "    \"processing_threshold\": 1\n",
    "}\n",
    "\n",
    "inputs_optional_threshold_not_provided = {\n",
    "    \"raw_item_name\": \"TestItem3\",\n",
    "    \"raw_item_details\": [\"detail C\"],\n",
    "}\n",
    "\n",
    "test_cases = {\n",
    "    \"Success Case\": inputs_success,\n",
    "    \"Failure Case (No Details)\": inputs_fail_no_details,\n",
    "    \"Success Case (Threshold Not Provided)\": inputs_optional_threshold_not_provided\n",
    "}\n",
    "\n",
    "for case_name, inputs_data in test_cases.items():\n",
    "    print(f\"\\n--- I/Oカスタマイズテスト: {case_name} ---\")\n",
    "    current_inputs = inputs_data.copy()\n",
    "    current_inputs.setdefault(\"messages\", []) \n",
    "\n",
    "    final_output_state = graph.invoke(current_inputs, {\"recursion_limit\": 10})\n",
    "    print(f\"最終的な応答: {final_output_state['messages'][-1].content}\")\n",
    "\n",
    "    if final_output_state.get(\"processed_data\"):\n",
    "        print(f\"  Processed Item ID: {final_output_state['processed_data']['item_id']}\")\n",
    "        print(f\"  Processed: {final_output_state['processed_data']['is_processed']}\")\n",
    "    if final_output_state.get(\"error_message\"):\n",
    "        print(f\"  Error: {final_output_state['error_message']}\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説009</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **コード解説:**\n",
    "    *   `ComplexIOState` (状態スキーマ):\n",
    "        *   **入力想定キー:** `raw_item_name`, `raw_item_details`, `processing_threshold` (これは `Optional` なので、入力時に省略可能)。これらはグラフ実行時に `invoke` や `stream` の `inputs` 引数で渡されることが期待されます。\n",
    "        *   **処理中キー:** `messages`, `internal_counter`。これらは主にグラフ内部の処理やログのために使われ、通常は入力時に指定しません（`messages` は `add_messages` のために空リストで初期化することがあります）。\n",
    "        *   **出力想定キー:** `processed_data` (処理成功時の結果), `error_message` (エラー発生時の情報)。これらのキーの値が、グラフ実行後の最終的な成果物となります。\n",
    "    *   `initialize_processing`ノード: 入力値を受け取り、処理に必要な内部状態（`internal_counter`など）や出力用キー（`processed_data`, `error_message`）を初期化します。\n",
    "    *   `main_processor`ノード: 主要な処理ロジックを担当します。入力された `raw_item_name` や `raw_item_details`、そして `processing_threshold` (入力されなければデフォルト値を使用) に基づいて処理を行い、成功すれば `processed_data` を、失敗すれば `error_message` を更新します。また、処理試行回数を `internal_counter` で管理します。\n",
    "    *   `check_status`ルーター関数: `error_message` があれば終了。`processed_data` があれば終了。それ以外（まだ処理が完了していない、またはエラーではないが成功もしていない）場合は `main_processor` に戻って処理を続行（リトライ/ループ）します。\n",
    "    *   グラフ実行時:\n",
    "        *   `inputs`辞書には、`ComplexIOState`で定義した入力想定キー（`raw_item_name`など）を指定します。\n",
    "        *   `invoke()` から返される `final_output_state` は、`ComplexIOState` と同じ構造の辞書です。この中から出力想定キー（`processed_data` や `error_message`）の値を確認することで、グラフの実行結果を得ます。\n",
    "*   **重要な点:**\n",
    "    *   状態スキーマ（`TypedDict`）は、グラフのインターフェース（入力と出力の形式）を定義する上で中心的な役割を果たします。\n",
    "    *   入力時にどのキーが必要で、どのキーがオプショナルか、そしてグラフ実行後にどのキーに出力結果が格納されるのかを明確に設計することが、再利用可能で理解しやすいグラフを作る上で重要です。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題010: 複数のLLM呼び出しを含むグラフ\n",
    "\n",
    "一つのグラフ内で、異なる役割やプロンプトを持つ複数のLLM呼び出しノードを組み込む方法を学びましょう。例えば、最初のLLMがアイデアを生成し、次のLLMがそのアイデアを評価・洗練する、といった連携が考えられます。この問題では、簡単な役割分担を持つ2つのLLMノードを直列に接続します。\n",
    "\n",
    "*   **学習内容:** 一つのグラフ内に、それぞれ異なるプロンプトや役割を持つ複数のLLM呼び出しノードを配置し、それらを連携させる方法を学びます。これにより、より複雑で多段階の思考や処理を行うエージェントやパイプラインを構築できます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄010\n",
    "from ____ import ____, ____, ____\n",
    "from ____.____ import ____, ____\n",
    "from ____.____.____ import ____\n",
    "from ____.____.____ import ____, ____\n",
    "from ____.____.____ import ____\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ____(____):\n",
    "    ____: ____[____, ____]\n",
    "    ____: str \n",
    "    ____: str | ____ \n",
    "    ____: str | ____ \n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def ____(state: MultiLLMState):\n",
    "    ____ = state[\"messages\"][-1].content\n",
    "    print(f\"get_topic: Original topic is '{topic}'\")\n",
    "    return {____: topic, ____: [AIMessage(content=f\"Topic received: {topic}\")]}\n",
    "\n",
    "def ____(state: MultiLLMState):\n",
    "    ____ = state[\"original_topic\"]\n",
    "    print(f\"idea_generation_node: Generating idea for topic: '{topic}'\")\n",
    "    \n",
    "    ____ = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたは新しいアイデアを生み出すのが得意なAIです。与えられたトピックに関して、ユニークで面白いアイデアを一つ提案してください。アイデアは簡潔に一行で述べてください。\"),\n",
    "        (\"human\", \"トピック: {topic}\")\n",
    "    ])\n",
    "    \n",
    "    ____ = prompt_template_idea | llm \n",
    "    ____ = chain.invoke({\"topic\": topic})\n",
    "    ____ = response.content.strip()\n",
    "    \n",
    "    print(f\"idea_generation_node: Generated idea: '{idea}'\")\n",
    "    return {____: idea, ____: [AIMessage(content=f\"Generated Idea: {idea}\")]}\n",
    "\n",
    "def ____(state: MultiLLMState):\n",
    "    ____ = state[\"generated_idea\"]\n",
    "    if not idea:\n",
    "        return {____: [AIMessage(content=\"No idea to evaluate.\")], ____: \"N/A\"}\n",
    "        \n",
    "    print(f\"idea_evaluation_node: Evaluating idea: '{idea}'\")\n",
    "    \n",
    "    ____ = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたはアイデアを客観的に評価するのが得意なAIです。与えられたアイデアについて、その実現可能性と面白さを評価し、短いコメントを述べてください。\"),\n",
    "        (\"human\", \"評価対象のアイデア: {idea}\")\n",
    "    ])\n",
    "    \n",
    "    ____ = prompt_template_eval | llm \n",
    "    ____ = chain.invoke({\"idea\": idea})\n",
    "    ____ = response.content.strip()\n",
    "    \n",
    "    print(f\"idea_evaluation_node: Evaluation: '{evaluation}'\")\n",
    "    return {____: evaluation, ____: [AIMessage(content=f\"Evaluation: {evaluation}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "____ = ____(MultiLLMState)\n",
    "\n",
    "____.____(\"capture_topic\", get_topic)\n",
    "____.____(\"generate_idea\", idea_generation_node)\n",
    "____.____(\"evaluate_idea\", idea_evaluation_node)\n",
    "\n",
    "____.____(\"capture_topic\")\n",
    "\n",
    "____.____(\"capture_topic\", \"generate_idea\")\n",
    "____.____(\"generate_idea\", \"evaluate_idea\")\n",
    "____.____(\"evaluate_idea\", END)\n",
    "\n",
    "____ = ____.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの実行と結果表示 ---\n",
    "topics_to_test = [\n",
    "    \"新しい料理のレシピ\",\n",
    "    \"未来の交通手段\",\n",
    "    \"週末の過ごし方\"\n",
    "]\n",
    "\n",
    "for topic_text in topics_to_test:\n",
    "    print(f\"\\n--- 複数LLM連携テスト (トピック: {topic_text}) ---\")\n",
    "    inputs = {\n",
    "        \"messages\": [HumanMessage(content=topic_text)],\n",
    "        \"original_topic\": \"\", \n",
    "        \"generated_idea\": None,\n",
    "        \"evaluated_idea\": None\n",
    "    }\n",
    "    final_state = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "    print(f\"  Original Topic: {final_state.get('original_topic')}\")\n",
    "    print(f\"  Generated Idea: {final_state.get('generated_idea')}\")\n",
    "    print(f\"  Evaluated Idea: {final_state.get('evaluated_idea')}\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答010</summary>\n",
    "\n",
    "``````python\n",
    "# 解答010\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class MultiLLMState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    original_topic: str # ユーザーからの最初のトピック\n",
    "    generated_idea: str | None # アイデア生成LLMの出力\n",
    "    evaluated_idea: str | None # アイデア評価LLMの出力\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def get_topic(state: MultiLLMState):\n",
    "    topic = state[\"messages\"][-1].content\n",
    "    print(f\"get_topic: Original topic is '{topic}'\")\n",
    "    return {\"original_topic\": topic, \"messages\": [AIMessage(content=f\"Topic received: {topic}\")]}\n",
    "\n",
    "def idea_generation_node(state: MultiLLMState):\n",
    "    topic = state[\"original_topic\"]\n",
    "    print(f\"idea_generation_node: Generating idea for topic: '{topic}'\")\n",
    "    \n",
    "    prompt_template_idea = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたは新しいアイデアを生み出すのが得意なAIです。与えられたトピックに関して、ユニークで面白いアイデアを一つ提案してください。アイデアは簡潔に一行で述べてください。\"),\n",
    "        (\"human\", \"トピック: {topic}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt_template_idea | llm \n",
    "    response = chain.invoke({\"topic\": topic})\n",
    "    idea = response.content.strip()\n",
    "    \n",
    "    print(f\"idea_generation_node: Generated idea: '{idea}'\")\n",
    "    return {\"generated_idea\": idea, \"messages\": [AIMessage(content=f\"Generated Idea: {idea}\")]}\n",
    "\n",
    "def idea_evaluation_node(state: MultiLLMState):\n",
    "    idea = state[\"generated_idea\"]\n",
    "    if not idea:\n",
    "        return {\"messages\": [AIMessage(content=\"No idea to evaluate.\")], \"evaluated_idea\": \"N/A\"}\n",
    "        \n",
    "    print(f\"idea_evaluation_node: Evaluating idea: '{idea}'\")\n",
    "    \n",
    "    prompt_template_eval = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたはアイデアを客観的に評価するのが得意なAIです。与えられたアイデアについて、その実現可能性と面白さを評価し、短いコメントを述べてください。\"),\n",
    "        (\"human\", \"評価対象のアイデア: {idea}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt_template_eval | llm \n",
    "    response = chain.invoke({\"idea\": idea})\n",
    "    evaluation = response.content.strip()\n",
    "    \n",
    "    print(f\"idea_evaluation_node: Evaluation: '{evaluation}'\")\n",
    "    return {\"evaluated_idea\": evaluation, \"messages\": [AIMessage(content=f\"Evaluation: {evaluation}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(MultiLLMState)\n",
    "\n",
    "workflow.add_node(\"capture_topic\", get_topic)\n",
    "workflow.add_node(\"generate_idea\", idea_generation_node)\n",
    "workflow.add_node(\"evaluate_idea\", idea_evaluation_node)\n",
    "\n",
    "workflow.set_entry_point(\"capture_topic\")\n",
    "\n",
    "workflow.add_edge(\"capture_topic\", \"generate_idea\")\n",
    "workflow.add_edge(\"generate_idea\", \"evaluate_idea\")\n",
    "workflow.add_edge(\"evaluate_idea\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "topics_to_test = [\n",
    "    \"新しい料理のレシピ\",\n",
    "    \"未来の交通手段\",\n",
    "    \"週末の過ごし方\"\n",
    "]\n",
    "\n",
    "for topic_text in topics_to_test:\n",
    "    print(f\"\\n--- 複数LLM連携テスト (トピック: {topic_text}) ---\")\n",
    "    inputs = {\n",
    "        \"messages\": [HumanMessage(content=topic_text)],\n",
    "        \"original_topic\": \"\", \n",
    "        \"generated_idea\": None,\n",
    "        \"evaluated_idea\": None\n",
    "    }\n",
    "    final_state = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "    print(f\"  Original Topic: {final_state.get('original_topic')}\")\n",
    "    print(f\"  Generated Idea: {final_state.get('generated_idea')}\")\n",
    "    print(f\"  Evaluated Idea: {final_state.get('evaluated_idea')}\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説010</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **コード解説:**\n",
    "    *   `MultiLLMState`には、ユーザーからの最初のトピック (`original_topic`)、最初のLLMが生成したアイデア (`generated_idea`)、そして二番目のLLMが評価した結果 (`evaluated_idea`) を保持するキーが定義されています。\n",
    "    *   `get_topic`ノード: ユーザーの入力を `original_topic` として状態に保存します。\n",
    "    *   `idea_generation_node`: `original_topic` に基づいて、アイデア生成用のプロンプト (`prompt_template_idea`) を使用してLLMを呼び出し、結果を `generated_idea` に保存します。\n",
    "    *   `idea_evaluation_node`: `generated_idea` に基づいて、アイデア評価用のプロンプト (`prompt_template_eval`) を使用してLLMを呼び出し（ここでも同じ `llm` インスタンスを使用していますが、プロンプトが異なるため役割が変わります）、結果を `evaluated_idea` に保存します。\n",
    "    *   グラフは `capture_topic` -> `generate_idea` -> `evaluate_idea` -> `END` という直列な流れで、各ステップで状態が更新されていきます。\n",
    "    *   各LLM呼び出しノード内では、`langchain_core.prompts.ChatPromptTemplate` を使ってそのノード専用のプロンプトを定義し、共通の `llm` インスタンスと組み合わせて (例: `chain = prompt_template | llm`) LLM呼び出しを行っています。これにより、同じLLMモデルでも異なる指示を与えることで、多様な処理を実現できます。\n",
    "*   **応用例:**\n",
    "    *   リサーチアシスタント: 質問受け付け -> 情報検索プロンプトでLLM -> 要約プロンプトでLLM -> 報告書作成プロンプトでLLM。\n",
    "    *   コード生成・レビュー: 要件定義 -> コード生成LLM -> 生成コード評価LLM -> 修正指示LLM。\n",
    "    *   このように、タスクを細分化し、各サブタスクに特化したプロンプトを持つLLMノードを連携させることで、より高品質な結果を得ることが期待できます。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題011: 第1章のまとめ - 簡単なQ&Aボットの改善\n",
    "\n",
    "第1章で学んだ様々な要素（状態管理、LLM連携、条件分岐、ループ、情報抽出など）を組み合わせて、問題004で作成したシンプルなチャットボットを改善してみましょう。このQ&Aボットは、ユーザーの質問のタイプ（例: 単純な挨拶、知識を問う質問、不明な質問）を判別し、応答を変化させたり、会話の回数をカウントしたりする機能を持つようにします。\n",
    "\n",
    "*   **学習内容:** 第1章で学んだ複数の概念（`StateGraph`の定義、`TypedDict`による状態管理、ノードとエッジの追加、LLM呼び出し、条件付きエッジによる分岐、ループ（会話ターン数制限による間接的なループ制御）、状態キーの更新）を統合し、少し複雑な対話型のQ&Aボットを構築します。これにより、LangGraphの基本的な要素を組み合わせて実用的なアプリケーションを作成する流れを体験します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄011\n",
    "from ____ import ____, ____, ____, ____\n",
    "import ____\n",
    "from ____.____ import ____, ____\n",
    "from ____.____.____ import ____\n",
    "from ____.____.____ import ____, ____\n",
    "from ____.____.____ import ____\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "____ = Literal[\"greeting\", \"knowledge_q\", \"opinion_q\", \"unknown_q\"]\n",
    "\n",
    "class ____(____):\n",
    "    ____: Annotated[list, add_messages]\n",
    "    ____: str \n",
    "    ____: Optional[QuestionCategory] \n",
    "    ____: Optional[str] \n",
    "    ____: int \n",
    "    ____: int \n",
    "    ____: Optional[str]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def ____(state: AdvancedQABotState):\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    current_turns = state.get(____, 0) + 1\n",
    "    print(f\"capture_input_and_increment_turn: User: '{user_message}', Turn: {current_turns}\")\n",
    "    return { \n",
    "        \"user_input\": user_message,\n",
    "        \"conversation_turns\": current_turns,\n",
    "        \"question_category\": None, \n",
    "        \"llm_response\": None, \n",
    "        \"error_message\": None \n",
    "    }\n",
    "\n",
    "def ____(state: AdvancedQABotState):\n",
    "    text = state[____].lower()\n",
    "    category: QuestionCategory = \"unknown_q\"\n",
    "    \n",
    "    if any(greet in text for greet in [\"こんにちは\", \"やあ\", \"どうも\", \"hello\", \"hi\"]):\n",
    "        category = \"greeting\"\n",
    "    elif any(q_word in text for q_word in [\"とは\", \"何ですか\", \"教えて\", \"なぜ\", \"what is\", \"tell me about\", \"why\"]) or \"?\" in text:\n",
    "        if any(opinion_word in text for opinion_word in [\"どう思う\", \"あなたの意見は\", \"what do you think about\"]):\n",
    "             category = \"opinion_q\"\n",
    "        else:\n",
    "            category = \"knowledge_q\"\n",
    "    \n",
    "    print(f\"categorize_question_node: Input '{text}' categorized as '{category}'\")\n",
    "    return {____: category, ____: [AIMessage(content=f\"Category determination: {category}\")]}\n",
    "\n",
    "def ____(state: AdvancedQABotState):\n",
    "    response = \"こんにちは！何かお手伝いできることはありますか？\"\n",
    "    print(f\"greeting_responder_node: Responding with '{response}'\")\n",
    "    return {____: response}\n",
    "\n",
    "def ____(state: AdvancedQABotState):\n",
    "    question = state[\"user_input\"]\n",
    "    print(f\"knowledge_llm_node: Asking LLM (knowledge): '{question}'\")\n",
    "    response_obj = ____.invoke([HumanMessage(content=question)]) \n",
    "    response_text = response_obj.content.strip()\n",
    "    print(f\"  LLM response: {response_text}\")\n",
    "    return {____: response_text}\n",
    "\n",
    "def ____(state: AdvancedQABotState):\n",
    "    question = state[\"user_input\"]\n",
    "    print(f\"opinion_llm_node: Asking LLM (opinion): '{question}'\")\n",
    "    prompt = ____.from_messages([\n",
    "        (\"system\", \"あなたは様々なトピックについて個人的な意見を述べることができるAIです。客観的な事実とあなたの意見を区別して話してください。\"),\n",
    "        (\"human\", \"{user_question}\")\n",
    "    ])\n",
    "    chain = prompt | ____\n",
    "    response_obj = chain.invoke({\"user_question\": question})\n",
    "    response_text = response_obj.content.strip()\n",
    "    print(f\"  LLM response: {response_text}\")\n",
    "    return {____: response_text}\n",
    "\n",
    "def ____(state: AdvancedQABotState):\n",
    "    response = \"申し訳ありませんが、ご質問の意図を正確に理解できませんでした。もう少し具体的に、または別の言葉で質問していただけますでしょうか？\"\n",
    "    print(f\"unknown_question_node: Responding with '{response}'\")\n",
    "    return {____: response}\n",
    "\n",
    "def ____(state: AdvancedQABotState):\n",
    "    final_resp = state.get(____, \"(AI did not generate a response for some reason)\")\n",
    "    print(f\"final_response_node: Final AI response to be added to messages: '{final_resp}'\")\n",
    "    return {____: [AIMessage(content=final_resp)]}\n",
    "\n",
    "# --- ルーター関数 ---\n",
    "def ____(state: AdvancedQABotState):\n",
    "    category = state.get(\"question_category\")\n",
    "    print(f\"route_by_category: Routing based on category '{category}'\")\n",
    "    if category == \"greeting\": return \"greeting_responder\"\n",
    "    if category == \"knowledge_q\": return \"knowledge_llm\"\n",
    "    if category == \"opinion_q\": return \"opinion_llm\"\n",
    "    return \"unknown_responder\"\n",
    "\n",
    "def ____(state: AdvancedQABotState):\n",
    "    current_turns = state.get(\"conversation_turns\", 0)\n",
    "    max_t = state.get(\"max_turns\", 3) \n",
    "    if current_turns >= max_t:\n",
    "        print(f\"check_conversation_limit: Max turns ({max_t}) reached for this interaction. Ending conversation.\")\n",
    "        return \"__end__\" \n",
    "    print(f\"check_conversation_limit: Turn {current_turns}/{max_t}. Continuing to categorize.\")\n",
    "    return \"continue_to_categorizer\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "____ = ____(AdvancedQABotState)\n",
    "\n",
    "____.____(\"input_handler\", capture_input_and_increment_turn)\n",
    "____.____(\"categorizer\", categorize_question_node)\n",
    "____.____(\"greeting_responder\", greeting_responder_node)\n",
    "____.____(\"knowledge_llm\", knowledge_llm_node)\n",
    "____.____(\"opinion_llm\", opinion_llm_node)\n",
    "____.____(\"unknown_responder\", unknown_question_node)\n",
    "____.____(\"final_responder\", final_response_node)\n",
    "\n",
    "____.____(\"input_handler\")\n",
    "\n",
    "____.____(\n",
    "    \"input_handler\",\n",
    "    check_conversation_limit,\n",
    "    {\n",
    "        \"continue_to_categorizer\": \"categorizer\",\n",
    "        \"__end__\": END \n",
    "    }\n",
    ")\n",
    "\n",
    "____.____(\n",
    "    \"categorizer\",\n",
    "    route_by_category,\n",
    "    {\n",
    "        \"greeting_responder\": \"greeting_responder\",\n",
    "        \"knowledge_llm\": \"knowledge_llm\",\n",
    "        \"opinion_llm\": \"opinion_llm\",\n",
    "        \"unknown_responder\": \"unknown_responder\"\n",
    "    }\n",
    ")\n",
    "\n",
    "____.____(\"greeting_responder\", \"final_responder\")\n",
    "____.____(\"knowledge_llm\", \"final_responder\")\n",
    "____.____(\"opinion_llm\", \"final_responder\")\n",
    "____.____(\"unknown_responder\", \"final_responder\")\n",
    "____.____(\"final_responder\", END)\n",
    "\n",
    "____ = ____.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- グラフの実行と結果表示 (インタラクティブテスト) ---\n",
    "chat_history_for_bot = []\n",
    "max_dialogue_turns = 3 \n",
    "\n",
    "print(f\"--- 第1章まとめ Q&Aボット (最大{max_dialogue_turns}往復) ---\")\n",
    "print(\"チャットを開始します。'exit' または 'quit' で終了します。\")\n",
    "\n",
    "for i in range(max_dialogue_turns):\n",
    "    user_text = input(f\"あなた (Turn {i + 1}): \")\n",
    "    if user_text.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"チャットを終了します。\")\n",
    "        break\n",
    "    \n",
    "    chat_history_for_bot.append(HumanMessage(content=user_text))\n",
    "    \n",
    "    current_invoke_inputs = {\n",
    "        \"messages\": chat_history_for_bot,\n",
    "        \"max_turns\": max_dialogue_turns,\n",
    "    }\n",
    "    if i == 0: \n",
    "        current_invoke_inputs[\"conversation_turns\"] = 0\n",
    "    # For subsequent turns, conversation_turns will be carried over from the previous state\n",
    "    # or re-initialized by capture_input_and_increment_turn if not present.\n",
    "\n",
    "    try:\n",
    "        final_bot_state = graph.invoke(current_invoke_inputs, {\"recursion_limit\": 25})\n",
    "\n",
    "        if final_bot_state and final_bot_state.get(\"messages\"):\n",
    "            ai_messages = [m for m in final_bot_state[\"messages\"] if isinstance(m, AIMessage)]\n",
    "            # The actual response to the user should be the last one added by final_responder\n",
    "            # which is the last AIMessage in the history if the graph completed that far.\n",
    "            if ai_messages and final_bot_state.get(\"llm_response\") is not None: # Check if llm_response was set\n",
    "                bot_actual_response = final_bot_state.get(\"llm_response\")\n",
    "                print(f\"AIボット: {bot_actual_response}\")\n",
    "                chat_history_for_bot.append(AIMessage(content=bot_actual_response))\n",
    "            elif not ai_messages and final_bot_state.get(\"conversation_turns\",0) >= final_bot_state.get(\"max_turns\", max_dialogue_turns):\n",
    "                 print(\"AIボット: (最大会話ターン数に達しましたので終了します。)\") # Explicit message for max turns reached at END\n",
    "                 break\n",
    "            else:\n",
    "                print(\"AIボット: (応答がありませんでした。)\")\n",
    "                # If the graph ended due to max_turns before final_responder, there might not be a new AIMessage.\n",
    "                # Check if it's due to max_turns.\n",
    "                if final_bot_state.get(\"conversation_turns\",0) >= final_bot_state.get(\"max_turns\", max_dialogue_turns):\n",
    "                    break # End chat if max turns hit and graph ended\n",
    "        else:\n",
    "            print(\"AIボット: (状態取得エラーまたは会話終了)\")\n",
    "            break\n",
    "\n",
    "        if final_bot_state.get(\"conversation_turns\", 0) >= final_bot_state.get(\"max_turns\", max_dialogue_turns) and not final_bot_state.get(\"llm_response\") : # type: ignore\n",
    "             # This case handles if check_conversation_limit routes directly to END\n",
    "             print(\"(最大会話ターン数に達しましたので終了します。)\")\n",
    "             break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        break\n",
    "else: \n",
    "    print(\"\\n最大会話往復数に達したのでチャットを終了します。\")\n",
    "\n",
    "print(\"\\n--- 最終的なチャット履歴 (ボットとの対話) ---\")\n",
    "for msg in chat_history_for_bot:\n",
    "    print(f\"  {msg.type.upper()}: {msg.content}\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答011</summary>\n",
    "\n",
    "``````python\n",
    "# 解答011\n",
    "from typing import TypedDict, Annotated, Literal, Optional\n",
    "import re\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "QuestionCategory = Literal[\"greeting\", \"knowledge_q\", \"opinion_q\", \"unknown_q\"]\n",
    "\n",
    "class AdvancedQABotState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_input: str \n",
    "    question_category: Optional[QuestionCategory] \n",
    "    llm_response: Optional[str] \n",
    "    conversation_turns: int \n",
    "    max_turns: int \n",
    "    error_message: Optional[str]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def capture_input_and_increment_turn(state: AdvancedQABotState):\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    # conversation_turnsはinvokeの入力で初期値を渡すか、.getで安全に取得\n",
    "    current_turns = state.get(\"conversation_turns\", 0) + 1 \n",
    "    print(f\"capture_input_and_increment_turn: User: '{user_message}', Turn: {current_turns}\")\n",
    "    return {\n",
    "        \"user_input\": user_message,\n",
    "        \"conversation_turns\": current_turns,\n",
    "        \"question_category\": None, \n",
    "        \"llm_response\": None, \n",
    "        \"error_message\": None \n",
    "    }\n",
    "\n",
    "def categorize_question_node(state: AdvancedQABotState):\n",
    "    text = state[\"user_input\"].lower()\n",
    "    category: QuestionCategory = \"unknown_q\"\n",
    "    \n",
    "    if any(greet in text for greet in [\"こんにちは\", \"やあ\", \"どうも\", \"hello\", \"hi\"]):\n",
    "        category = \"greeting\"\n",
    "    elif any(q_word in text for q_word in [\"とは\", \"何ですか\", \"教えて\", \"なぜ\", \"what is\", \"tell me about\", \"why\"]) or \"?\" in text:\n",
    "        if any(opinion_word in text for opinion_word in [\"どう思う\", \"あなたの意見は\", \"what do you think about\"]):\n",
    "             category = \"opinion_q\"\n",
    "        else:\n",
    "            category = \"knowledge_q\"\n",
    "    \n",
    "    print(f\"categorize_question_node: Input '{text}' categorized as '{category}'\")\n",
    "    return {\"question_category\": category, \"messages\": [AIMessage(content=f\"Category determination: {category}\")]}\n",
    "\n",
    "def greeting_responder_node(state: AdvancedQABotState):\n",
    "    response = \"こんにちは！何かお手伝いできることはありますか？\"\n",
    "    print(f\"greeting_responder_node: Responding with '{response}'\")\n",
    "    return {\"llm_response\": response}\n",
    "\n",
    "def knowledge_llm_node(state: AdvancedQABotState):\n",
    "    question = state[\"user_input\"]\n",
    "    print(f\"knowledge_llm_node: Asking LLM (knowledge): '{question}'\")\n",
    "    response_obj = llm.invoke([HumanMessage(content=question)])\n",
    "    response_text = response_obj.content.strip()\n",
    "    print(f\"  LLM response: {response_text}\")\n",
    "    return {\"llm_response\": response_text}\n",
    "\n",
    "def opinion_llm_node(state: AdvancedQABotState):\n",
    "    question = state[\"user_input\"]\n",
    "    print(f\"opinion_llm_node: Asking LLM (opinion): '{question}'\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたは様々なトピックについて個人的な意見を述べることができるAIです。客観的な事実とあなたの意見を区別して話してください。\"),\n",
    "        (\"human\", \"{user_question}\")\n",
    "    ])\n",
    "    chain = prompt | llm\n",
    "    response_obj = chain.invoke({\"user_question\": question})\n",
    "    response_text = response_obj.content.strip()\n",
    "    print(f\"  LLM response: {response_text}\")\n",
    "    return {\"llm_response\": response_text}\n",
    "\n",
    "def unknown_question_node(state: AdvancedQABotState):\n",
    "    response = \"申し訳ありませんが、ご質問の意図を正確に理解できませんでした。もう少し具体的に、または別の言葉で質問していただけますでしょうか？\"\n",
    "    print(f\"unknown_question_node: Responding with '{response}'\")\n",
    "    return {\"llm_response\": response}\n",
    "\n",
    "def final_response_node(state: AdvancedQABotState):\n",
    "    final_resp = state.get(\"llm_response\", \"(AI did not generate a response for some reason)\")\n",
    "    print(f\"final_response_node: Final AI response to be added to messages: '{final_resp}'\")\n",
    "    return {\"messages\": [AIMessage(content=final_resp)]}\n",
    "\n",
    "def route_by_category(state: AdvancedQABotState):\n",
    "    category = state.get(\"question_category\")\n",
    "    print(f\"route_by_category: Routing based on category '{category}'\")\n",
    "    if category == \"greeting\": return \"greeting_responder\"\n",
    "    if category == \"knowledge_q\": return \"knowledge_llm\"\n",
    "    if category == \"opinion_q\": return \"opinion_llm\"\n",
    "    return \"unknown_responder\"\n",
    "\n",
    "def check_conversation_limit(state: AdvancedQABotState):\n",
    "    current_turns = state.get(\"conversation_turns\", 0)\n",
    "    max_t = state.get(\"max_turns\", 3)\n",
    "    if current_turns >= max_t:\n",
    "        print(f\"check_conversation_limit: Max turns ({max_t}) reached. Routing to END.\")\n",
    "        return \"__end__\" \n",
    "    print(f\"check_conversation_limit: Turn {current_turns}/{max_t}. Continuing to categorize.\")\n",
    "    return \"continue_to_categorizer\"\n",
    "\n",
    "workflow = StateGraph(AdvancedQABotState)\n",
    "workflow.add_node(\"input_handler\", capture_input_and_increment_turn)\n",
    "workflow.add_node(\"categorizer\", categorize_question_node)\n",
    "workflow.add_node(\"greeting_responder\", greeting_responder_node)\n",
    "workflow.add_node(\"knowledge_llm\", knowledge_llm_node)\n",
    "workflow.add_node(\"opinion_llm\", opinion_llm_node)\n",
    "workflow.add_node(\"unknown_responder\", unknown_question_node)\n",
    "workflow.add_node(\"final_responder\", final_response_node)\n",
    "workflow.set_entry_point(\"input_handler\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"input_handler\",\n",
    "    check_conversation_limit,\n",
    "    {\n",
    "        \"continue_to_categorizer\": \"categorizer\",\n",
    "        \"__end__\": END \n",
    "    }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"categorizer\",\n",
    "    route_by_category,\n",
    "    {\n",
    "        \"greeting_responder\": \"greeting_responder\",\n",
    "        \"knowledge_llm\": \"knowledge_llm\",\n",
    "        \"opinion_llm\": \"opinion_llm\",\n",
    "        \"unknown_responder\": \"unknown_responder\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"greeting_responder\", \"final_responder\")\n",
    "workflow.add_edge(\"knowledge_llm\", \"final_responder\")\n",
    "workflow.add_edge(\"opinion_llm\", \"final_responder\")\n",
    "workflow.add_edge(\"unknown_responder\", \"final_responder\")\n",
    "workflow.add_edge(\"final_responder\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "chat_history_for_bot = []\n",
    "max_dialogue_turns = 3\n",
    "print(f\"--- 第1章まとめ Q&Aボット (最大{max_dialogue_turns}往復) ---\")\n",
    "print(\"チャットを開始します。'exit' または 'quit' で終了します。\")\n",
    "for i in range(max_dialogue_turns):\n",
    "    user_text = input(f\"あなた (Turn {i + 1}): \")\n",
    "    if user_text.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"チャットを終了します。\")\n",
    "        break\n",
    "    \n",
    "    chat_history_for_bot.append(HumanMessage(content=user_text))\n",
    "    current_invoke_inputs = {\n",
    "        \"messages\": chat_history_for_bot,\n",
    "        \"max_turns\": max_dialogue_turns,\n",
    "        \"conversation_turns\": i # Pass current turn index for the graph's logic\n",
    "    }\n",
    "    try:\n",
    "        final_bot_state = graph.invoke(current_invoke_inputs, {\"recursion_limit\": 25})\n",
    "        if final_bot_state and final_bot_state.get(\"messages\"):\n",
    "            # Check if the graph execution led to a response being set in llm_response\n",
    "            if final_bot_state.get(\"llm_response\") is not None:\n",
    "                bot_actual_response = final_bot_state[\"llm_response\"]\n",
    "                print(f\"AIボット: {bot_actual_response}\")\n",
    "                chat_history_for_bot.append(AIMessage(content=bot_actual_response))\n",
    "            # If max_turns was hit and routed to END, llm_response might not be set by final_responder\n",
    "            elif final_bot_state.get(\"conversation_turns\", 0) >= final_bot_state.get(\"max_turns\", max_dialogue_turns):\n",
    "                 print(\"AIボット: (最大会話ターン数に達しました。)\")\n",
    "                 break \n",
    "            else:\n",
    "                print(\"AIボット: (予期せぬ状態で応答がありませんでした。)\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"AIボット: (会話が終了しました。)\")\n",
    "            break\n",
    "        # Check again if max turns was reached by the graph's internal logic\n",
    "        if final_bot_state.get(\"conversation_turns\", 0) >= final_bot_state.get(\"max_turns\", max_dialogue_turns):\n",
    "             break # Exit the loop if graph decided it's max_turns\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        break\n",
    "else:\n",
    "    print(\"\\n最大会話往復数に達したのでチャットを終了します。\")\n",
    "print(\"\\n--- 最終的なチャット履歴 (ボットとの対話) ---\")\n",
    "for msg in chat_history_for_bot:\n",
    "    print(f\"  {msg.type.upper()}: {msg.content}\")\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  }
 ]
}
