{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1章: グラフの基本要素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "以下のセルを順番に実行して、演習に必要な環境をセットアップします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "\n",
    "このセルは、LangGraphおよび関連するLangChainライブラリをインストールします。実行には数分かかる場合があります。\n",
    "ご利用になるLLMプロバイダーに応じて、コメントアウトを解除して必要なライブラリをインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ライブラリのインストール ===\n",
    "# 基本ライブラリ (LangGraphとLangChain Core)\n",
    "!pip install -qU langchain langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLMプロバイダー別ライブラリ ---\n",
    "# ご利用になるLLMプロバイダーに応じて、以下の該当する行のコメントを解除して実行してください。\n",
    "\n",
    "# OpenAI (GPTシリーズ)\n",
    "# !pip install -qU langchain_openai\n",
    "\n",
    "# Azure OpenAI\n",
    "# !pip install -qU langchain_openai # Azureもlangchain_openaiを利用\n",
    "\n",
    "# Google Cloud Vertex AI (Gemini, PaLM等)\n",
    "# !pip install -qU langchain_google_vertexai\n",
    "\n",
    "# Google Gemini API (Google AI Studioで利用するGemini)\n",
    "# !pip install -qU langchain_google_genai\n",
    "\n",
    "# Anthropic (Claudeシリーズ)\n",
    "# !pip install -qU langchain_anthropic\n",
    "\n",
    "# Amazon Bedrock (AWS上の各種モデル、Claudeも含む)\n",
    "# !pip install -qU langchain_aws boto3 # Bedrock利用時はboto3も必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- その他の推奨ライブラリ ---\n",
    "# グラフの可視化や環境変数管理など、演習全体を通して利用する可能性のあるライブラリ\n",
    "!pip install -qU python-dotenv pygraphviz pydotplus graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMプロバイダーの選択\n",
    "\n",
    "このセルでは、使用するLLMプロバイダーを選択します。\n",
    "`LLM_PROVIDER` 変数に、利用したいプロバイダー名を設定してください。\n",
    "選択可能なプロバイダー: `\"openai\"`, `\"azure\"`, `\"google\"` (Vertex AI), `\"google_genai\"` (Gemini API), `\"anthropic\"`, `\"bedrock\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMプロバイダーの選択 ===\n",
    "# 利用したいLLMプロバイダーを以下の変数で指定してください。\n",
    "# \"openai\", \"azure\", \"google\" (Vertex AI), \"google_genai\" (Gemini API), \"anthropic\", \"bedrock\" のいずれかを選択できます。\n",
    "LLM_PROVIDER = \"openai\"  # 例: OpenAI を利用する場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIキー/環境変数の設定\n",
    "\n",
    "以下のセルを実行する前に、選択したLLMプロバイダーに応じたAPIキーまたは環境変数を設定する必要があります。\n",
    "\n",
    "**一般的な手順:**\n",
    "1.  `.env.sample` ファイルをコピーして `.env` ファイルを作成します。\n",
    "2.  `.env` ファイルを開き、選択したLLMプロバイダーに対応するAPIキーや必要な情報を記述します。\n",
    "    *   **OpenAI:** `OPENAI_API_KEY`\n",
    "    *   **Azure OpenAI:** `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `OPENAI_API_VERSION`, `AZURE_OPENAI_DEPLOYMENT_NAME`\n",
    "    *   **Google (Vertex AI):** `GOOGLE_CLOUD_PROJECT_ID`, `GOOGLE_CLOUD_LOCATION` (Colab環境外で実行する場合、`GOOGLE_APPLICATION_CREDENTIALS` 環境変数の設定も必要になることがあります)\n",
    "    *   **Google (Gemini API):** `GOOGLE_API_KEY`\n",
    "    *   **Anthropic:** `ANTHROPIC_API_KEY`\n",
    "    *   **AWS Bedrock:** `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION_NAME` (IAMロールを使用する場合は、これらのキー設定は不要な場合がありますが、リージョン名は必須です)\n",
    "3.  ファイルを保存します。\n",
    "\n",
    "**Google Colab を使用している場合:**\n",
    "上記の `.env` ファイルを使用する代わりに、Colabのシークレットマネージャーに必要なキーを登録してください。\n",
    "例えば、OpenAIを使用する場合は `OPENAI_API_KEY` という名前でシークレットを登録します。\n",
    "Vertex AI を利用する場合は、Colab上での認証 (`google.colab.auth.authenticate_user()`) が実行されます。\n",
    "\n",
    "このセルは、設定された情報に基づいて環境変数をロードし、LLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === APIキー/環境変数の設定 ===\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envファイルから環境変数を読み込む (存在する場合)\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# --- OpenAI ---\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY and IS_COLAB:\n",
    "        OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"OpenAI APIキーが設定されていません。環境変数 OPENAI_API_KEY を設定するか、Colab環境の場合はシークレットに OPENAI_API_KEY を設定してください。\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# --- Azure OpenAI ---\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not AZURE_OPENAI_API_KEY: AZURE_OPENAI_API_KEY = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not AZURE_OPENAI_ENDPOINT: AZURE_OPENAI_ENDPOINT = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        if not OPENAI_API_VERSION: OPENAI_API_VERSION = userdata.get(\"OPENAI_API_VERSION\") # 例: \"2023-07-01-preview\"\n",
    "        if not AZURE_OPENAI_DEPLOYMENT_NAME: AZURE_OPENAI_DEPLOYMENT_NAME = userdata.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if not AZURE_OPENAI_API_KEY: raise ValueError(\"Azure OpenAI APIキー (AZURE_OPENAI_API_KEY) が設定されていません。\")\n",
    "    if not AZURE_OPENAI_ENDPOINT: raise ValueError(\"Azure OpenAI エンドポイント (AZURE_OPENAI_ENDPOINT) が設定されていません。\")\n",
    "    if not OPENAI_API_VERSION: OPENAI_API_VERSION = \"2023-07-01-preview\" # デフォルトを設定することも可能\n",
    "    if not AZURE_OPENAI_DEPLOYMENT_NAME: raise ValueError(\"Azure OpenAI デプロイメント名 (AZURE_OPENAI_DEPLOYMENT_NAME) が設定されていません。\")\n",
    "\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "    os.environ[\"OPENAI_API_VERSION\"] = OPENAI_API_VERSION\n",
    "\n",
    "# --- Google Cloud Vertex AI (Gemini) ---\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT_ID\") # .env 用に修正\n",
    "    LOCATION = os.environ.get(\"GOOGLE_CLOUD_LOCATION\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not PROJECT_ID: PROJECT_ID = userdata.get(\"GOOGLE_CLOUD_PROJECT_ID\")\n",
    "        if not LOCATION: LOCATION = userdata.get(\"GOOGLE_CLOUD_LOCATION\") # 例: \"us-central1\"\n",
    "        from google.colab import auth as google_auth\n",
    "        google_auth.authenticate_user() # Vertex AI を使う場合は Colab での認証を推奨\n",
    "    else: # Colab外の場合、.envから読み込んだ値で環境変数を設定\n",
    "        if PROJECT_ID: os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID # Vertex AI SDKが参照する標準的な環境変数名\n",
    "        if LOCATION: os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "\n",
    "    if not PROJECT_ID: raise ValueError(\"Google Cloud Project ID が設定されていません。環境変数 GOOGLE_CLOUD_PROJECT_ID を設定するか、Colab環境の場合はシークレットに GOOGLE_CLOUD_PROJECT_ID を設定してください。\")\n",
    "    if not LOCATION: LOCATION = \"us-central1\" # デフォルトロケーション\n",
    "\n",
    "# --- Google Gemini API (langchain-google-genai) ---\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY and IS_COLAB:\n",
    "        GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"Google APIキーが設定されていません。環境変数 GOOGLE_API_KEY を設定するか、Colab環境の場合はシークレットに GOOGLE_API_KEY を設定してください。\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# --- Anthropic (Claude) ---\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY and IS_COLAB:\n",
    "        ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        raise ValueError(\"Anthropic APIキーが設定されていません。環境変数 ANTHROPIC_API_KEY を設定するか、Colab環境の場合はシークレットに ANTHROPIC_API_KEY を設定してください。\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "# --- Amazon Bedrock (Claude) ---\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    AWS_REGION_NAME = os.environ.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if IS_COLAB: \n",
    "        if not AWS_ACCESS_KEY_ID: AWS_ACCESS_KEY_ID = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
    "        if not AWS_SECRET_ACCESS_KEY: AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        if not AWS_REGION_NAME: AWS_REGION_NAME = userdata.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if not AWS_REGION_NAME:\n",
    "         raise ValueError(\"AWSリージョン名 (AWS_REGION_NAME) が設定されていません。Bedrock利用にはリージョン指定が必要です。\")\n",
    "\n",
    "    # 環境変数に設定 (boto3がこれらを自動で読み込む)\n",
    "    if AWS_ACCESS_KEY_ID: os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "    if AWS_SECRET_ACCESS_KEY: os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION_NAME # boto3が参照する標準的なリージョン環境変数名\n",
    "    os.environ[\"AWS_REGION\"] = AWS_REGION_NAME # いくつかのライブラリはこちらを参照することもある\n",
    "\n",
    "print(f\"APIキー/環境変数の設定完了 (プロバイダー: {LLM_PROVIDER})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMクライアントの初期化\n",
    "\n",
    "このセルは、上で選択・設定したLLMプロバイダーに基づいて、対応するLLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMクライアントの動的初期化 ===\n",
    "llm = None\n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), # 環境変数から取得\n",
    "        openai_api_version=os.environ.get(\"OPENAI_API_VERSION\"), # 環境変数から取得\n",
    "        temperature=0,\n",
    "    )\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    from langchain_google_vertexai import ChatVertexAI\n",
    "    # PROJECT_ID, LOCATION は前のセルで環境変数に設定済みか、Colabの場合は直接利用\n",
    "    llm = ChatVertexAI(model_name=\"gemini-2.0-flash\", temperature=0, project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"), location=os.environ.get(\"GOOGLE_CLOUD_LOCATION\"))\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    from langchain_anthropic import ChatAnthropic\n",
    "    llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    from langchain_aws import ChatBedrock # langchain_community.chat_models から langchain_aws に変更の可能性あり\n",
    "    # AWS_REGION_NAME は前のセルで環境変数 AWS_DEFAULT_REGION に設定済み\n",
    "    llm = ChatBedrock( # BedrockChat ではなく ChatBedrock が一般的\n",
    "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        # region_name=os.environ.get(\"AWS_DEFAULT_REGION\"), # 通常、boto3が環境変数から自動で読み込む\n",
    "        model_kwargs={\"temperature\": 0},\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Unsupported LLM_PROVIDER: {LLM_PROVIDER}. \"\n",
    "        \"Please choose from 'openai', 'azure', 'google', 'google_genai', 'anthropic', or 'bedrock'.\"\n",
    "    )\n",
    "\n",
    "print(f\"LLM Provider: {LLM_PROVIDER}\")\n",
    "if llm:\n",
    "    print(f\"LLM Client Type: {type(llm)}\")\n",
    "    # モデル名取得の試行を汎用的に\n",
    "    model_attr = (\n",
    "                 getattr(llm, 'model', None) or\n",
    "                 getattr(llm, 'model_name', None) or\n",
    "                 getattr(llm, 'model_id', None) or\n",
    "                 (hasattr(llm, 'llm') and getattr(llm.llm, 'model', None)) # 一部のLLMクライアントのネスト構造に対応\n",
    "    )\n",
    "    if hasattr(llm, 'azure_deployment') and not model_attr: # Azure特有の属性\n",
    "        model_attr = llm.azure_deployment\n",
    "        \n",
    "    if model_attr:\n",
    "        print(f\"LLM Model: {model_attr}\")\n",
    "    else:\n",
    "        print(\"LLM Model: (Could not determine model name from client attributes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題001: 最小構成のLangGraphグラフの構築\n",
    "\n",
    "### 課題\n",
    "LangGraphの最も基本的な構成要素である`StateGraph`と`State`を理解し、シンプルなグラフを構築してみましょう。この問題では、入力された文字列をそのまま出力するだけの、単一のノードを持つグラフを作成します。\n",
    "\n",
    "*   **学習内容:** この問題では、`StateGraph`、`TypedDict`を用いた`State`の定義、`add_node`、`set_entry_point`、`add_edge`、`END`といったLangGraphの最も基本的なAPIを学びます。また、`Annotated`と`add_messages`を使ってメッセージ履歴を管理する方法も理解します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def simple_node(state: GraphState):\n",
    "    print(f'simple_node: {state[\"messages\"][-1].content}')\n",
    "    return {\"messages\": [state[\"messages\"][-1]]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(____)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"simple_node\", ____)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(____)\n",
    "\n",
    "# 終了ポイントの設定\n",
    "workflow.add_edge(\"simple_node\", ____)\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs = {\"messages\": [(\"user\", \"Hello, LangGraph!\")]}\n",
    "\n",
    "# 最終結果の確認\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "# 解答001\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    # グラフの状態を保持する辞書\n",
    "    # ここでは、入力メッセージを保持する\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def simple_node(state: GraphState):\n",
    "    # 入力されたメッセージをそのまま返すノード\n",
    "    print(f'simple_node: {state[\"messages\"][-1].content}')\n",
    "    return {\"messages\": [state[\"messages\"][-1]]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"simple_node\", simple_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"simple_node\")\n",
    "\n",
    "# 終了ポイントの設定\n",
    "workflow.add_edge(\"simple_node\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs = {\"messages\": [(\"user\", \"Hello, LangGraph!\")]}\n",
    "\n",
    "# 最終結果の確認\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題002: グラフの可視化\n",
    "\n",
    "### 課題\n",
    "問題001で構築した最小構成のグラフの構造を、視覚的に確認する方法を学びましょう。\n",
    "\n",
    "*   **学習内容:** `graph.get_graph().draw_png()` を使用して、コンパイル済みのLangGraphグラフ構造をPNG画像として描画し、Jupyter Notebook上に表示する方法を学びます。これにより、グラフのノードとエッジの接続関係を直感的に理解できるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "# 問題001のコードを再掲（このセルでグラフを定義・コンパイルします）\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def simple_node(state: GraphState):\n",
    "    # このノードは状態を更新せず、最後のメッセージをログに出力するだけ\n",
    "    print(f\"simple_node: Received message -> {state['messages'][-1].content}\")\n",
    "    # LangGraphでは、ノードがNoneまたは空の辞書を返すと、状態は更新されない\n",
    "    return\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"simple_node\", simple_node)\n",
    "workflow.set_entry_point(\"simple_node\")\n",
    "workflow.add_edge(\"simple_node\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    png_data = graph.get_graph().____()\n",
    "    display(Image(png_data))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの動作確認 ---\n",
    "# 可視化したグラフが問題001と同様に動作することを確認します。\n",
    "try:\n",
    "    inputs = {\"messages\": [HumanMessage(content=\"Hello, this is a test.\")]}\n",
    "    final_state = graph.invoke(inputs)\n",
    "    print(\"\\nグラフの実行が完了しました。\")\n",
    "    print(f\"最終的なmessagesの状態: {final_state['messages']}\")\n",
    "except NameError:\n",
    "    print(\"グラフが定義されていません。前のセルを先に実行してください。\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "# 解答002\n",
    "\n",
    "# 問題001のコードを再掲（このセルでグラフを定義・コンパイルします）\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def simple_node(state: GraphState):\n",
    "    # このノードは状態を更新せず、最後のメッセージをログに出力するだけ\n",
    "    print(f\"simple_node: Received message -> {state['messages'][-1].content}\")\n",
    "    # LangGraphでは、ノードがNoneまたは空の辞書を返すと、状態は更新されない\n",
    "    return\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"simple_node\", simple_node)\n",
    "workflow.set_entry_point(\"simple_node\")\n",
    "workflow.add_edge(\"simple_node\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    png_data = graph.get_graph().draw_png()\n",
    "    display(Image(png_data))\n",
    "    print(\"グラフが正常に可視化されました。\")\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの動作確認 ---\n",
    "# 可視化したグラフが問題001と同様に動作することを確認します。\n",
    "try:\n",
    "    inputs = {\"messages\": [HumanMessage(content=\"Hello, this is a test.\")]}\n",
    "    final_state = graph.invoke(inputs)\n",
    "    print(\"\\nグラフの実行が完了しました。\")\n",
    "    print(f\"最終的なmessagesの状態: {final_state['messages']}\")\n",
    "except NameError:\n",
    "    print(\"グラフが定義されていません。前のセルを先に実行してください。\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題003: 複数のノードを持つシーケンシャルグラフの構築\n",
    "\n",
    "### 課題\n",
    "前の問題で学んだ基本的なグラフ構築に加えて、複数のノードを直列に接続し、データがノード間をどのように流れるかを理解しましょう。ここでは、入力された文字列を加工する2つのノード（例：大文字化、逆順化）を持つグラフを作成します。\n",
    "\n",
    "*   **学習内容:** 複数のノードを`add_edge`で直列に接続する方法と、ノード間で状態がどのように引き継がれるかを学びます。`HumanMessage`と`AIMessage`を使って、メッセージの送信元を明示する方法も理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def uppercase_node(state: GraphState):\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"uppercase_node: {last_message_content}\")\n",
    "    return {\"messages\": [____(content=last_message_content.upper())]}\n",
    "\n",
    "def reverse_node(state: GraphState):\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"reverse_node: {last_message_content}\")\n",
    "    return {\"messages\": [____(content=last_message_content[::-1])]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(____)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"uppercase\", ____)\n",
    "workflow.add_node(\"reverse\", ____)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(____)\n",
    "\n",
    "# エッジの追加 (直列接続)\n",
    "workflow.add_edge(\"uppercase\", ____)\n",
    "workflow.add_edge(\"reverse\", ____)\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs = {\"messages\": [____(content=\"Hello LangGraph\")]}\n",
    "\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "# 解答003\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def uppercase_node(state: GraphState):\n",
    "    # 最新のメッセージを大文字に変換するノード\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"uppercase_node: {last_message_content}\")\n",
    "    return {\"messages\": [AIMessage(content=last_message_content.upper())]}\n",
    "\n",
    "def reverse_node(state: GraphState):\n",
    "    # 最新のメッセージを逆順にするノード\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"reverse_node: {last_message_content}\")\n",
    "    return {\"messages\": [AIMessage(content=last_message_content[::-1])]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"uppercase\", uppercase_node)\n",
    "workflow.add_node(\"reverse\", reverse_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"uppercase\")\n",
    "\n",
    "# エッジの追加 (直列接続)\n",
    "workflow.add_edge(\"uppercase\", \"reverse\")\n",
    "workflow.add_edge(\"reverse\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Hello LangGraph\")]}\n",
    "\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題004: グラフ内でのLLMの利用（シンプルなチャットボット）\n",
    "\n",
    "### 課題\n",
    "LangGraphのノード内で大規模言語モデル（LLM）を呼び出す方法を学び、シンプルなチャットボットを構築しましょう。ここでは、ユーザーからの入力に対してLLMが応答を生成し、その応答を返すグラフを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import os\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "# (from langchain_openai import ChatOpenAI や llm = ChatOpenAI(...) といった行はここには不要)\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def llm_node(state: GraphState):\n",
    "    # LLMを呼び出し、応答を生成するノード\n",
    "    # ノートブック冒頭で初期化された共通の `llm` 変数を使用します。\n",
    "    print(f\"llm_node: Calling LLM with messages: {state['messages']}\")\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    print(f\"llm_node: LLM response: {response.content}\")\n",
    "    return {\"messages\": [response]} # responseはAIMessageオブジェクトを期待 (ここは歯抜けにしない)\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"llm_responder\", llm_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"llm_responder\")\n",
    "\n",
    "# 終了ポイントの設定\n",
    "workflow.add_edge(\"llm_responder\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- チャットボットのテスト ---\")\n",
    "# 最初のメッセージはHumanMessageであると想定\n",
    "inputs = {\"messages\": [HumanMessage(content=\"こんにちは、あなたの名前は何ですか？\")]}\n",
    "\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n--- 別の質問 ---\")\n",
    "inputs2 = {\"messages\": [HumanMessage(content=\"今日の天気は？\")]}\n",
    "\n",
    "final_state2 = graph.invoke(inputs2)\n",
    "print(f\"最終的な応答: {final_state2['messages'][-1].content}\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "# 解答004\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import os # osはAPIキー設定のコメントアウト部分で使われているので残しても良いが、直接は不要になる\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def llm_node(state: GraphState):\n",
    "    # LLMを呼び出し、応答を生成するノード\n",
    "    # ノートブック冒頭で初期化された共通の `llm` 変数を使用します。\n",
    "    print(f\"llm_node: Calling LLM with messages: {state['messages']}\")\n",
    "    response = llm.invoke(state[\"messages\"]) # 共通llmを使用\n",
    "    print(f\"llm_node: LLM response: {response.content}\")\n",
    "    return {\"messages\": [response]} # responseはAIMessageオブジェクトを期待\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"llm_responder\", llm_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"llm_responder\")\n",
    "\n",
    "# 終了ポイントの設定\n",
    "workflow.add_edge(\"llm_responder\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- チャットボットのテスト ---\")\n",
    "# 最初のメッセージはHumanMessageであると想定\n",
    "inputs = {\"messages\": [HumanMessage(content=\"こんにちは、あなたの名前は何ですか？\")]}\n",
    "\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n--- 別の質問 ---\")\n",
    "inputs2 = {\"messages\": [HumanMessage(content=\"今日の天気は？\")]}\n",
    "\n",
    "final_state2 = graph.invoke(inputs2)\n",
    "print(f\"最終的な応答: {final_state2['messages'][-1].content}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題005: 状態の更新 - 特定キーの値を上書きする\n",
    "\n",
    "### 課題\n",
    "`add_messages` によるメッセージ履歴の追加だけでなく、グラフの状態(`State`)内の特定のキーの値を直接更新する方法を学びましょう。ここでは、カウンター値を保持する状態キーを定義し、ノードでその値をインクリメントするグラフを作成します。\n",
    "\n",
    "*   **学習内容:** `TypedDict`で定義する状態クラスに、`messages`以外のカスタムキー（ここでは`counter: int`）を追加し、ノード関数内でその値を直接読み書きする方法を学びます。これにより、メッセージ履歴だけでなく、数値や文字列、ブール値など、より多様なデータをグラフ全体で管理できるようになります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class CounterState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    counter: int \n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def increment_counter(state: CounterState):\n",
    "    # counterの値を1増やすノード\n",
    "    current_count = state.get(\"counter\", 0) # stateからcounterの値を取得、なければ0\n",
    "    new_count = current_count + 1\n",
    "    print(f\"increment_counter: Current count: {current_count}, New count: {new_count}\")\n",
    "    return {\"counter\": new_count, \"messages\": [HumanMessage(content=f\"Counter incremented to {new_count}\")]}\n",
    "\n",
    "def display_count(state: CounterState):\n",
    "    # counterの最終値を表示するノード (実際にはmessagesに追加されたもので確認)\n",
    "    print(f\"display_count: Final counter value is {state['counter']}\")\n",
    "    # このノードは状態を更新しないが、メッセージを追加しても良い\n",
    "    return {\"messages\": [HumanMessage(content=f\"Final count: {state['counter']}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(CounterState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"increment\", increment_counter)\n",
    "workflow.add_node(\"display\", display_count)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"increment\")\n",
    "\n",
    "# エッジの追加\n",
    "workflow.add_edge(\"increment\", \"display\")\n",
    "workflow.add_edge(\"display\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- カウンターテスト (初期値0から) ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Start counting\")], \"counter\": 0} # 初期カウンター値を設定\n",
    "\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n--- カウンターテスト (初期値5から) ---\")\n",
    "inputs_2 = {\"messages\": [HumanMessage(content=\"Start counting from 5\")], \"counter\": 5} # 初期カウンター値を設定\n",
    "\n",
    "final_state_2 = graph.invoke(inputs_2)\n",
    "print(f\"最終的な応答: {final_state_2['messages'][-1].content}\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "# 解答005\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class CounterState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    counter: int # 新しくカウンター用の状態キーを定義\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def increment_counter(state: CounterState):\n",
    "    # counterの値を1増やすノード\n",
    "    current_count = state.get(\"counter\", 0) # stateからcounterの値を取得、なければ0\n",
    "    new_count = current_count + 1\n",
    "    print(f\"increment_counter: Current count: {current_count}, New count: {new_count}\")\n",
    "    return {\"counter\": new_count, \"messages\": [HumanMessage(content=f\"Counter incremented to {new_count}\")]}\n",
    "\n",
    "def display_count(state: CounterState):\n",
    "    # counterの最終値を表示するノード (実際にはmessagesに追加されたもので確認)\n",
    "    print(f\"display_count: Final counter value is {state['counter']}\")\n",
    "    # このノードは状態を更新しないが、メッセージを追加しても良い\n",
    "    return {\"messages\": [HumanMessage(content=f\"Final count: {state['counter']}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(CounterState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"increment\", increment_counter)\n",
    "workflow.add_node(\"display\", display_count)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"increment\")\n",
    "\n",
    "# エッジの追加\n",
    "workflow.add_edge(\"increment\", \"display\")\n",
    "workflow.add_edge(\"display\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- カウンターテスト (初期値0から) ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Start counting\")], \"counter\": 0} # 初期カウンター値を設定\n",
    "\n",
    "final_state = graph.invoke(inputs)\n",
    "print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n--- カウンターテスト (初期値5から) ---\")\n",
    "inputs_2 = {\"messages\": [HumanMessage(content=\"Start counting from 5\")], \"counter\": 5} # 初期カウンター値を設定\n",
    "\n",
    "final_state_2 = graph.invoke(inputs_2)\n",
    "print(f\"最終的な応答: {final_state_2['messages'][-1].content}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題006: 状態の更新 - 複数のキーを一度に更新する\n",
    "\n",
    "### 課題\n",
    "一つのノードから、状態(`State`)の複数のキーを同時に更新する方法を学びましょう。ここでは、ユーザーからのメッセージ内容に応じて、応答メッセージと共に「応答タイプ」という別の状態キーも更新するグラフを作成します。\n",
    "\n",
    "*   **学習内容:** 一つのノード関数から返す辞書に複数のキーと値のペアを含めることで、グラフの状態(`State`)の複数の属性を一度に更新する方法を学びます。また、`typing.Literal`を使って、状態キーが取りうる値を制限する方法も示します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "ResponseType = Literal[\"greeting\", \"question\", \"other\", \"unknown\"]\n",
    "\n",
    "class MultiUpdateState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    response_type: ResponseType\n",
    "    last_user_message: str # 最後に入力されたユーザーメッセージ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def process_input(state: MultiUpdateState):\n",
    "    user_message = state[\"messages\"][-1].content.lower()\n",
    "    response_text = \"\"\n",
    "    resp_type: ResponseType = \"unknown\"\n",
    "\n",
    "    if \"こんにちは\" in user_message or \"こんばんは\" in user_message:\n",
    "        response_text = \"こんにちは！何かお手伝いできますか？\"\n",
    "        resp_type = \"greeting\" \n",
    "    elif \"?\" in user_message or \"教えて\" in user_message:\n",
    "        response_text = \"ご質問ありがとうございます。それについては現在調べています。\"\n",
    "        resp_type = \"question\" \n",
    "    else:\n",
    "        response_text = \"メッセージありがとうございます。\"\n",
    "        resp_type = \"other\" \n",
    "    \n",
    "    print(f\"process_input: User: '{user_message}', AI: '{response_text}', Type: '{resp_type}'\")\n",
    "    # 複数のキーを同時に更新して返す\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response_text)],\n",
    "        \"response_type\": resp_type,\n",
    "        \"last_user_message\": user_message\n",
    "    }\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(MultiUpdateState)\n",
    "\n",
    "workflow.add_node(\"processor\", process_input)\n",
    "workflow.set_entry_point(\"processor\")\n",
    "workflow.add_edge(\"processor\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "test_inputs = [\n",
    "    {\"messages\": [HumanMessage(content=\"こんにちは\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"LangGraphについて教えてください\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"今日の天気は晴れですね\")]}\n",
    "]\n",
    "\n",
    "for i, inputs in enumerate(test_inputs):\n",
    "    print(f\"\\n--- テスト実行 {i+1} ---\")\n",
    "\n",
    "    final_state = graph.invoke(inputs, {\"recursion_limit\": 3})\n",
    "    print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "    assert \"response_type\" in final_state\n",
    "    assert \"last_user_message\" in final_state\n",
    "    print(f\"Response Type: {final_state['response_type']}\")\n",
    "    print(f\"Last User Message: {final_state['last_user_message']}\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "# 解答006\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "ResponseType = Literal[\"greeting\", \"question\", \"other\", \"unknown\"]\n",
    "\n",
    "class MultiUpdateState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    response_type: ResponseType # 応答の種類を保持するキー\n",
    "    last_user_message: str # 最後に入力されたユーザーメッセージ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def process_input(state: MultiUpdateState):\n",
    "    user_message = state[\"messages\"][-1].content.lower()\n",
    "    response_text = \"\"\n",
    "    resp_type: ResponseType = \"unknown\"\n",
    "\n",
    "    if \"こんにちは\" in user_message or \"こんばんは\" in user_message:\n",
    "        response_text = \"こんにちは！何かお手伝いできますか？\"\n",
    "        resp_type = \"greeting\"\n",
    "    elif \"?\" in user_message or \"教えて\" in user_message:\n",
    "        response_text = \"ご質問ありがとうございます。それについては現在調べています。\"\n",
    "        resp_type = \"question\"\n",
    "    else:\n",
    "        response_text = \"メッセージありがとうございます。\"\n",
    "        resp_type = \"other\"\n",
    "    \n",
    "    print(f\"process_input: User: '{user_message}', AI: '{response_text}', Type: '{resp_type}'\")\n",
    "    # 複数のキーを同時に更新して返す\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response_text)],\n",
    "        \"response_type\": resp_type,\n",
    "        \"last_user_message\": user_message # 元のユーザーメッセージを保存\n",
    "    }\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(MultiUpdateState)\n",
    "\n",
    "workflow.add_node(\"processor\", process_input)\n",
    "workflow.set_entry_point(\"processor\")\n",
    "workflow.add_edge(\"processor\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "test_inputs = [\n",
    "    {\"messages\": [HumanMessage(content=\"こんにちは\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"LangGraphについて教えてください\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"今日の天気は晴れですね\")]}\n",
    "]\n",
    "\n",
    "for i, inputs in enumerate(test_inputs):\n",
    "    print(f\"\\n--- テスト実行 {i+1} ---\")\n",
    "    # 初期状態としてresponse_typeやlast_user_messageを渡すことも可能だが、\n",
    "    # この問題ではノード内でこれらが設定されることを確認する\n",
    "    initial_state = inputs.copy()\n",
    "    # 必要であれば、初期値を設定\n",
    "    # initial_state.setdefault(\"response_type\", \"unknown\") \n",
    "    # initial_state.setdefault(\"last_user_message\", \"\")\n",
    "\n",
    "    final_state = graph.invoke(initial_state, {\"recursion_limit\": 3})\n",
    "    print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "    assert \"response_type\" in final_state\n",
    "    assert \"last_user_message\" in final_state\n",
    "    print(f\"Response Type: {final_state['response_type']}\")\n",
    "    print(f\"Last User Message: {final_state['last_user_message']}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題007: `END` 以外の終了ノードの指定（概念理解）\n",
    "\n",
    "### 課題\n",
    "LangGraphでは、グラフの終点は通常、特別な `END` ノードに接続することで示されます。しかし、概念的には、あるノードが処理の最終ステップであり、それ以上後続のノードが存在しない場合、そのノードが事実上の「終了ノード」として機能すると考えることもできます。この問題では、特定のノードを実行した後、明示的に `END` に接続せず、グラフがそこで停止することを確認します。ただし、LangGraphのベストプラクティスとしては、可能な限り `END` を使用することが推奨されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class FinalNodeState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    status: str\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def start_process(state: FinalNodeState):\n",
    "    print(\"start_process: Process started.\")\n",
    "    return {\"status\": \"Processing\", \"messages\": [AIMessage(content=\"Process initiated.\")]}\n",
    "\n",
    "def final_processing_node(state: FinalNodeState):\n",
    "    # このノードが処理の最後とする\n",
    "    final_message = \"Process completed at final_processing_node.\"\n",
    "    print(f\"final_processing_node: {final_message}\")\n",
    "    return {\"status\": \"Completed\", \"messages\": [AIMessage(content=final_message)]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(FinalNodeState)\n",
    "\n",
    "workflow.add_node(\"start\", start_process)\n",
    "workflow.add_node(\"final_step\", final_processing_node)\n",
    "\n",
    "workflow.set_entry_point(\"start\")\n",
    "\n",
    "# startノードからfinal_stepノードへのエッジ\n",
    "workflow.add_edge(\"start\", \"final_step\")\n",
    "\n",
    "# final_stepノードからENDへのエッジを意図的に作成しない\n",
    "# workflow.add_edge(\"final_step\", END) # ← これをコメントアウトまたは削除\n",
    "\n",
    "# グラフのコンパイル\n",
    "# check_interruptions=True をつけると、ENDに到達しない場合にエラーになるため、\n",
    "# この例では明示的にENDに繋がないことを示すために compile() の引数なし、\n",
    "# または check_interruptions=False (デフォルト) を利用します。\n",
    "# Langfuseなどのトレーシングツールと連携する場合、ENDに到達しないとトレースが終了しないことがあるため注意。\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- 最終ノードテスト ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Begin process\")], \"status\": \"Initial\"}\n",
    "final_state = None\n",
    "\n",
    "for chunk in graph.stream(inputs, {\"recursion_limit\": 5}):\n",
    "    print(f\"Stream chunk: {chunk}\")\n",
    "    # streamの最後の chunk のキー（ノード名）とその値（状態）をfinal_stateとする\n",
    "    # chunkは {ノード名: 状態} という形式\n",
    "    if chunk:\n",
    "        final_state_key = list(chunk.keys())[0]\n",
    "        final_state = chunk[final_state_key]\n",
    "\n",
    "print(f\"Final State from stream: {final_state}\")\n",
    "\n",
    "# invokeを呼び出す場合、ENDに到達しないとエラーになるか、最後のノードの結果が返るかは\n",
    "# LangGraphのバージョンや設定に依存する可能性がある。\n",
    "# ここではstreamの挙動を中心に確認。\n",
    "try:\n",
    "    invoked_state = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"最終的な応答: {invoked_state['messages'][-1].content}\")\n",
    "    # invokeがエラーなく値を返す場合、その値を最終状態として扱う\n",
    "    # 基本的にはENDに到達することが期待される\n",
    "except Exception as e:\n",
    "    print(f\"Invoke failed as expected or due to other reasons: {e}\")\n",
    "    print(\"InvokeはENDに到達しない場合、エラーを発生させることがあります。streamの最後の出力で状態を確認してください。\")\n",
    "\n",
    "assert final_state is not None, \"Final state was not captured from stream\"\n",
    "assert final_state[\"status\"] == \"Completed\"\n",
    "assert \"Process completed at final_processing_node.\" in final_state[\"messages\"][-1].content\n",
    "print(\"Assertion for final_state from stream passed.\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "# 解答007\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class FinalNodeState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    status: str\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def start_process(state: FinalNodeState):\n",
    "    print(\"start_process: Process started.\")\n",
    "    return {\"status\": \"Processing\", \"messages\": [AIMessage(content=\"Process initiated.\")]}\n",
    "\n",
    "def final_processing_node(state: FinalNodeState):\n",
    "    # このノードが処理の最後とする\n",
    "    final_message = \"Process completed at final_processing_node.\"\n",
    "    print(f\"final_processing_node: {final_message}\")\n",
    "    return {\"status\": \"Completed\", \"messages\": [AIMessage(content=final_message)]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(FinalNodeState)\n",
    "\n",
    "workflow.add_node(\"start\", start_process)\n",
    "workflow.add_node(\"final_step\", final_processing_node)\n",
    "\n",
    "workflow.set_entry_point(\"start\")\n",
    "\n",
    "# startノードからfinal_stepノードへのエッジ\n",
    "workflow.add_edge(\"start\", \"final_step\")\n",
    "\n",
    "# final_stepノードからENDへのエッジを意図的に作成しない\n",
    "# workflow.add_edge(\"final_step\", END) # ← コメントアウトまたは削除\n",
    "\n",
    "# グラフのコンパイル\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- 最終ノードテスト ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Begin process\")], \"status\": \"Initial\"}\n",
    "final_state_from_stream = None\n",
    "\n",
    "print(\"Streaming execution:\")\n",
    "for chunk in graph.stream(inputs, {\"recursion_limit\": 5}):\n",
    "    print(f\"  Stream chunk: {chunk}\")\n",
    "    if chunk:\n",
    "        # chunkのキーはノード名。その値がそのノード実行後の状態。\n",
    "        # 最後のchunkの状態が、この実行における最終状態となる。\n",
    "        final_state_from_stream = chunk[list(chunk.keys())[0]]\n",
    "\n",
    "print(f\"Final State from stream: {final_state_from_stream}\")\n",
    "\n",
    "# invokeの挙動確認\n",
    "invoked_state = None\n",
    "try:\n",
    "    # ENDに繋がっていない場合、invokeはエラーを出すか、最後に実行されたノードの状態を返す\n",
    "    # LangGraphのバージョンや内部実装により挙動が変わりうるため注意\n",
    "    # ここでは、streamの結果を正として扱う\n",
    "    invoked_state = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"最終的な応答: {invoked_state['messages'][-1].content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Invoke call resulted in an error or unexpected behavior: {e}\")\n",
    "    print(\"This might be expected if the graph doesn't explicitly reach END.\")\n",
    "\n",
    "# streamから取得した最終状態でアサーションを行う\n",
    "assert final_state_from_stream is not None, \"Final state was not captured from stream\"\n",
    "assert final_state_from_stream[\"status\"] == \"Completed\"\n",
    "assert \"Process completed at final_processing_node.\" in final_state_from_stream[\"messages\"][-1].content\n",
    "print(\"Assertion for final_state_from_stream passed.\")\n",
    "\n",
    "# もしinvokeが値を返した場合、それも確認 (参考程度)\n",
    "if invoked_state:\n",
    "    print(f\"Invoked state status: {invoked_state.get('status')}\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "    print(\"Graph visualized. Note that 'final_step' does not point to END.\")\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題008: LLMノードと非LLMノードの連携強化\n",
    "\n",
    "### 課題\n",
    "LLM（大規模言語モデル）を組み込んだノードと、LLM以外の処理を行うノード（例: 文字列操作、データ抽出など）を連携させる方法を学びましょう。ここでは、LLMに質問を投げて得られた応答（文字列）から、特定の情報を抽出・加工して状態を更新する、より実践的なグラフを作成します。\n",
    "\n",
    "*   **学習内容:** LLMを呼び出すノードと、その出力を処理する非LLMノードを組み合わせることで、より高度な情報処理パイプラインを構築する方法を学びます。具体的には、LLMの自然言語応答から正規表現などを用いて構造化された情報を抽出し、グラフの状態を更新します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated\n",
    "import re # 正規表現モジュールをインポート\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ExtractionState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_question: str # ユーザーの元の質問\n",
    "    llm_response_text: str # LLMの生の応答テキスト\n",
    "    extracted_info: str | None # LLMの応答から抽出された情報\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def get_user_question(state: ExtractionState):\n",
    "    # ユーザーの質問を状態に保存\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"get_user_question: Received question: '{last_message_content}'\")\n",
    "    return {\"user_question\": last_message_content}\n",
    "\n",
    "def llm_responder_node(state: ExtractionState):\n",
    "    # LLMに質問を投げるノード\n",
    "    question = state[\"user_question\"]\n",
    "    print(f\"llm_responder_node: Asking LLM: '{question}'\")\n",
    "    # LLMに渡すメッセージは、過去の履歴全体でも、最新の質問だけでも良い\n",
    "    # ここでは簡単のため、最新の質問のみをHumanMessageとして渡す\n",
    "    response = llm.invoke([HumanMessage(content=question)])\n",
    "    response_content = response.content\n",
    "    print(f\"llm_responder_node: LLM raw response: '{response_content}'\")\n",
    "    return {\"messages\": [response], \"llm_response_text\": response_content}\n",
    "\n",
    "def data_extractor_node(state: ExtractionState):\n",
    "    # LLMの応答から情報を抽出するノード\n",
    "    raw_response = state[\"llm_response_text\"]\n",
    "    # 例: LLMが「日本の首都は東京です。」と答えたら「東京」を抽出\n",
    "    # ここでは簡単な正規表現で「XXはYYです」のYY部分を抽出試行\n",
    "    extracted = None\n",
    "    match = re.search(r\"(?:は|is)\\s*([^。.]+)[.。]?\", raw_response) # 簡易的な抽出\n",
    "    if match:\n",
    "        extracted = match.group(1).strip()\n",
    "    \n",
    "    print(f\"data_extractor_node: Raw response: '{raw_response}', Extracted: '{extracted}'\")\n",
    "    return {\"extracted_info\": extracted, \"messages\": [AIMessage(content=f\"Extracted: {extracted}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(ExtractionState)\n",
    "\n",
    "workflow.add_node(\"capture_question\", get_user_question)\n",
    "workflow.add_node(\"ask_llm\", llm_responder_node)\n",
    "workflow.add_node(\"extract_data\", data_extractor_node)\n",
    "\n",
    "workflow.set_entry_point(\"capture_question\")\n",
    "\n",
    "workflow.add_edge(\"capture_question\", \"ask_llm\")\n",
    "workflow.add_edge(\"ask_llm\", \"extract_data\")\n",
    "workflow.add_edge(\"extract_data\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "questions = [\n",
    "    \"日本の首都は何ですか？\",\n",
    "    \"フランスの有名な画家の名前を一人教えてください。\",\n",
    "    \"今日の天気は？\" # これは抽出パターンにマッチしない可能性\n",
    "]\n",
    "\n",
    "for q_text in questions:\n",
    "    print(f\"\\n--- LLM連携と情報抽出テスト (質問: {q_text}) ---\")\n",
    "    inputs = {\"messages\": [HumanMessage(content=q_text)]}\n",
    "    final_state = None\n",
    "    for chunk in graph.stream(inputs, {\"recursion_limit\": 5}):\n",
    "        print(f\"Stream chunk: {chunk}\")\n",
    "        if chunk:\n",
    "            final_state_key = list(chunk.keys())[0]\n",
    "            final_state = chunk[final_state_key]\n",
    "\n",
    "    if final_state:\n",
    "        print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "        print(f\"  User Question: {final_state.get('user_question')}\")\n",
    "        print(f\"  LLM Response: {final_state.get('llm_response_text')}\")\n",
    "        print(f\"  Extracted Info: {final_state.get('extracted_info')}\")\n",
    "    else:\n",
    "        # streamの最後がENDだった場合など、特定のノード名で状態が取れない場合がある\n",
    "        # その場合はinvokeで最終状態を取得\n",
    "        final_state_invoked = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "        print(f\"最終的な応答: {final_state_invoked['messages'][-1].content}\")\n",
    "        print(f\"  User Question: {final_state_invoked.get('user_question')}\")\n",
    "        print(f\"  LLM Response: {final_state_invoked.get('llm_response_text')}\")\n",
    "        print(f\"  Extracted Info: {final_state_invoked.get('extracted_info')}\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "# 解答008\n",
    "from typing import TypedDict, Annotated\n",
    "import re # 正規表現モジュールをインポート\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ExtractionState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_question: str # ユーザーの元の質問\n",
    "    llm_response_text: str # LLMの生の応答テキスト\n",
    "    extracted_info: str | None # LLMの応答から抽出された情報\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def get_user_question(state: ExtractionState):\n",
    "    # ユーザーの質問を状態に保存\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"get_user_question: Received question: '{last_message_content}'\")\n",
    "    return {\"user_question\": last_message_content}\n",
    "\n",
    "def llm_responder_node(state: ExtractionState):\n",
    "    # LLMに質問を投げるノード\n",
    "    question = state[\"user_question\"]\n",
    "    print(f\"llm_responder_node: Asking LLM: '{question}'\")\n",
    "    response = llm.invoke([HumanMessage(content=question)])\n",
    "    response_content = response.content\n",
    "    print(f\"llm_responder_node: LLM raw response: '{response_content}'\")\n",
    "    return {\"messages\": [response], \"llm_response_text\": response_content}\n",
    "\n",
    "def data_extractor_node(state: ExtractionState):\n",
    "    # LLMの応答から情報を抽出するノード\n",
    "    raw_response = state[\"llm_response_text\"]\n",
    "    extracted = None\n",
    "    # 改善された正規表現: 「XXはYYです」「XX is YY」のようなパターンや、単に「YYです」のような応答にも対応試行\n",
    "    # 質問が「日本の首都は何ですか？」で応答が「東京です。」の場合「東京」を抽出\n",
    "    # 質問が「日本の首都は？」で応答が「東京」の場合「東京」を抽出\n",
    "    patterns = [\n",
    "        r\"(?:.+は|.+\\s*is)\\s*(.+?)(?:です|。|\\.|\\s*for|$)\", #「～はXです」「～ is X」\n",
    "        r\"^([^。.]+?)(?:です|。|\\.|\\s*for|$)\" # 文頭から「Xです」\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, raw_response)\n",
    "        if match:\n",
    "            extracted = match.group(1).strip()\n",
    "            if extracted.lower() == state[\"user_question\"].lower().replace(\"何ですか\",\"\").replace(\"何\",\"зиру\").strip(\"?？\") : # 質問自体が答えになるような場合を除外\n",
    "                 extracted = None # 例：「天気は？」->「晴れです」はOKだが、「天気は？」->「天気」はNG\n",
    "                 continue\n",
    "            break\n",
    "    \n",
    "    # もし上記で抽出できなかった場合、LLMの応答が単語やフレーズそのものである可能性を考慮\n",
    "    if not extracted and len(raw_response.split()) < 5 and not state[\"user_question\"].startswith(raw_response): # 短い応答で、質問の繰り返しでない場合\n",
    "        extracted = raw_response.strip()\n",
    "\n",
    "    print(f\"data_extractor_node: Raw response: '{raw_response}', Extracted: '{extracted}'\")\n",
    "    return {\"extracted_info\": extracted, \"messages\": [AIMessage(content=f\"Extracted info: {extracted if extracted else 'N/A'}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(ExtractionState)\n",
    "\n",
    "workflow.add_node(\"capture_question\", get_user_question)\n",
    "workflow.add_node(\"ask_llm\", llm_responder_node)\n",
    "workflow.add_node(\"extract_data\", data_extractor_node)\n",
    "\n",
    "workflow.set_entry_point(\"capture_question\")\n",
    "\n",
    "workflow.add_edge(\"capture_question\", \"ask_llm\")\n",
    "workflow.add_edge(\"ask_llm\", \"extract_data\")\n",
    "workflow.add_edge(\"extract_data\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "questions = [\n",
    "    \"日本の首都は何ですか？\",\n",
    "    \"フランスの有名な画家の名前を一人教えてください。\", # LLMの回答次第で抽出成功/失敗が変わる\n",
    "    \"1+1は何？\", # LLMが「2です」と答えれば抽出できるかも\n",
    "    \"今日の天気は？\" # 「晴れです」なら「晴れ」を抽出期待\n",
    "]\n",
    "\n",
    "for q_text in questions:\n",
    "    print(f\"\\n--- LLM連携と情報抽出テスト (質問: {q_text}) ---\")\n",
    "    inputs = {\"messages\": [HumanMessage(content=q_text)], \"llm_response_text\": \"\", \"extracted_info\": None} # 初期値を設定\n",
    "    \n",
    "    # invokeで最終状態を取得する方がシンプル\n",
    "    final_state_data = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "\n",
    "    if final_state_data:\n",
    "        print(f\"最終的な応答: {final_state_data['messages'][-1].content}\")\n",
    "        print(f\"  User Question: {final_state_data.get('user_question')}\")\n",
    "        print(f\"  LLM Response: {final_state_data.get('llm_response_text')}\")\n",
    "        print(f\"  Extracted Info: {final_state_data.get('extracted_info')}\")\n",
    "    else:\n",
    "        print(\"Could not retrieve final state.\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題009: グラフの入力と出力のカスタマイズと明確化\n",
    "\n",
    "### 課題\n",
    "LangGraphのグラフを実行する際、`invoke()` や `stream()` メソッドに渡す初期状態の構造と、グラフ全体の最終的な出力状態の構造を意識することが重要です。`StateGraph` に渡す状態クラス（例: `TypedDict`）の定義が、実質的にグラフの入力と出力のスキーマ（型定義）となります。この問題では、入力として複数の情報を受け取り、それらを処理して特定の構造で出力するグラフを作成し、入出力の対応関係を明確に意識します。\n",
    "\n",
    "*   **学習内容:** `TypedDict` を使ってグラフの状態スキーマを定義する際、どのキーがグラフへの主要な「入力」として期待され、どのキーがグラフからの主要な「出力」として扱われるのかを明確に意識することの重要性を学びます。また、`Optional`型やデフォルト値の扱い方についても触れます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (入力と出力のスキーマを兼ねる) ---\n",
    "class ProcessedData(TypedDict):\n",
    "    item_id: str\n",
    "    description: str\n",
    "    is_processed: bool\n",
    "\n",
    "class ComplexIOState(TypedDict):\n",
    "    # 入力として期待されるキー\n",
    "    raw_item_name: str\n",
    "    raw_item_details: List[str]\n",
    "    processing_threshold: Optional[int]\n",
    "\n",
    "    # 処理中に使われるキー\n",
    "    messages: Annotated[list, add_messages]\n",
    "    internal_counter: int\n",
    "\n",
    "    # 出力として期待される主要なキー\n",
    "    processed_data: Optional[ProcessedData] # 処理結果\n",
    "    error_message: Optional[str] # エラー発生時のメッセージ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def initialize_processing(state: ComplexIOState):\n",
    "    print(f\"initialize_processing: Received item '{state['raw_item_name']}' with details {state['raw_item_details']}\")\n",
    "    threshold = state.get(\"processing_threshold\", 0)\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Initializing processing for {state['raw_item_name']}\")],\n",
    "        \"internal_counter\": 0,\n",
    "        \"processed_data\": None, # 出力キーを初期化\n",
    "        \"error_message\": None   # 出力キーを初期化\n",
    "        # processing_threshold は入力からそのまま引き継がれる\n",
    "    }\n",
    "\n",
    "def main_processor(state: ComplexIOState):\n",
    "    name = state[\"raw_item_name\"]\n",
    "    details_count = len(state[\"raw_item_details\"])\n",
    "    counter = state[\"internal_counter\"] + 1\n",
    "    threshold = state.get(\"processing_threshold\", 0)\n",
    "    \n",
    "    log_msg = f\"Processing '{name}', detail count: {details_count}, attempt: {counter}, threshold: {threshold}\"\n",
    "    print(f\"main_processor: {log_msg}\")\n",
    "\n",
    "    if details_count == 0:\n",
    "        err_msg = \"No details provided.\"\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {err_msg}\")],\n",
    "            \"error_message\": err_msg,\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "    \n",
    "    # 簡単な処理のシミュレーション\n",
    "    if counter > threshold: # processing_threshold を超えたら処理成功とするデモ\n",
    "        processed_item = ProcessedData(\n",
    "            item_id=f\"PROC_{name.upper()}_{counter}\",\n",
    "            description=f\"Successfully processed {name} with {details_count} details after {counter} attempts.\",\n",
    "            is_processed=True\n",
    "        )\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Successfully processed {name}\")],\n",
    "            \"processed_data\": processed_item,\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Attempt {counter} for {name} did not meet threshold.\")],\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def check_status(state: ComplexIOState):\n",
    "    if state.get(\"error_message\"):\n",
    "        print(\"check_status: Error detected, routing to END.\")\n",
    "        return \"__end__\" # エラーなら即終了 (ENDに直接マッピングされる特別な名前)\n",
    "    if state.get(\"processed_data\") and state[\"processed_data\"][\"is_processed\"]:\n",
    "        print(\"check_status: Successfully processed, routing to END.\")\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        print(\"check_status: Not yet processed or error, routing back to main_processor.\")\n",
    "        return \"retry_processing\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(ComplexIOState)\n",
    "\n",
    "workflow.add_node(\"initializer\", initialize_processing)\n",
    "workflow.add_node(\"processor\", main_processor)\n",
    "\n",
    "workflow.set_entry_point(\"initializer\")\n",
    "workflow.add_edge(\"initializer\", \"processor\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"processor\",\n",
    "    check_status,\n",
    "    {\n",
    "        \"__end__\": END,\n",
    "        \"retry_processing\": \"processor\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs_success = {\n",
    "    \"raw_item_name\": \"TestItem1\",\n",
    "    \"raw_item_details\": [\"detail A\", \"detail B\"],\n",
    "    \"processing_threshold\": 2 # 3回目のprocessor呼び出しで成功するはず\n",
    "}\n",
    "\n",
    "inputs_fail_no_details = {\n",
    "    \"raw_item_name\": \"TestItem2\",\n",
    "    \"raw_item_details\": [], # 詳細なし -> エラー\n",
    "    \"processing_threshold\": 1\n",
    "}\n",
    "\n",
    "test_cases = {\"Success Case\": inputs_success, \"Failure Case (No Details)\": inputs_fail_no_details}\n",
    "\n",
    "for case_name, inputs_data in test_cases.items():\n",
    "    print(f\"\\n--- I/Oカスタマイズテスト: {case_name} ---\")\n",
    "    # 入力時には、Stateで定義された必須キーとオプショナルなキーを渡す\n",
    "    # messages, internal_counter, processed_data, error_message は処理中に設定されるので入力不要\n",
    "    current_inputs = inputs_data.copy()\n",
    "    # messagesはadd_messagesの仕様上、リストでないとエラーになるため、空リストで初期化することが安全\n",
    "    current_inputs.setdefault(\"messages\", []) \n",
    "\n",
    "    final_output_state = graph.invoke(current_inputs, {\"recursion_limit\": 10})\n",
    "    print(f\"最終的な応答: {final_output_state['messages'][-1].content}\")\n",
    "\n",
    "    # 出力状態の確認 (processed_data または error_message が期待通りか)\n",
    "    if final_output_state.get(\"processed_data\"):\n",
    "        print(f\"  Processed Item ID: {final_output_state['processed_data']['item_id']}\")\n",
    "        print(f\"  Processed: {final_output_state['processed_data']['is_processed']}\")\n",
    "    if final_output_state.get(\"error_message\"):\n",
    "        print(f\"  Error: {final_output_state['error_message']}\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "# 解答009\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (入力と出力のスキーマを兼ねる) ---\n",
    "class ProcessedData(TypedDict):\n",
    "    item_id: str\n",
    "    description: str\n",
    "    is_processed: bool\n",
    "\n",
    "class ComplexIOState(TypedDict):\n",
    "    # 入力として期待されるキー\n",
    "    raw_item_name: str\n",
    "    raw_item_details: List[str]\n",
    "    processing_threshold: Optional[int]\n",
    "\n",
    "    # 処理中に使われるキー\n",
    "    messages: Annotated[list, add_messages] # 処理ログ用\n",
    "    internal_counter: int\n",
    "\n",
    "    # 出力として期待される主要なキー\n",
    "    processed_data: Optional[ProcessedData] # 処理結果\n",
    "    error_message: Optional[str] # エラー発生時のメッセージ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def initialize_processing(state: ComplexIOState):\n",
    "    print(f\"initialize_processing: Received item '{state['raw_item_name']}' with details {state['raw_item_details']}\")\n",
    "    # processing_thresholdが入力で与えられていない場合のデフォルト値を設定\n",
    "    # この例では、state.getで取得する際にデフォルト値を設定するアプローチではなく、\n",
    "    # 入力時に processing_threshold が Optional であることを明示し、\n",
    "    # main_processor で利用する際に .get でデフォルト値を扱う。\n",
    "    # ここでは、入力された値をそのまま引き継ぎ、他の内部状態を初期化。\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Initializing processing for {state['raw_item_name']}\")],\n",
    "        \"internal_counter\": 0,\n",
    "        \"processed_data\": None, # 出力キーを初期化\n",
    "        \"error_message\": None   # 出力キーを初期化\n",
    "    }\n",
    "\n",
    "def main_processor(state: ComplexIOState):\n",
    "    name = state[\"raw_item_name\"]\n",
    "    details_count = len(state[\"raw_item_details\"])\n",
    "    counter = state[\"internal_counter\"] + 1\n",
    "    # processing_threshold が None の場合はデフォルト値 0 を使用\n",
    "    threshold = state.get(\"processing_threshold\") if state.get(\"processing_threshold\") is not None else 0\n",
    "    \n",
    "    log_msg = f\"Processing '{name}', detail count: {details_count}, attempt: {counter}, threshold: {threshold}\"\n",
    "    print(f\"main_processor: {log_msg}\")\n",
    "\n",
    "    if details_count == 0:\n",
    "        err_msg = \"No details provided.\"\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {err_msg}\")],\n",
    "            \"error_message\": err_msg,\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "    \n",
    "    if counter > threshold:\n",
    "        processed_item = ProcessedData(\n",
    "            item_id=f\"PROC_{name.upper()}_{counter}\",\n",
    "            description=f\"Successfully processed {name} with {details_count} details after {counter} attempts.\",\n",
    "            is_processed=True\n",
    "        )\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Successfully processed {name}\")],\n",
    "            \"processed_data\": processed_item,\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Attempt {counter} for {name} did not meet threshold.\")],\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def check_status(state: ComplexIOState):\n",
    "    if state.get(\"error_message\"):\n",
    "        print(\"check_status: Error detected, routing to END.\")\n",
    "        return \"__end__\" \n",
    "    if state.get(\"processed_data\") and state[\"processed_data\"][\"is_processed\"]:\n",
    "        print(\"check_status: Successfully processed, routing to END.\")\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        print(\"check_status: Not yet processed or error, routing back to main_processor.\")\n",
    "        return \"retry_processing\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(ComplexIOState)\n",
    "\n",
    "workflow.add_node(\"initializer\", initialize_processing)\n",
    "workflow.add_node(\"processor\", main_processor)\n",
    "\n",
    "workflow.set_entry_point(\"initializer\")\n",
    "workflow.add_edge(\"initializer\", \"processor\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"processor\",\n",
    "    check_status,\n",
    "    {\n",
    "        \"__end__\": END,\n",
    "        \"retry_processing\": \"processor\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs_success = {\n",
    "    \"raw_item_name\": \"TestItem1\",\n",
    "    \"raw_item_details\": [\"detail A\", \"detail B\"],\n",
    "    \"processing_threshold\": 2 \n",
    "}\n",
    "\n",
    "inputs_fail_no_details = {\n",
    "    \"raw_item_name\": \"TestItem2\",\n",
    "    \"raw_item_details\": [], \n",
    "    \"processing_threshold\": 1\n",
    "}\n",
    "\n",
    "inputs_optional_threshold_not_provided = {\n",
    "    \"raw_item_name\": \"TestItem3\",\n",
    "    \"raw_item_details\": [\"detail C\"],\n",
    "    # processing_threshold は提供しない (OptionalなのでOK)\n",
    "}\n",
    "\n",
    "test_cases = {\n",
    "    \"Success Case\": inputs_success,\n",
    "    \"Failure Case (No Details)\": inputs_fail_no_details,\n",
    "    \"Success Case (Threshold Not Provided)\": inputs_optional_threshold_not_provided\n",
    "}\n",
    "\n",
    "for case_name, inputs_data in test_cases.items():\n",
    "    print(f\"\\n--- I/Oカスタマイズテスト: {case_name} ---\")\n",
    "    current_inputs = inputs_data.copy()\n",
    "    current_inputs.setdefault(\"messages\", []) \n",
    "    # Optionalでない内部状態キーで、初期化ノードで設定されるものは入力時に含めなくても良い\n",
    "    # 例: internal_counter, processed_data, error_message\n",
    "\n",
    "    final_output_state = graph.invoke(current_inputs, {\"recursion_limit\": 10})\n",
    "    print(f\"最終的な応答: {final_output_state['messages'][-1].content}\")\n",
    "\n",
    "    if final_output_state.get(\"processed_data\"):\n",
    "        print(f\"  Processed Item ID: {final_output_state['processed_data']['item_id']}\")\n",
    "        print(f\"  Processed: {final_output_state['processed_data']['is_processed']}\")\n",
    "    if final_output_state.get(\"error_message\"):\n",
    "        print(f\"  Error: {final_output_state['error_message']}\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題010: 複数のLLM呼び出しを含むグラフ\n",
    "\n",
    "### 課題\n",
    "一つのグラフ内で、異なる役割やプロンプトを持つ複数のLLM呼び出しノードを組み込む方法を学びましょう。例えば、最初のLLMがアイデアを生成し、次のLLMがそのアイデアを評価・洗練する、といった連携が考えられます。この問題では、簡単な役割分担を持つ2つのLLMノードを直列に接続します。\n",
    "\n",
    "*   **学習内容:** 一つのグラフ内に、それぞれ異なるプロンプトや役割を持つ複数のLLM呼び出しノードを配置し、それらを連携させる方法を学びます。これにより、より複雑で多段階の思考や処理を行うエージェントやパイプラインを構築できます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class MultiLLMState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    original_topic: str # ユーザーからの最初のトピック\n",
    "    generated_idea: str | None # アイデア生成LLMの出力\n",
    "    evaluated_idea: str | None # アイデア評価LLMの出力\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def get_topic(state: MultiLLMState):\n",
    "    topic = state[\"messages\"][-1].content\n",
    "    print(f\"get_topic: Original topic is '{topic}'\")\n",
    "    return {\"original_topic\": topic, \"messages\": [AIMessage(content=f\"Topic received: {topic}\")]}\n",
    "\n",
    "def idea_generation_node(state: MultiLLMState):\n",
    "    topic = state[\"original_topic\"]\n",
    "    print(f\"idea_generation_node: Generating idea for topic: '{topic}'\")\n",
    "    \n",
    "    # このLLM用のプロンプトテンプレート\n",
    "    prompt_template_idea = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたは新しいアイデアを生み出すのが得意なAIです。与えられたトピックに関して、ユニークで面白いアイデアを一つ提案してください。アイデアは簡潔に一行で述べてください。\"),\n",
    "        (\"human\", \"トピック: {topic}\")\n",
    "    ])\n",
    "    \n",
    "    # llm変数を使ってプロンプトを実行 (ノートブック冒頭で初期化された共通のllm)\n",
    "    chain = prompt_template_idea | llm\n",
    "    response = chain.invoke({\"topic\": topic})\n",
    "    idea = response.content.strip()\n",
    "    \n",
    "    print(f\"idea_generation_node: Generated idea: '{idea}'\")\n",
    "    return {\"generated_idea\": idea, \"messages\": [AIMessage(content=f\"Generated Idea: {idea}\")]}\n",
    "\n",
    "def idea_evaluation_node(state: MultiLLMState):\n",
    "    idea = state[\"generated_idea\"]\n",
    "    if not idea:\n",
    "        return {\"messages\": [AIMessage(content=\"No idea to evaluate.\")], \"evaluated_idea\": \"N/A\"}\n",
    "        \n",
    "    print(f\"idea_evaluation_node: Evaluating idea: '{idea}'\")\n",
    "    \n",
    "    # このLLM用のプロンプトテンプレート\n",
    "    prompt_template_eval = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたはアイデアを客観的に評価するのが得意なAIです。与えられたアイデアについて、その実現可能性と面白さを評価し、短いコメントを述べてください。\"),\n",
    "        (\"human\", \"評価対象のアイデア: {idea}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt_template_eval | llm\n",
    "    response = chain.invoke({\"idea\": idea})\n",
    "    evaluation = response.content.strip()\n",
    "    \n",
    "    print(f\"idea_evaluation_node: Evaluation: '{evaluation}'\")\n",
    "    return {\"evaluated_idea\": evaluation, \"messages\": [AIMessage(content=f\"Evaluation: {evaluation}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(MultiLLMState)\n",
    "\n",
    "workflow.add_node(\"capture_topic\", get_topic)\n",
    "workflow.add_node(\"generate_idea\", idea_generation_node)\n",
    "workflow.add_node(\"evaluate_idea\", idea_evaluation_node)\n",
    "\n",
    "workflow.set_entry_point(\"capture_topic\")\n",
    "\n",
    "workflow.add_edge(\"capture_topic\", \"generate_idea\")\n",
    "workflow.add_edge(\"generate_idea\", \"evaluate_idea\")\n",
    "workflow.add_edge(\"evaluate_idea\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "topics_to_test = [\n",
    "    \"新しい料理のレシピ\",\n",
    "    \"未来の交通手段\",\n",
    "    \"週末の過ごし方\"\n",
    "]\n",
    "\n",
    "for topic_text in topics_to_test:\n",
    "    print(f\"\\n--- 複数LLM連携テスト (トピック: {topic_text}) ---\")\n",
    "    inputs = {\n",
    "        \"messages\": [HumanMessage(content=topic_text)],\n",
    "        \"original_topic\": \"\", # 初期化ノードで設定される\n",
    "        \"generated_idea\": None,\n",
    "        \"evaluated_idea\": None\n",
    "    }\n",
    "    final_state = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "    print(f\"  Original Topic: {final_state.get('original_topic')}\")\n",
    "    print(f\"  Generated Idea: {final_state.get('generated_idea')}\")\n",
    "    print(f\"  Evaluated Idea: {final_state.get('evaluated_idea')}\")\n",
    "    # print(f\"  Message History: {final_state.get('messages')}\") # 必要なら表示\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "# 解答010\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class MultiLLMState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    original_topic: str # ユーザーからの最初のトピック\n",
    "    generated_idea: str | None # アイデア生成LLMの出力\n",
    "    evaluated_idea: str | None # アイデア評価LLMの出力\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def get_topic(state: MultiLLMState):\n",
    "    topic = state[\"messages\"][-1].content\n",
    "    print(f\"get_topic: Original topic is '{topic}'\")\n",
    "    return {\"original_topic\": topic, \"messages\": [AIMessage(content=f\"Topic received: {topic}\")]}\n",
    "\n",
    "def idea_generation_node(state: MultiLLMState):\n",
    "    topic = state[\"original_topic\"]\n",
    "    print(f\"idea_generation_node: Generating idea for topic: '{topic}'\")\n",
    "    \n",
    "    prompt_template_idea = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたは新しいアイデアを生み出すのが得意なAIです。与えられたトピックに関して、ユニークで面白いアイデアを一つ提案してください。アイデアは簡潔に一行で述べてください。\"),\n",
    "        (\"human\", \"トピック: {topic}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt_template_idea | llm \n",
    "    response = chain.invoke({\"topic\": topic})\n",
    "    idea = response.content.strip()\n",
    "    \n",
    "    print(f\"idea_generation_node: Generated idea: '{idea}'\")\n",
    "    return {\"generated_idea\": idea, \"messages\": [AIMessage(content=f\"Generated Idea: {idea}\")]}\n",
    "\n",
    "def idea_evaluation_node(state: MultiLLMState):\n",
    "    idea = state[\"generated_idea\"]\n",
    "    if not idea:\n",
    "        return {\"messages\": [AIMessage(content=\"No idea to evaluate.\")], \"evaluated_idea\": \"N/A\"}\n",
    "        \n",
    "    print(f\"idea_evaluation_node: Evaluating idea: '{idea}'\")\n",
    "    \n",
    "    prompt_template_eval = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたはアイデアを客観的に評価するのが得意なAIです。与えられたアイデアについて、その実現可能性と面白さを評価し、短いコメントを述べてください。\"),\n",
    "        (\"human\", \"評価対象のアイデア: {idea}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt_template_eval | llm \n",
    "    response = chain.invoke({\"idea\": idea})\n",
    "    evaluation = response.content.strip()\n",
    "    \n",
    "    print(f\"idea_evaluation_node: Evaluation: '{evaluation}'\")\n",
    "    return {\"evaluated_idea\": evaluation, \"messages\": [AIMessage(content=f\"Evaluation: {evaluation}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(MultiLLMState)\n",
    "\n",
    "workflow.add_node(\"capture_topic\", get_topic)\n",
    "workflow.add_node(\"generate_idea\", idea_generation_node)\n",
    "workflow.add_node(\"evaluate_idea\", idea_evaluation_node)\n",
    "\n",
    "workflow.set_entry_point(\"capture_topic\")\n",
    "\n",
    "workflow.add_edge(\"capture_topic\", \"generate_idea\")\n",
    "workflow.add_edge(\"generate_idea\", \"evaluate_idea\")\n",
    "workflow.add_edge(\"evaluate_idea\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "topics_to_test = [\n",
    "    \"新しい料理のレシピ\",\n",
    "    \"未来の交通手段\",\n",
    "    \"週末の過ごし方\"\n",
    "]\n",
    "\n",
    "for topic_text in topics_to_test:\n",
    "    print(f\"\\n--- 複数LLM連携テスト (トピック: {topic_text}) ---\")\n",
    "    inputs = {\n",
    "        \"messages\": [HumanMessage(content=topic_text)],\n",
    "        # original_topic, generated_idea, evaluated_idea はグラフ内で設定されるので初期値はNoneや空文字でOK\n",
    "        \"original_topic\": \"\", \n",
    "        \"generated_idea\": None,\n",
    "        \"evaluated_idea\": None\n",
    "    }\n",
    "    final_state = graph.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"最終的な応答: {final_state['messages'][-1].content}\")\n",
    "    print(f\"  Original Topic: {final_state.get('original_topic')}\")\n",
    "    print(f\"  Generated Idea: {final_state.get('generated_idea')}\")\n",
    "    print(f\"  Evaluated Idea: {final_state.get('evaluated_idea')}\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題011: 第1章のまとめ - 簡単なQ&Aボットの改善\n",
    "\n",
    "### 課題\n",
    "第1章で学んだ様々な要素（状態管理、LLM連携、条件分岐、ループ、情報抽出など）を組み合わせて、問題004で作成したシンプルなチャットボットを改善してみましょう。このQ&Aボットは、ユーザーの質問のタイプ（例: 単純な挨拶、知識を問う質問、不明な質問）を判別し、応答を変化させたり、会話の回数をカウントしたりする機能を持つようにします。\n",
    "\n",
    "*   **学習内容:** 第1章で学んだ複数の概念（`StateGraph`の定義、`TypedDict`による状態管理、ノードとエッジの追加、LLM呼び出し、条件付きエッジによる分岐、ループ（会話ターン数制限による間接的なループ制御）、状態キーの更新）を統合し、少し複雑な対話型のQ&Aボットを構築します。これにより、LangGraphの基本的な要素を組み合わせて実用的なアプリケーションを作成する流れを体験します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼ YOUR CODE HERE ▼▼▼▼▼▼▼▼▼▼\n",
    "from typing import TypedDict, Annotated, Literal, Optional\n",
    "import re\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "QuestionCategory = Literal[\"greeting\", \"knowledge_q\", \"opinion_q\", \"unknown_q\"]\n",
    "\n",
    "class AdvancedQABotState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_input: str # 最新のユーザー入力\n",
    "    question_category: Optional[QuestionCategory] # 質問のカテゴリ\n",
    "    llm_response: Optional[str] # LLMからの最終的な応答\n",
    "    conversation_turns: int # 会話のターン数\n",
    "    max_turns: int # 最大許容ターン数\n",
    "    error_message: Optional[str]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def capture_input_and_increment_turn(state: AdvancedQABotState):\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    current_turns = state.get(\"conversation_turns\", 0) + 1\n",
    "    print(f\"capture_input_and_increment_turn: User: '{user_message}', Turn: {current_turns}\")\n",
    "    return {\n",
    "        \"user_input\": user_message,\n",
    "        \"conversation_turns\": current_turns,\n",
    "        \"question_category\": None, # 各処理の前に初期化\n",
    "        \"llm_response\": None, # 各処理の前に初期化\n",
    "        \"error_message\": None # 各処理の前に初期化\n",
    "    }\n",
    "\n",
    "def categorize_question_node(state: AdvancedQABotState):\n",
    "    text = state[\"user_input\"].lower()\n",
    "    category: QuestionCategory = \"unknown_q\"\n",
    "    \n",
    "    # 簡単なキーワードベースのカテゴリ分け（LLMを使っても良い）\n",
    "    if any(greet in text for greet in [\"こんにちは\", \"やあ\", \"どうも\"]):\n",
    "        category = \"greeting\"\n",
    "    elif any(q_word in text for q_word in [\"何ですか\", \"教えて\", \"とは\", \"なぜ\"]) or \"?\" in text:\n",
    "        # ここでは知識を問う質問と意見を問う質問を単純に分けるのは難しいので、一旦「知識」としておく\n",
    "        category = \"knowledge_q\"\n",
    "        if any(opinion_word in text for opinion_word in [\"どう思う\", \"意見は\"]):\n",
    "             category = \"opinion_q\"\n",
    "    \n",
    "    print(f\"categorize_question_node: Input '{text}' categorized as '{category}'\")\n",
    "    return {\"question_category\": category, \"messages\": [AIMessage(content=f\"Category: {category}\")]}\n",
    "\n",
    "def greeting_responder_node(state: AdvancedQABotState):\n",
    "    response = \"こんにちは！ご用件は何でしょうか？\"\n",
    "    print(f\"greeting_responder_node: Responding with '{response}'\")\n",
    "    return {\"llm_response\": response}\n",
    "\n",
    "def knowledge_llm_node(state: AdvancedQABotState):\n",
    "    question = state[\"user_input\"]\n",
    "    print(f\"knowledge_llm_node: Asking LLM (knowledge): '{question}'\")\n",
    "    # 知識ベースの質問応答用プロンプト (問題013のように専用プロンプトも可)\n",
    "    # ここではシンプルにそのまま質問\n",
    "    response_obj = llm.invoke([HumanMessage(content=question)])\n",
    "    response_text = response_obj.content.strip()\n",
    "    print(f\"  LLM response: {response_text}\")\n",
    "    return {\"llm_response\": response_text}\n",
    "\n",
    "def opinion_llm_node(state: AdvancedQABotState):\n",
    "    question = state[\"user_input\"]\n",
    "    print(f\"opinion_llm_node: Asking LLM (opinion): '{question}'\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたは様々なトピックについて個人的な意見を述べることができるAIです。ただし、客観的な事実と意見を区別して話してください。\"),\n",
    "        (\"human\", \"{user_question}\")\n",
    "    ])\n",
    "    chain = prompt | llm\n",
    "    response_obj = chain.invoke({\"user_question\": question})\n",
    "    response_text = response_obj.content.strip()\n",
    "    print(f\"  LLM response: {response_text}\")\n",
    "    return {\"llm_response\": response_text}\n",
    "\n",
    "def unknown_question_node(state: AdvancedQABotState):\n",
    "    response = \"申し訳ありません、よく理解できませんでした。別の言葉で質問していただけますか？\"\n",
    "    print(f\"unknown_question_node: Responding with '{response}'\")\n",
    "    return {\"llm_response\": response}\n",
    "\n",
    "def final_response_node(state: AdvancedQABotState):\n",
    "    # 最終的な応答をmessagesに追加\n",
    "    final_resp = state.get(\"llm_response\", \"(No response generated)\")\n",
    "    print(f\"final_response_node: Final AI response: '{final_resp}'\")\n",
    "    return {\"messages\": [AIMessage(content=final_resp)]}\n",
    "\n",
    "# --- ルーター関数 ---\n",
    "def route_by_category(state: AdvancedQABotState):\n",
    "    category = state.get(\"question_category\")\n",
    "    print(f\"route_by_category: Routing based on category '{category}'\")\n",
    "    if category == \"greeting\":\n",
    "        return \"greeting_responder\"\n",
    "    elif category == \"knowledge_q\":\n",
    "        return \"knowledge_llm\"\n",
    "    elif category == \"opinion_q\":\n",
    "        return \"opinion_llm\"\n",
    "    else: # unknown_q or None\n",
    "        return \"unknown_responder\"\n",
    "\n",
    "def check_conversation_limit(state: AdvancedQABotState):\n",
    "    current_turns = state.get(\"conversation_turns\", 0)\n",
    "    max_t = state.get(\"max_turns\", 3)\n",
    "    if current_turns >= max_t:\n",
    "        print(f\"check_conversation_limit: Max turns ({max_t}) reached. Ending conversation.\")\n",
    "        # 最後の応答を生成するために、一旦 final_response_node には行かせる\n",
    "        # もし、ここで完全に終了させたいなら、ENDに直接つなぐノードを用意する\n",
    "        return \"__end__\"\n",
    "    print(f\"check_conversation_limit: Turn {current_turns}/{max_t}. Continuing.\")\n",
    "    return \"continue_conversation\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(AdvancedQABotState)\n",
    "\n",
    "workflow.add_node(\"input_handler\", capture_input_and_increment_turn)\n",
    "workflow.add_node(\"categorizer\", categorize_question_node)\n",
    "workflow.add_node(\"greeting_responder\", greeting_responder_node)\n",
    "workflow.add_node(\"knowledge_llm\", knowledge_llm_node)\n",
    "workflow.add_node(\"opinion_llm\", opinion_llm_node)\n",
    "workflow.add_node(\"unknown_responder\", unknown_question_node)\n",
    "workflow.add_node(\"final_responder\", final_response_node)\n",
    "\n",
    "workflow.set_entry_point(\"input_handler\")\n",
    "\n",
    "# 入力後、会話ターン数チェック\n",
    "workflow.add_conditional_edges(\n",
    "    \"input_handler\",\n",
    "    check_conversation_limit,\n",
    "    {\n",
    "        \"continue_conversation\": \"categorizer\",\n",
    "        \"__end__\": END \n",
    "    }\n",
    ")\n",
    "\n",
    "# カテゴリ分類後、各処理ノードへ\n",
    "workflow.add_conditional_edges(\n",
    "    \"categorizer\",\n",
    "    route_by_category,\n",
    "    {\n",
    "        \"greeting_responder\": \"greeting_responder\",\n",
    "        \"knowledge_llm\": \"knowledge_llm\",\n",
    "        \"opinion_llm\": \"opinion_llm\",\n",
    "        \"unknown_responder\": \"unknown_responder\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 各応答生成ノードから最終応答整形ノードへ\n",
    "workflow.add_edge(\"greeting_responder\", \"final_responder\")\n",
    "workflow.add_edge(\"knowledge_llm\", \"final_responder\")\n",
    "workflow.add_edge(\"opinion_llm\", \"final_responder\")\n",
    "workflow.add_edge(\"unknown_responder\", \"final_responder\")\n",
    "\n",
    "# 最終応答整形後、終了\n",
    "workflow.add_edge(\"final_responder\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "chat_history = []\n",
    "max_total_turns = 3 # このデモでの最大会話ターン数\n",
    "\n",
    "print(f\"--- Q&Aボット改善テスト (最大{max_total_turns}ターン) ---\")\n",
    "for turn in range(max_total_turns * 2): # ユーザー入力の機会を多めに用意\n",
    "    user_q = input(f\"You (Turn {turn//2 + 1}): \")\n",
    "    if user_q.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting chat.\")\n",
    "        break\n",
    "    \n",
    "    chat_history.append(HumanMessage(content=user_q))\n",
    "    inputs = {\n",
    "        \"messages\": chat_history,\n",
    "        \"max_turns\": max_total_turns \n",
    "        # conversation_turns は capture_input_and_increment_turn で初期化/更新\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        final_bot_state = graph.invoke(inputs, {\"recursion_limit\": 15})\n",
    "        if final_bot_state and final_bot_state.get(\"messages\"):\n",
    "            bot_response_message = final_bot_state[\"messages\"][-1]\n",
    "            if isinstance(bot_response_message, AIMessage):\n",
    "                print(f\"Bot: {bot_response_message.content}\")\n",
    "                chat_history.append(bot_response_message)\n",
    "            else: # ENDに直接到達した場合など、AIMessageがない場合\n",
    "                print(\"(Bot ended conversation due to max turns or other condition)\")\n",
    "                if final_bot_state.get(\"llm_response\"):\n",
    "                     print(f\"Bot (last intended response): {final_bot_state['llm_response']}\") # END直前の応答を表示試行\n",
    "                break \n",
    "        else:\n",
    "            print(\"(No response from bot or bot ended)\")\n",
    "            break\n",
    "\n",
    "        # conversation_turns が max_turns に達したか、それ以上ならループを抜ける\n",
    "        if final_bot_state.get(\"conversation_turns\", 0) >= final_bot_state.get(\"max_turns\", max_total_turns):\n",
    "            print(\"Max conversation turns reached from bot's perspective.\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during bot invocation: {e}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n--- Final Chat History ---\")\n",
    "for msg in chat_history:\n",
    "    print(f\"  {msg.type.upper()}: {msg.content}\")\n",
    "# ▲▲▲▲▲▲▲▲▲▲ YOUR CODE HERE ▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答例を見る</summary>\n",
    "\n",
    "``````python\n",
    "# 解答011\n",
    "from typing import TypedDict, Annotated, Literal, Optional\n",
    "import re\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "QuestionCategory = Literal[\"greeting\", \"knowledge_q\", \"opinion_q\", \"unknown_q\"]\n",
    "\n",
    "class AdvancedQABotState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_input: str # 最新のユーザー入力\n",
    "    question_category: Optional[QuestionCategory] # 質問のカテゴリ\n",
    "    llm_response: Optional[str] # LLMからの最終的な応答\n",
    "    conversation_turns: int # 会話のターン数\n",
    "    max_turns: int # 最大許容ターン数\n",
    "    error_message: Optional[str]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def capture_input_and_increment_turn(state: AdvancedQABotState):\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    # messages以外のキーは、invokeの入力で与えられなければ、前のターンの値が残る。\n",
    "    # conversation_turns は入力で初期値0を渡すか、ここで .get(key, default) を使う。\n",
    "    current_turns = state.get(\"conversation_turns\", 0) + 1\n",
    "    print(f\"capture_input_and_increment_turn: User: '{user_message}', Turn: {current_turns}\")\n",
    "    return {\n",
    "        \"user_input\": user_message,\n",
    "        \"conversation_turns\": current_turns,\n",
    "        \"question_category\": None, \n",
    "        \"llm_response\": None, \n",
    "        \"error_message\": None \n",
    "    }\n",
    "\n",
    "def categorize_question_node(state: AdvancedQABotState):\n",
    "    text = state[\"user_input\"].lower()\n",
    "    category: QuestionCategory = \"unknown_q\"\n",
    "    \n",
    "    if any(greet in text for greet in [\"こんにちは\", \"やあ\", \"どうも\", \"hello\", \"hi\"]):\n",
    "        category = \"greeting\"\n",
    "    elif any(q_word in text for q_word in [\"とは\", \"何ですか\", \"教えて\", \"なぜ\", \"what is\", \"tell me about\", \"why\"]) or \"?\" in text:\n",
    "        if any(opinion_word in text for opinion_word in [\"どう思う\", \"あなたの意見は\", \"what do you think about\"]):\n",
    "             category = \"opinion_q\"\n",
    "        else:\n",
    "            category = \"knowledge_q\"\n",
    "    \n",
    "    print(f\"categorize_question_node: Input '{text}' categorized as '{category}'\")\n",
    "    return {\"question_category\": category, \"messages\": [AIMessage(content=f\"Category determination: {category}\")]}\n",
    "\n",
    "def greeting_responder_node(state: AdvancedQABotState):\n",
    "    response = \"こんにちは！何かお手伝いできることはありますか？\"\n",
    "    print(f\"greeting_responder_node: Responding with '{response}'\")\n",
    "    return {\"llm_response\": response}\n",
    "\n",
    "def knowledge_llm_node(state: AdvancedQABotState):\n",
    "    question = state[\"user_input\"]\n",
    "    print(f\"knowledge_llm_node: Asking LLM (knowledge): '{question}'\")\n",
    "    response_obj = llm.invoke([HumanMessage(content=question)])\n",
    "    response_text = response_obj.content.strip()\n",
    "    print(f\"  LLM response: {response_text}\")\n",
    "    return {\"llm_response\": response_text}\n",
    "\n",
    "def opinion_llm_node(state: AdvancedQABotState):\n",
    "    question = state[\"user_input\"]\n",
    "    print(f\"opinion_llm_node: Asking LLM (opinion): '{question}'\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたは様々なトピックについて個人的な意見を述べることができるAIです。客観的な事実とあなたの意見を区別して話してください。\"),\n",
    "        (\"human\", \"{user_question}\")\n",
    "    ])\n",
    "    chain = prompt | llm\n",
    "    response_obj = chain.invoke({\"user_question\": question})\n",
    "    response_text = response_obj.content.strip()\n",
    "    print(f\"  LLM response: {response_text}\")\n",
    "    return {\"llm_response\": response_text}\n",
    "\n",
    "def unknown_question_node(state: AdvancedQABotState):\n",
    "    response = \"申し訳ありませんが、ご質問の意図を正確に理解できませんでした。もう少し具体的に、または別の言葉で質問していただけますでしょうか？\"\n",
    "    print(f\"unknown_question_node: Responding with '{response}'\")\n",
    "    return {\"llm_response\": response}\n",
    "\n",
    "def final_response_node(state: AdvancedQABotState):\n",
    "    final_resp = state.get(\"llm_response\", \"(AI did not generate a response for some reason)\")\n",
    "    print(f\"final_response_node: Final AI response to be added to messages: '{final_resp}'\")\n",
    "    return {\"messages\": [AIMessage(content=final_resp)]}\n",
    "\n",
    "# --- ルーター関数 ---\n",
    "def route_by_category(state: AdvancedQABotState):\n",
    "    category = state.get(\"question_category\")\n",
    "    print(f\"route_by_category: Routing based on category '{category}'\")\n",
    "    if category == \"greeting\": return \"greeting_responder\"\n",
    "    if category == \"knowledge_q\": return \"knowledge_llm\"\n",
    "    if category == \"opinion_q\": return \"opinion_llm\"\n",
    "    return \"unknown_responder\"\n",
    "\n",
    "def check_conversation_limit(state: AdvancedQABotState):\n",
    "    current_turns = state.get(\"conversation_turns\", 0)\n",
    "    max_t = state.get(\"max_turns\", 3) # デフォルトの最大ターン数を設定\n",
    "    if current_turns >= max_t:\n",
    "        print(f\"check_conversation_limit: Max turns ({max_t}) reached for this interaction. Ending conversation.\")\n",
    "        return \"__end__\" \n",
    "    print(f\"check_conversation_limit: Turn {current_turns}/{max_t}. Continuing to categorize.\")\n",
    "    return \"continue_to_categorizer\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(AdvancedQABotState)\n",
    "\n",
    "workflow.add_node(\"input_handler\", capture_input_and_increment_turn)\n",
    "workflow.add_node(\"categorizer\", categorize_question_node)\n",
    "workflow.add_node(\"greeting_responder\", greeting_responder_node)\n",
    "workflow.add_node(\"knowledge_llm\", knowledge_llm_node)\n",
    "workflow.add_node(\"opinion_llm\", opinion_llm_node)\n",
    "workflow.add_node(\"unknown_responder\", unknown_question_node)\n",
    "workflow.add_node(\"final_responder\", final_response_node)\n",
    "\n",
    "workflow.set_entry_point(\"input_handler\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"input_handler\",\n",
    "    check_conversation_limit,\n",
    "    {\n",
    "        \"continue_to_categorizer\": \"categorizer\",\n",
    "        \"__end__\": END \n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"categorizer\",\n",
    "    route_by_category,\n",
    "    {\n",
    "        \"greeting_responder\": \"greeting_responder\",\n",
    "        \"knowledge_llm\": \"knowledge_llm\",\n",
    "        \"opinion_llm\": \"opinion_llm\",\n",
    "        \"unknown_responder\": \"unknown_responder\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"greeting_responder\", \"final_responder\")\n",
    "workflow.add_edge(\"knowledge_llm\", \"final_responder\")\n",
    "workflow.add_edge(\"opinion_llm\", \"final_responder\")\n",
    "workflow.add_edge(\"unknown_responder\", \"final_responder\")\n",
    "workflow.add_edge(\"final_responder\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 (インタラクティブテスト) ---\n",
    "chat_history_for_bot = []\n",
    "max_dialogue_turns = 3 # このインタラクティブセッションでの最大会話往復数\n",
    "\n",
    "print(f\"--- 第1章まとめ Q&Aボット (最大{max_dialogue_turns}往復) ---\")\n",
    "print(\"チャットを開始します。'exit' または 'quit' で終了します。\")\n",
    "\n",
    "for i in range(max_dialogue_turns):\n",
    "    user_text = input(f\"あなた (Turn {i + 1}): \")\n",
    "    if user_text.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"チャットを終了します。\")\n",
    "        break\n",
    "    \n",
    "    chat_history_for_bot.append(HumanMessage(content=user_text))\n",
    "    \n",
    "    current_invoke_inputs = {\n",
    "        \"messages\": chat_history_for_bot,\n",
    "        \"max_turns\": max_dialogue_turns,\n",
    "    }\n",
    "    # 最初のターンのみ conversation_turns を明示的に0に設定\n",
    "    # それ以降はグラフ内でインクリメントされた値が引き継がれる\n",
    "    if i == 0:\n",
    "        current_invoke_inputs[\"conversation_turns\"] = 0\n",
    "\n",
    "    try:\n",
    "        final_bot_state = graph.invoke(current_invoke_inputs, {\"recursion_limit\": 25})\n",
    "\n",
    "        if final_bot_state and final_bot_state.get(\"messages\"):\n",
    "            ai_messages = [m for m in final_bot_state[\"messages\"] if isinstance(m, AIMessage)]\n",
    "            # final_responder が最後に追加したメッセージがボットの応答\n",
    "            # ただし、check_conversation_limit でENDに到達した場合は final_responder を通らない\n",
    "            if final_bot_state.get(\"__end__\") and not ai_messages[-1].content.startswith(\"Category determination:\"):\n",
    "                 # グラフがENDで終了したが、final_responderを通らなかった場合（max_turnsで終了）\n",
    "                 # AIMessageがmessagesに追加されていないので、llm_responseから取得試行\n",
    "                 bot_actual_response = final_bot_state.get(\"llm_response\", \"(最大ターン数に達したため、応答を生成せずに終了しました)\")\n",
    "            elif ai_messages:\n",
    "                 # 通常はfinal_responderが追加した最後のAIMessage\n",
    "                 bot_actual_response = ai_messages[-1].content\n",
    "            else:\n",
    "                bot_actual_response = \"(AIからの応答がありませんでした)\"\n",
    "\n",
    "            print(f\"AIボット: {bot_actual_response}\")\n",
    "            chat_history_for_bot.append(AIMessage(content=bot_actual_response))\n",
    "        else:\n",
    "            print(\"AIボット: (状態取得エラーまたは会話終了)\")\n",
    "            break\n",
    "\n",
    "        if final_bot_state.get(\"__end__\") or final_bot_state.get(\"conversation_turns\", 0) >= final_bot_state.get(\"max_turns\", max_dialogue_turns):\n",
    "             print(\"(最大会話ターン数に達したか、グラフが終了しました)\")\n",
    "             break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        break\n",
    "else: \n",
    "    print(\"\\n最大会話往復数に達したのでチャットを終了します。\")\n",
    "\n",
    "print(\"\\n--- 最終的なチャット履歴 (ボットとの対話) ---\")\n",
    "for msg in chat_history_for_bot:\n",
    "    print(f\"  {msg.type.upper()}: {msg.content}\")\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
