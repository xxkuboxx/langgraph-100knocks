{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1章: グラフの基本要素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "以下のセルを順番に実行して、演習に必要な環境をセットアップします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "\n",
    "このセルは、LangGraphおよび関連するLangChainライブラリをインストールします。実行には数分かかる場合があります。\n",
    "ご利用になるLLMプロバイダーに応じて、コメントアウトを解除して必要なライブラリをインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ライブラリのインストール ===\n",
    "# 基本ライブラリ (LangGraphとLangChain Core)\n",
    "!pip install -qU langchain langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLMプロバイダー別ライブラリ ---\n",
    "# ご利用になるLLMプロバイダーに応じて、以下の該当する行のコメントを解除して実行してください。\n",
    "\n",
    "# OpenAI (GPTシリーズ)\n",
    "# !pip install -qU langchain_openai\n",
    "\n",
    "# Azure OpenAI\n",
    "# !pip install -qU langchain_openai # Azureもlangchain_openaiを利用\n",
    "\n",
    "# Google Cloud Vertex AI (Gemini, PaLM等)\n",
    "# !pip install -qU langchain_google_vertexai\n",
    "\n",
    "# Google Gemini API (Google AI Studioで利用するGemini)\n",
    "# !pip install -qU langchain_google_genai\n",
    "\n",
    "# Anthropic (Claudeシリーズ)\n",
    "# !pip install -qU langchain_anthropic\n",
    "\n",
    "# Amazon Bedrock (AWS上の各種モデル、Claudeも含む)\n",
    "# !pip install -qU langchain_aws boto3 # Bedrock利用時はboto3も必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- その他の推奨ライブラリ ---\n",
    "# グラフの可視化や環境変数管理など、演習全体を通して利用する可能性のあるライブラリ\n",
    "!pip install -qU python-dotenv pygraphviz pydotplus graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMプロバイダーの選択\n",
    "\n",
    "このセルでは、使用するLLMプロバイダーを選択します。\n",
    "`LLM_PROVIDER` 変数に、利用したいプロバイダー名を設定してください。\n",
    "選択可能なプロバイダー: `\"openai\"`, `\"azure\"`, `\"google\"` (Vertex AI), `\"google_genai\"` (Gemini API), `\"anthropic\"`, `\"bedrock\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMプロバイダーの選択 ===\n",
    "# 利用したいLLMプロバイダーを以下の変数で指定してください。\n",
    "# \"openai\", \"azure\", \"google\" (Vertex AI), \"google_genai\" (Gemini API), \"anthropic\", \"bedrock\" のいずれかを選択できます。\n",
    "LLM_PROVIDER = \"openai\"  # 例: OpenAI を利用する場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIキー/環境変数の設定\n",
    "\n",
    "以下のセルを実行する前に、選択したLLMプロバイダーに応じたAPIキーまたは環境変数を設定する必要があります。\n",
    "\n",
    "**一般的な手順:**\n",
    "1.  `.env.sample` ファイルをコピーして `.env` ファイルを作成します。\n",
    "2.  `.env` ファイルを開き、選択したLLMプロバイダーに対応するAPIキーや必要な情報を記述します。\n",
    "    *   **OpenAI:** `OPENAI_API_KEY`\n",
    "    *   **Azure OpenAI:** `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `OPENAI_API_VERSION`, `AZURE_OPENAI_DEPLOYMENT_NAME`\n",
    "    *   **Google (Vertex AI):** `GOOGLE_CLOUD_PROJECT_ID`, `GOOGLE_CLOUD_LOCATION` (Colab環境外で実行する場合、`GOOGLE_APPLICATION_CREDENTIALS` 環境変数の設定も必要になることがあります)\n",
    "    *   **Google (Gemini API):** `GOOGLE_API_KEY`\n",
    "    *   **Anthropic:** `ANTHROPIC_API_KEY`\n",
    "    *   **AWS Bedrock:** `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION_NAME` (IAMロールを使用する場合は、これらのキー設定は不要な場合がありますが、リージョン名は必須です)\n",
    "3.  ファイルを保存します。\n",
    "\n",
    "**Google Colab を使用している場合:**\n",
    "上記の `.env` ファイルを使用する代わりに、Colabのシークレットマネージャーに必要なキーを登録してください。\n",
    "例えば、OpenAIを使用する場合は `OPENAI_API_KEY` という名前でシークレットを登録します。\n",
    "Vertex AI を利用する場合は、Colab上での認証 (`google.colab.auth.authenticate_user()`) が実行されます。\n",
    "\n",
    "このセルは、設定された情報に基づいて環境変数をロードし、LLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === APIキー/環境変数の設定 ===\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envファイルから環境変数を読み込む (存在する場合)\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# --- OpenAI ---\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY and IS_COLAB:\n",
    "        OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"OpenAI APIキーが設定されていません。環境変数 OPENAI_API_KEY を設定するか、Colab環境の場合はシークレットに OPENAI_API_KEY を設定してください。\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# --- Azure OpenAI ---\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not AZURE_OPENAI_API_KEY: AZURE_OPENAI_API_KEY = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not AZURE_OPENAI_ENDPOINT: AZURE_OPENAI_ENDPOINT = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        if not OPENAI_API_VERSION: OPENAI_API_VERSION = userdata.get(\"OPENAI_API_VERSION\") # 例: \"2023-07-01-preview\"\n",
    "        if not AZURE_OPENAI_DEPLOYMENT_NAME: AZURE_OPENAI_DEPLOYMENT_NAME = userdata.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if not AZURE_OPENAI_API_KEY: raise ValueError(\"Azure OpenAI APIキー (AZURE_OPENAI_API_KEY) が設定されていません。\")\n",
    "    if not AZURE_OPENAI_ENDPOINT: raise ValueError(\"Azure OpenAI エンドポイント (AZURE_OPENAI_ENDPOINT) が設定されていません。\")\n",
    "    if not OPENAI_API_VERSION: OPENAI_API_VERSION = \"2023-07-01-preview\" # デフォルトを設定することも可能\n",
    "    if not AZURE_OPENAI_DEPLOYMENT_NAME: raise ValueError(\"Azure OpenAI デプロイメント名 (AZURE_OPENAI_DEPLOYMENT_NAME) が設定されていません。\")\n",
    "\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "    os.environ[\"OPENAI_API_VERSION\"] = OPENAI_API_VERSION\n",
    "\n",
    "# --- Google Cloud Vertex AI (Gemini) ---\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT_ID\") # .env 用に修正\n",
    "    LOCATION = os.environ.get(\"GOOGLE_CLOUD_LOCATION\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not PROJECT_ID: PROJECT_ID = userdata.get(\"GOOGLE_CLOUD_PROJECT_ID\")\n",
    "        if not LOCATION: LOCATION = userdata.get(\"GOOGLE_CLOUD_LOCATION\") # 例: \"us-central1\"\n",
    "        from google.colab import auth as google_auth\n",
    "        google_auth.authenticate_user() # Vertex AI を使う場合は Colab での認証を推奨\n",
    "    else: # Colab外の場合、.envから読み込んだ値で環境変数を設定\n",
    "        if PROJECT_ID: os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID # Vertex AI SDKが参照する標準的な環境変数名\n",
    "        if LOCATION: os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "\n",
    "    if not PROJECT_ID: raise ValueError(\"Google Cloud Project ID が設定されていません。環境変数 GOOGLE_CLOUD_PROJECT_ID を設定するか、Colab環境の場合はシークレットに GOOGLE_CLOUD_PROJECT_ID を設定してください。\")\n",
    "    if not LOCATION: LOCATION = \"us-central1\" # デフォルトロケーション\n",
    "\n",
    "# --- Google Gemini API (langchain-google-genai) ---\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY and IS_COLAB:\n",
    "        GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"Google APIキーが設定されていません。環境変数 GOOGLE_API_KEY を設定するか、Colab環境の場合はシークレットに GOOGLE_API_KEY を設定してください。\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# --- Anthropic (Claude) ---\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY and IS_COLAB:\n",
    "        ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        raise ValueError(\"Anthropic APIキーが設定されていません。環境変数 ANTHROPIC_API_KEY を設定するか、Colab環境の場合はシークレットに ANTHROPIC_API_KEY を設定してください。\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "# --- Amazon Bedrock (Claude) ---\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    AWS_REGION_NAME = os.environ.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if IS_COLAB: \n",
    "        if not AWS_ACCESS_KEY_ID: AWS_ACCESS_KEY_ID = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
    "        if not AWS_SECRET_ACCESS_KEY: AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        if not AWS_REGION_NAME: AWS_REGION_NAME = userdata.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if not AWS_REGION_NAME:\n",
    "         raise ValueError(\"AWSリージョン名 (AWS_REGION_NAME) が設定されていません。Bedrock利用にはリージョン指定が必要です。\")\n",
    "\n",
    "    # 環境変数に設定 (boto3がこれらを自動で読み込む)\n",
    "    if AWS_ACCESS_KEY_ID: os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "    if AWS_SECRET_ACCESS_KEY: os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION_NAME # boto3が参照する標準的なリージョン環境変数名\n",
    "    os.environ[\"AWS_REGION\"] = AWS_REGION_NAME # いくつかのライブラリはこちらを参照することもある\n",
    "\n",
    "print(f\"APIキー/環境変数の設定完了 (プロバイダー: {LLM_PROVIDER})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMクライアントの初期化\n",
    "\n",
    "このセルは、上で選択・設定したLLMプロバイダーに基づいて、対応するLLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMクライアントの動的初期化 ===\n",
    "llm = None\n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), # 環境変数から取得\n",
    "        openai_api_version=os.environ.get(\"OPENAI_API_VERSION\"), # 環境変数から取得\n",
    "        temperature=0,\n",
    "    )\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    from langchain_google_vertexai import ChatVertexAI\n",
    "    # PROJECT_ID, LOCATION は前のセルで環境変数に設定済みか、Colabの場合は直接利用\n",
    "    llm = ChatVertexAI(model_name=\"gemini-2.0-flash\", temperature=0, project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"), location=os.environ.get(\"GOOGLE_CLOUD_LOCATION\"))\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0, convert_system_message_to_human=True)\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    from langchain_anthropic import ChatAnthropic\n",
    "    llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    from langchain_aws import ChatBedrock # langchain_community.chat_models から langchain_aws に変更の可能性あり\n",
    "    # AWS_REGION_NAME は前のセルで環境変数 AWS_DEFAULT_REGION に設定済み\n",
    "    llm = ChatBedrock( # BedrockChat ではなく ChatBedrock が一般的\n",
    "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        # region_name=os.environ.get(\"AWS_DEFAULT_REGION\"), # 通常、boto3が環境変数から自動で読み込む\n",
    "        model_kwargs={\"temperature\": 0},\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Unsupported LLM_PROVIDER: {LLM_PROVIDER}. \"\n",
    "        \"Please choose from 'openai', 'azure', 'google', 'google_genai', 'anthropic', or 'bedrock'.\"\n",
    "    )\n",
    "\n",
    "print(f\"LLM Provider: {LLM_PROVIDER}\")\n",
    "if llm:\n",
    "    print(f\"LLM Client Type: {type(llm)}\")\n",
    "    # モデル名取得の試行を汎用的に\n",
    "    model_attr = (\n",
    "                 getattr(llm, 'model', None) or\n",
    "                 getattr(llm, 'model_name', None) or\n",
    "                 getattr(llm, 'model_id', None) or\n",
    "                 (hasattr(llm, 'llm') and getattr(llm.llm, 'model', None)) # 一部のLLMクライアントのネスト構造に対応\n",
    "    )\n",
    "    if hasattr(llm, 'azure_deployment') and not model_attr: # Azure特有の属性\n",
    "        model_attr = llm.azure_deployment\n",
    "        \n",
    "    if model_attr:\n",
    "        print(f\"LLM Model: {model_attr}\")\n",
    "    else:\n",
    "        print(\"LLM Model: (Could not determine model name from client attributes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題001: 最小構成のLangGraphグラフの構築\n",
    "\n",
    "LangGraphの最も基本的な構成要素である`StateGraph`と`State`を理解し、シンプルなグラフを構築してみましょう。この問題では、入力された文字列をそのまま出力するだけの、単一のノードを持つグラフを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, ____]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def simple_node(state: GraphState):\n",
    "    print(f'simple_node: {state[\"messages\"][-1].content}')\n",
    "    return {\"messages\": [state[\"messages\"][-1]]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.____(\"simple_node\", simple_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.____(\"simple_node\")\n",
    "\n",
    "# 終了ポイントの設定\n",
    "workflow.____(\"simple_node\", ____)\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.____()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs = {\"messages\": [(\"user\", \"Hello, LangGraph!\")]}\n",
    "for s in app.____(inputs):\n",
    "    print(s)\n",
    "\n",
    "# 最終結果の確認\n",
    "final_state = app.____(inputs)\n",
    "print(f\"Final State: {final_state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答001</summary>\n",
    "\n",
    "``````python\n",
    "# 解答001\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    # グラフの状態を保持する辞書\n",
    "    # ここでは、入力メッセージを保持する\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def simple_node(state: GraphState):\n",
    "    # 入力されたメッセージをそのまま返すノード\n",
    "    print(f'simple_node: {state[\"messages\"][-1].content}')\n",
    "    return {\"messages\": [state[\"messages\"][-1]]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"simple_node\", simple_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"simple_node\")\n",
    "\n",
    "# 終了ポイントの設定\n",
    "workflow.add_edge(\"simple_node\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs = {\"messages\": [(\"user\", \"Hello, LangGraph!\")]}\n",
    "for s in app.stream(inputs):\n",
    "    print(s)\n",
    "\n",
    "# 最終結果の確認\n",
    "final_state = app.invoke(inputs)\n",
    "print(f\"Final State: {final_state}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説001</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** この問題では、`StateGraph`、`TypedDict`を用いた`State`の定義、`add_node`、`set_entry_point`、`add_edge`、`END`といったLangGraphの最も基本的なAPIを学びます。また、`Annotated`と`add_messages`を使ってメッセージ履歴を管理する方法も理解します。\n",
    "*   **コード解説:**\n",
    "    *   `GraphState`は、グラフ全体で共有される状態を定義します。`TypedDict`を使うことで、状態のスキーマを明確にできます。`messages: Annotated[list, add_messages]`は、LangChainのメッセージ形式のリストを状態として持ち、新しいメッセージが追加されるたびに自動的にリストの末尾に追加されるように設定しています。\n",
    "    *   `simple_node`関数は、グラフのノードとして機能します。`state`引数として現在のグラフの状態を受け取り、新しい状態を辞書として返します。ここでは、入力された最後のメッセージをそのまま返しています。\n",
    "    *   `StateGraph(GraphState)`でグラフのインスタンスを作成し、`GraphState`で定義した状態スキーマを渡します。\n",
    "    *   `workflow.add_node(\"simple_node\", simple_node)`で、`simple_node`関数を`simple_node`という名前のノードとしてグラフに追加します。\n",
    "    *   `workflow.set_entry_point(\"simple_node\")`は、グラフの実行が開始される最初のノードを指定します。\n",
    "    *   `workflow.add_edge(\"simple_node\", END)`は、`simple_node`の実行が完了したらグラフを終了することを示します。`END`はLangGraphが提供する特別な終了ノードです。\n",
    "    *   `app = workflow.compile()`で、定義したワークフローを実行可能なアプリケーションにコンパイルします。\n",
    "    *   `app.stream(inputs)`は、グラフの実行過程をストリーミングで受け取ることができます。`app.invoke(inputs)`は、グラフの実行が完了した最終状態を返します。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題002: 複数のノードを持つシーケンシャルグラフの構築\n",
    "\n",
    "前の問題で学んだ基本的なグラフ構築に加えて、複数のノードを直列に接続し、データがノード間をどのように流れるかを理解しましょう。ここでは、入力された文字列を加工する2つのノード（例：大文字化、逆順化）を持つグラフを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, ____]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def uppercase_node(state: GraphState):\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"uppercase_node: {last_message_content}\")\n",
    "    return {\"messages\": [____(content=last_message_content.upper())]}\n",
    "\n",
    "def reverse_node(state: GraphState):\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"reverse_node: {last_message_content}\")\n",
    "    return {\"messages\": [____(content=last_message_content[::-1])]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.____(\"uppercase\", uppercase_node)\n",
    "workflow.____(\"reverse\", reverse_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.____(\"uppercase\")\n",
    "\n",
    "# エッジの追加 (直列接続)\n",
    "workflow.____(\"uppercase\", \"reverse\")\n",
    "workflow.____(\"reverse\", ____)\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.____()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs = {\"messages\": [____(content=\"Hello LangGraph\")]}\n",
    "for s in app.____(inputs):\n",
    "    print(s)\n",
    "\n",
    "final_state = app.____(inputs)\n",
    "print(f\"Final State: {final_state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答002</summary>\n",
    "\n",
    "``````python\n",
    "# 解答002\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def uppercase_node(state: GraphState):\n",
    "    # 最新のメッセージを大文字に変換するノード\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"uppercase_node: {last_message_content}\")\n",
    "    return {\"messages\": [AIMessage(content=last_message_content.upper())]}\n",
    "\n",
    "def reverse_node(state: GraphState):\n",
    "    # 最新のメッセージを逆順にするノード\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"reverse_node: {last_message_content}\")\n",
    "    return {\"messages\": [AIMessage(content=last_message_content[::-1])]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"uppercase\", uppercase_node)\n",
    "workflow.add_node(\"reverse\", reverse_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"uppercase\")\n",
    "\n",
    "# エッジの追加 (直列接続)\n",
    "workflow.add_edge(\"uppercase\", \"reverse\")\n",
    "workflow.add_edge(\"reverse\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Hello LangGraph\")]}\n",
    "for s in app.stream(inputs):\n",
    "    print(s)\n",
    "\n",
    "final_state = app.invoke(inputs)\n",
    "print(f\"Final State: {final_state}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説002</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** 複数のノードを`add_edge`で直列に接続する方法と、ノード間で状態がどのように引き継がれるかを学びます。`HumanMessage`と`AIMessage`を使って、メッセージの送信元を明示する方法も理解します。\n",
    "*   **コード解説:**\n",
    "    *   `uppercase_node`と`reverse_node`は、それぞれ入力メッセージを大文字化、逆順化する処理を行います。重要なのは、各ノードが新しい`AIMessage`を作成して状態に返す点です。これにより、次のノードは前のノードの処理結果を`state[\"messages\"][-1]`で取得できます。\n",
    "    *   `workflow.add_edge(\"uppercase\", \"reverse\")`は、`uppercase`ノードの実行が完了したら、次に`reverse`ノードを実行するように指示します。このようにして、処理の流れを定義します。\n",
    "    *   入力メッセージを`HumanMessage`として渡すことで、ユーザーからの入力であることを明示しています。ノードからの出力は`AIMessage`として返され、メッセージ履歴にAIの応答として記録されます。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題003: 条件付きエッジによる分岐の導入\n",
    "\n",
    "LangGraphの強力な機能の一つである条件付きエッジを導入し、グラフの実行パスを動的に制御する方法を学びましょう。ここでは、入力された数値が偶数か奇数かによって、異なる処理を行うグラフを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, ____]\n",
    "    number: int # 新たに数値を保持する状態を追加\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def check_number(state: GraphState):\n",
    "    # 入力メッセージから数値を抽出し、状態に保存するノード\n",
    "    try:\n",
    "        num = int(state[\"messages\"][-1].content)\n",
    "        print(f\"check_number: Extracted number {num}\")\n",
    "        return {\"number\": num}\n",
    "    except ValueError:\n",
    "        print(\"check_number: Invalid input, not a number.\")\n",
    "        return {\"number\": 0} # エラー時は0として扱うか、適切なエラーハンドリングを実装\n",
    "\n",
    "def even_node(state: GraphState):\n",
    "    # 偶数だった場合の処理ノード\n",
    "    print(f\"even_node: Number {state[\"number\"]} is even.\")\n",
    "    return {\"messages\": [____(content=f\"The number {state[\"number\"]} is even.\")]}\n",
    "\n",
    "def odd_node(state: GraphState):\n",
    "    # 奇数だった場合の処理ノード\n",
    "    print(f\"odd_node: Number {state[\"number\"]} is odd.\")\n",
    "    return {\"messages\": [____(content=f\"The number {state[\"number\"]} is odd.\")]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def route_number(state: GraphState):\n",
    "    # 数値の状態に基づいて次のノードを決定する\n",
    "    if state[\"number\"] % 2 == 0:\n",
    "        print(\"Routing to even_node\")\n",
    "        return \"even_node\"\n",
    "    else:\n",
    "        print(\"Routing to odd_node\")\n",
    "        return \"odd_node\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.____(\"check_number\", check_number)\n",
    "workflow.____(\"even_node\", even_node)\n",
    "workflow.____(\"odd_node\", odd_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.____(\"check_number\")\n",
    "\n",
    "# 条件付きエッジの追加\n",
    "workflow.____(\n",
    "    \"check_number\", # 遷移元のノード\n",
    "    ____,   # ルーター関数\n",
    "    {\n",
    "        \"even_node\": \"even_node\", # ルーター関数の戻り値とノード名のマッピング\n",
    "        \"odd_node\": \"odd_node\"\n",
    "    }\n",
    " )\n",
    "\n",
    "# 各分岐からの終了エッジ\n",
    "workflow.____(\"even_node\", ____)\n",
    "workflow.____(\"odd_node\", ____)\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.____()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- 偶数のテスト ---\")\n",
    "inputs_even = {\"messages\": [____(content=\"42\")]}\n",
    "for s in app.____(inputs_even):\n",
    "    print(s)\n",
    "final_state_even = app.____(inputs_even)\n",
    "print(f\"Final State (Even): {final_state_even}\")\n",
    "\n",
    "print(\"\\n--- 奇数のテスト ---\")\n",
    "inputs_odd = {\"messages\": [____(content=\"77\")]}\n",
    "for s in app.____(inputs_odd):\n",
    "    print(s)\n",
    "final_state_odd = app.____(inputs_odd)\n",
    "print(f\"Final State (Odd): {final_state_odd}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答003</summary>\n",
    "\n",
    "``````python\n",
    "# 解答003\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    number: int # 新たに数値を保持する状態を追加\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def check_number(state: GraphState):\n",
    "    # 入力メッセージから数値を抽出し、状態に保存するノード\n",
    "    try:\n",
    "        num = int(state[\"messages\"][-1].content)\n",
    "        print(f\"check_number: Extracted number {num}\")\n",
    "        return {\"number\": num}\n",
    "    except ValueError:\n",
    "        print(\"check_number: Invalid input, not a number.\")\n",
    "        return {\"number\": 0} # エラー時は0として扱うか、適切なエラーハンドリングを実装\n",
    "\n",
    "def even_node(state: GraphState):\n",
    "    # 偶数だった場合の処理ノード\n",
    "    print(f\"even_node: Number {state[\"number\"]} is even.\")\n",
    "    return {\"messages\": [AIMessage(content=f\"The number {state[\"number\"]} is even.\")]}\n",
    "\n",
    "def odd_node(state: GraphState):\n",
    "    # 奇数だった場合の処理ノード\n",
    "    print(f\"odd_node: Number {state[\"number\"]} is odd.\")\n",
    "    return {\"messages\": [AIMessage(content=f\"The number {state[\"number\"]} is odd.\")]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def route_number(state: GraphState):\n",
    "    # 数値の状態に基づいて次のノードを決定する\n",
    "    if state[\"number\"] % 2 == 0:\n",
    "        print(\"Routing to even_node\")\n",
    "        return \"even_node\"\n",
    "    else:\n",
    "        print(\"Routing to odd_node\")\n",
    "        return \"odd_node\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"check_number\", check_number)\n",
    "workflow.add_node(\"even_node\", even_node)\n",
    "workflow.add_node(\"odd_node\", odd_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"check_number\")\n",
    "\n",
    "# 条件付きエッジの追加\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_number\", # 遷移元のノード\n",
    "    route_number,   # ルーター関数\n",
    "    {\n",
    "        \"even_node\": \"even_node\", # ルーター関数の戻り値とノード名のマッピング\n",
    "        \"odd_node\": \"odd_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 各分岐からの終了エッジ\n",
    "workflow.add_edge(\"even_node\", END)\n",
    "workflow.add_edge(\"odd_node\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- 偶数のテスト ---\")\n",
    "inputs_even = {\"messages\": [HumanMessage(content=\"42\")]}\n",
    "for s in app.stream(inputs_even):\n",
    "    print(s)\n",
    "final_state_even = app.invoke(inputs_even)\n",
    "print(f\"Final State (Even): {final_state_even}\")\n",
    "\n",
    "print(\"\\n--- 奇数のテスト ---\")\n",
    "inputs_odd = {\"messages\": [HumanMessage(content=\"77\")]}\n",
    "for s in app.stream(inputs_odd):\n",
    "    print(s)\n",
    "final_state_odd = app.invoke(inputs_odd)\n",
    "print(f\"Final State (Odd): {final_state_odd}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説003</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** `add_conditional_edges`を使用して、グラフの実行パスを動的に制御する方法を学びます。ルーター関数がどのように次のノードを決定するのか、そして状態が分岐間でどのように共有されるかを理解します。\n",
    "*   **コード解説:**\n",
    "    *   `GraphState`に`number`という新しいキーを追加し、入力された数値を保持するようにしました。\n",
    "    *   `check_number`ノードは、入力メッセージから数値を抽出し、`number`状態を更新します。\n",
    "    *   `even_node`と`odd_node`は、それぞれ偶数と奇数だった場合の最終処理を行います。\n",
    "    *   `route_number`関数がルーターとして機能します。この関数は現在の`state`を受け取り、次に実行すべきノードの名前（文字列）を返します。LangGraphは、この戻り値に基づいて適切なエッジを辿ります。\n",
    "    *   `workflow.add_conditional_edges(\"check_number\", route_number, {\"even_node\": \"even_node\", \"odd_node\": \"odd_node\"})`は、`check_number`ノードの後に`route_number`関数を実行し、その戻り値が`\"even_node\"`なら`even_node`へ、`\"odd_node\"`なら`odd_node`へ遷移するように設定しています。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題004: グラフ内でのLLMの利用（シンプルなチャットボット）\n",
    "\n",
    "LangGraphのノード内で大規模言語モデル（LLM）を呼び出す方法を学び、シンプルなチャットボットを構築しましょう。ここでは、ユーザーからの入力に対してLLMが応答を生成し、その応答を返すグラフを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import os\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "# (from langchain_openai import ChatOpenAI や llm = ChatOpenAI(...) といった行はここには不要)\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def llm_node(state: GraphState):\n",
    "    # LLMを呼び出し、応答を生成するノード\n",
    "    # ノートブック冒頭で初期化された共通の `llm` 変数を使用します。\n",
    "    print(f\"llm_node: Calling LLM with messages: {state['messages']}\")\n",
    "    response = llm.invoke(state[\"messages\"]) # 共通llmを使用 (ここは歯抜けにしない)\n",
    "    print(f\"llm_node: LLM response: {response.content}\")\n",
    "    return {\"messages\": [response]} # responseはAIMessageオブジェクトを期待 (ここは歯抜けにしない)\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(GraphState) # StateGraph\n",
    "\n",
    "# ノードの追加\n",
    "workflow.____(\"llm_responder\", llm_node) # add_node\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.____(\"llm_responder\") # set_entry_point\n",
    "\n",
    "# 終了ポイントの設定\n",
    "workflow.____(\"llm_responder\", ____) # add_edge, END\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.____() # compile\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- チャットボットのテスト ---\")\n",
    "# 最初のメッセージはHumanMessageであると想定\n",
    "inputs = {\"messages\": [____(content=\"こんにちは、あなたの名前は何ですか？\")]} # HumanMessage\n",
    "for s in app.____(inputs): # stream\n",
    "    print(s)\n",
    "\n",
    "final_state = app.____(inputs) # invoke\n",
    "print(f\"Final State: {final_state}\")\n",
    "\n",
    "print(\"\\n--- 別の質問 ---\")\n",
    "inputs2 = {\"messages\": [____(content=\"今日の天気は？\")]} # HumanMessage\n",
    "for s in app.____(inputs2): # stream\n",
    "    print(s)\n",
    "\n",
    "final_state2 = app.____(inputs2) # invoke\n",
    "print(f\"Final State: {final_state2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答004</summary>\n",
    "\n",
    "``````python\n",
    "# 解答004\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import os # osはAPIキー設定のコメントアウト部分で使われているので残しても良いが、直接は不要になる\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def llm_node(state: GraphState):\n",
    "    # LLMを呼び出し、応答を生成するノード\n",
    "    # ノートブック冒頭で初期化された共通の `llm` 変数を使用します。\n",
    "    print(f\"llm_node: Calling LLM with messages: {state['messages']}\")\n",
    "    response = llm.invoke(state[\"messages\"]) # 共通llmを使用\n",
    "    print(f\"llm_node: LLM response: {response.content}\")\n",
    "    return {\"messages\": [response]} # responseはAIMessageオブジェクトを期待\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"llm_responder\", llm_node)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"llm_responder\")\n",
    "\n",
    "# 終了ポイントの設定\n",
    "workflow.add_edge(\"llm_responder\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- チャットボットのテスト ---\")\n",
    "# 最初のメッセージはHumanMessageであると想定\n",
    "inputs = {\"messages\": [HumanMessage(content=\"こんにちは、あなたの名前は何ですか？\")]}\n",
    "for s in app.stream(inputs):\n",
    "    print(s)\n",
    "\n",
    "final_state = app.invoke(inputs)\n",
    "print(f\"Final State: {final_state}\")\n",
    "\n",
    "print(\"\\n--- 別の質問 ---\")\n",
    "inputs2 = {\"messages\": [HumanMessage(content=\"今日の天気は？\")]}\n",
    "for s in app.stream(inputs2):\n",
    "    print(s)\n",
    "\n",
    "final_state2 = app.invoke(inputs2)\n",
    "print(f\"Final State: {final_state2}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説004</summary>\n",
    "\n",
    "このノートブックでは、様々なLLMプラットフォーム（OpenAI, Azure OpenAI, Google Cloud Vertex AI, Google Gemini (Gemini API), Anthropic Claude, Amazon Bedrockなど）を簡単に切り替えて試せるように設計されています。\n",
    "ノートブックの冒頭にある `LLM_PROVIDER` 変数で使用したいLLMを選択し、対応するAPIキーや環境変数を設定するだけで、この問題を含む全てのLLM呼び出し箇所で選択したLLMが利用されます。\n",
    "選択した `LLM_PROVIDER` に応じて、必要なAPIキーが設定されているか（環境変数またはGoogle Colabのシークレット経由）、ノートブック起動時にチェックされます。\n",
    "\n",
    "ここでは、ノートブックの先頭で設定・初期化された共通の `llm` 変数を使用して、LLMに質問をしています。\n",
    "`llm.invoke()` という統一されたインターフェースで、どのLLMプロバイダーを利用しているかに関わらず、同じようにLLMを呼び出すことができます。\n",
    "これにより、特定のLLMサービスに依存しない、より汎用的なコードを作成するメリットを手軽に体験できます。\n",
    "\n",
    "もしエラーが発生した場合は、ノートブック冒頭の `LLM_PROVIDER` の設定、および選択したプロバイダーに応じたAPIキーや環境変数の設定（例: `OPENAI_API_KEY`, `GOOGLE_API_KEY`, `AZURE_OPENAI_ENDPOINT`など）が正しく行われているかを確認してください。\n",
    "各プロバイダー固有の設定項目（例えばVertex AIのProject ID、AzureのDeployment Name、Bedrockのリージョンなど）も見直してください。\n",
    "プロバイダーによっては、`pip install` コマンドで対応するライブラリ (例: `langchain-google-genai`) がインストールされているかも確認点です。\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題005: グラフの可視化とデバッグ\n",
    "\n",
    "構築したLangGraphグラフの構造を視覚的に確認し、デバッグに役立てる方法を学びましょう。ここでは、これまでに作成したグラフのいずれか（例：問題003の条件分岐グラフ）を可視化し、その構造を理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, ____]\n",
    "    number: int\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def check_number(state: GraphState):\n",
    "    try:\n",
    "        num = int(state[\"messages\"][-1].content)\n",
    "        return {\"number\": num}\n",
    "    except ValueError:\n",
    "        return {\"number\": 0}\n",
    "\n",
    "def even_node(state: GraphState):\n",
    "    return {\"messages\": [____(content=f\"The number {state[\"number\"]} is even.\")]}\n",
    "\n",
    "def odd_node(state: GraphState):\n",
    "    return {\"messages\": [____(content=f\"The number {state[\"number\"]} is odd.\")]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def route_number(state: GraphState):\n",
    "    if state[\"number\"] % 2 == 0:\n",
    "        return \"even_node\"\n",
    "    else:\n",
    "        return \"odd_node\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(GraphState)\n",
    "\n",
    "workflow.____(\"check_number\", check_number)\n",
    "workflow.____(\"even_node\", even_node)\n",
    "workflow.____(\"odd_node\", odd_node)\n",
    "\n",
    "workflow.____(\"check_number\")\n",
    "\n",
    "workflow.____(\n",
    "    \"check_number\",\n",
    "    ____,\n",
    "    {\n",
    "        \"even_node\": \"even_node\",\n",
    "        \"odd_node\": \"odd_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.____(\"even_node\", ____)\n",
    "workflow.____(\"odd_node\", ____)\n",
    "\n",
    "app = workflow.____()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "# グラフを画像として表示\n",
    "# graphvizがインストールされている必要があります: pip install pygraphviz pydotplus graphviz\n",
    "# また、システムにGraphvizがインストールされている必要があります。\n",
    "# Windows: https://graphviz.org/download/\n",
    "# Mac: brew install graphviz\n",
    "# Linux: sudo apt-get install graphviz\n",
    "try:\n",
    "    ____(____(app.____().____()))\n",
    "    print(\"グラフが正常に可視化されました。\")\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの実行と結果表示 (オプション) ---\n",
    "# 可視化したグラフが正しく動作するか確認するために、再度実行してみる\n",
    "print(\"\\n--- 偶数のテスト (可視化後の確認) ---\")\n",
    "inputs_even = {\"messages\": [____(content=\"10\")]}\n",
    "for s in app.____(inputs_even):\n",
    "    print(s)\n",
    "final_state_even = app.____(inputs_even)\n",
    "print(f\"Final State: {final_state_even}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答005</summary>\n",
    "\n",
    "``````python\n",
    "# 解答005\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    number: int\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def check_number(state: GraphState):\n",
    "    try:\n",
    "        num = int(state[\"messages\"][-1].content)\n",
    "        return {\"number\": num}\n",
    "    except ValueError:\n",
    "        return {\"number\": 0}\n",
    "\n",
    "def even_node(state: GraphState):\n",
    "    return {\"messages\": [AIMessage(content=f\"The number {state[\"number\"]} is even.\")]}\n",
    "\n",
    "def odd_node(state: GraphState):\n",
    "    return {\"messages\": [AIMessage(content=f\"The number {state[\"number\"]} is odd.\")]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def route_number(state: GraphState):\n",
    "    if state[\"number\"] % 2 == 0:\n",
    "        return \"even_node\"\n",
    "    else:\n",
    "        return \"odd_node\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"check_number\", check_number)\n",
    "workflow.add_node(\"even_node\", even_node)\n",
    "workflow.add_node(\"odd_node\", odd_node)\n",
    "\n",
    "workflow.set_entry_point(\"check_number\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_number\",\n",
    "    route_number,\n",
    "    {\n",
    "        \"even_node\": \"even_node\",\n",
    "        \"odd_node\": \"odd_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"even_node\", END)\n",
    "workflow.add_edge(\"odd_node\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの可視化 ---\n",
    "# グラフを画像として表示\n",
    "# graphvizがインストールされている必要があります: pip install pygraphviz pydotplus graphviz\n",
    "# また、システムにGraphvizがインストールされている必要があります。\n",
    "# Windows: https://graphviz.org/download/\n",
    "# Mac: brew install graphviz\n",
    "# Linux: sudo apt-get install graphviz\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "    print(\"グラフが正常に可視化されました。\")\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")\n",
    "\n",
    "# --- グラフの実行と結果表示 (オプション) ---\n",
    "# 可視化したグラフが正しく動作するか確認するために、再度実行してみる\n",
    "print(\"\\n--- 偶数のテスト (可視化後の確認) ---\")\n",
    "inputs_even = {\"messages\": [HumanMessage(content=\"10\")]}\n",
    "for s in app.stream(inputs_even):\n",
    "    print(s)\n",
    "final_state_even = app.invoke(inputs_even)\n",
    "print(f\"Final State: {final_state_even}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説005</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** `app.get_graph().draw_png()`を使用してLangGraphのグラフ構造を画像として可視化する方法を学びます。これにより、複雑なグラフのデバッグや理解が容易になります。\n",
    "*   **コード解説:**\n",
    "    *   この問題では、問題003で作成した条件分岐グラフを再利用しています。これは、可視化の有用性を示すのに適した例だからです。\n",
    "    *   `app.get_graph()`は、コンパイルされたグラフの内部表現を取得します。\n",
    "    *   `.draw_png()`メソッドは、そのグラフ構造をPNG画像としてバイト列で返します。この機能を利用するには、システムにGraphvizがインストールされている必要があります。また、Pythonの`pygraphviz`や`pydotplus`といったライブラリも必要になる場合があります。\n",
    "    *   `IPython.display.Image`と`display`を使うことで、Jupyter Notebook内で直接画像をレンダリングして表示できます。\n",
    "    *   `try-except`ブロックでGraphvizのインストール状況によるエラーをハンドリングし、ユーザーに適切なメッセージを表示するようにしています。グラフが複雑になるにつれて、この可視化機能はデバッグや設計の確認に不可欠となります。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題006: 状態の更新 - 特定キーの値を上書きする\n",
    "\n",
    "`add_messages` によるメッセージ履歴の追加だけでなく、グラフの状態(`State`)内の特定のキーの値を直接更新する方法を学びましょう。ここでは、カウンター値を保持する状態キーを定義し、ノードでその値をインクリメントするグラフを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄006\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class CounterState(TypedDict):\n",
    "    messages: Annotated[list, ____] # add_messages\n",
    "    counter: int # 新しくカウンター用の状態キーを定義\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def increment_counter(state: CounterState):\n",
    "    # counterの値を1増やすノード\n",
    "    current_count = state.get(\"counter\", 0) # stateからcounterの値を取得、なければ0\n",
    "    new_count = current_count + 1\n",
    "    print(f\"increment_counter: Current count: {current_count}, New count: {new_count}\")\n",
    "    return {\"counter\": ____, \"messages\": [HumanMessage(content=f\"Counter incremented to {new_count}\")]} # new_count\n",
    "\n",
    "def display_count(state: CounterState):\n",
    "    # counterの最終値を表示するノード (実際にはmessagesに追加されたもので確認)\n",
    "    print(f\"display_count: Final counter value is {state['counter']}\")\n",
    "    # このノードは状態を更新しないが、メッセージを追加しても良い\n",
    "    return {\"messages\": [HumanMessage(content=f\"Final count: {state['counter']}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(CounterState) # StateGraph\n",
    "\n",
    "# ノードの追加\n",
    "workflow.____(\"increment\", increment_counter) # add_node\n",
    "workflow.____(\"display\", display_count) # add_node\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.____(\"increment\") # set_entry_point\n",
    "\n",
    "# エッジの追加\n",
    "workflow.____(\"increment\", \"display\") # add_edge\n",
    "workflow.____(\"display\", ____) # add_edge, END\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.____() # compile\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- カウンターテスト (初期値0から) ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Start counting\")], \"counter\": 0} # 初期カウンター値を設定\n",
    "for s in app.____(inputs): # stream\n",
    "    print(s)\n",
    "final_state = app.____(inputs) # invoke\n",
    "print(f\"Final State: {final_state}\")\n",
    "\n",
    "print(\"\\n--- カウンターテスト (初期値5から) ---\")\n",
    "inputs_2 = {\"messages\": [HumanMessage(content=\"Start counting from 5\")], \"counter\": 5} # 初期カウンター値を設定\n",
    "for s in app.____(inputs_2): # stream\n",
    "    print(s)\n",
    "final_state_2 = app.____(inputs_2) # invoke\n",
    "print(f\"Final State: {final_state_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答006</summary>\n",
    "\n",
    "``````python\n",
    "# 解答006\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class CounterState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    counter: int # 新しくカウンター用の状態キーを定義\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def increment_counter(state: CounterState):\n",
    "    # counterの値を1増やすノード\n",
    "    current_count = state.get(\"counter\", 0) # stateからcounterの値を取得、なければ0\n",
    "    new_count = current_count + 1\n",
    "    print(f\"increment_counter: Current count: {current_count}, New count: {new_count}\")\n",
    "    return {\"counter\": new_count, \"messages\": [HumanMessage(content=f\"Counter incremented to {new_count}\")]}\n",
    "\n",
    "def display_count(state: CounterState):\n",
    "    # counterの最終値を表示するノード (実際にはmessagesに追加されたもので確認)\n",
    "    print(f\"display_count: Final counter value is {state['counter']}\")\n",
    "    # このノードは状態を更新しないが、メッセージを追加しても良い\n",
    "    return {\"messages\": [HumanMessage(content=f\"Final count: {state['counter']}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(CounterState)\n",
    "\n",
    "# ノードの追加\n",
    "workflow.add_node(\"increment\", increment_counter)\n",
    "workflow.add_node(\"display\", display_count)\n",
    "\n",
    "# エントリポイントの設定\n",
    "workflow.set_entry_point(\"increment\")\n",
    "\n",
    "# エッジの追加\n",
    "workflow.add_edge(\"increment\", \"display\")\n",
    "workflow.add_edge(\"display\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- カウンターテスト (初期値0から) ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Start counting\")], \"counter\": 0} # 初期カウンター値を設定\n",
    "for s in app.stream(inputs):\n",
    "    print(s)\n",
    "final_state = app.invoke(inputs)\n",
    "print(f\"Final State: {final_state}\")\n",
    "\n",
    "print(\"\\n--- カウンターテスト (初期値5から) ---\")\n",
    "inputs_2 = {\"messages\": [HumanMessage(content=\"Start counting from 5\")], \"counter\": 5} # 初期カウンター値を設定\n",
    "for s in app.stream(inputs_2):\n",
    "    print(s)\n",
    "final_state_2 = app.invoke(inputs_2)\n",
    "print(f\"Final State: {final_state_2}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説006</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** `TypedDict`で定義する状態クラスに、`messages`以外のカスタムキー（ここでは`counter: int`）を追加し、ノード関数内でその値を直接読み書きする方法を学びます。これにより、メッセージ履歴だけでなく、数値や文字列、ブール値など、より多様なデータをグラフ全体で管理できるようになります。\n",
    "*   **コード解説:**\n",
    "    *   `CounterState`に`counter: int`を追加しました。これにより、グラフの状態はメッセージリストと整数型のカウンターを持つことになります。\n",
    "    *   `increment_counter`ノードでは、`state.get(\"counter\", 0)`を使って現在のカウンター値を取得しています。`.get()`メソッドを使うことで、キーが存在しない場合のデフォルト値を指定できます（ここでは初回実行時を想定して0）。その後、値をインクリメントし、更新後の値を`{\"counter\": new_count}`という辞書形式で返しています。LangGraphは、ノードが返す辞書のキーに基づいて対応する状態を更新します。\n",
    "    *   `messages`キーも同時に返すことで、状態更新のログや情報をメッセージ履歴に残すことができます。\n",
    "    *   グラフ実行時 (`app.invoke`や`app.stream`) に、`inputs`辞書に`\"counter\": 0`（または任意の値）を含めることで、`counter`キーの初期値を設定できます。\n",
    "    *   このように、ノードは状態の一部または全部を更新する辞書を返すことで、グラフの状態を変化させます。`add_messages`はメッセージリストの更新に特化した便利な方法ですが、他のキーは直接値を指定して更新します。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題007: 状態の更新 - 複数のキーを一度に更新する\n",
    "\n",
    "一つのノードから、状態(`State`)の複数のキーを同時に更新する方法を学びましょう。ここでは、ユーザーからのメッセージ内容に応じて、応答メッセージと共に「応答タイプ」という別の状態キーも更新するグラフを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄007\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "ResponseType = Literal[\"greeting\", \"question\", \"other\", \"unknown\"]\n",
    "\n",
    "class MultiUpdateState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    response_type: ____ # ResponseType (応答の種類を保持するキー)\n",
    "    last_user_message: str # 最後に入力されたユーザーメッセージ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def process_input(state: MultiUpdateState):\n",
    "    user_message = state[\"messages\"][-1].content.lower()\n",
    "    response_text = \"\"\n",
    "    resp_type: ResponseType = \"unknown\"\n",
    "\n",
    "    if \"こんにちは\" in user_message or \"こんばんは\" in user_message:\n",
    "        response_text = \"こんにちは！何かお手伝いできますか？\"\n",
    "        resp_type = ____ # \"greeting\"\n",
    "    elif \"?\" in user_message or \"教えて\" in user_message:\n",
    "        response_text = \"ご質問ありがとうございます。それについては現在調べています。\"\n",
    "        resp_type = ____ # \"question\"\n",
    "    else:\n",
    "        response_text = \"メッセージありがとうございます。\"\n",
    "        resp_type = ____ # \"other\"\n",
    "    \n",
    "    print(f\"process_input: User: '{user_message}', AI: '{response_text}', Type: '{resp_type}'\")\n",
    "    # 複数のキーを同時に更新して返す\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response_text)],\n",
    "        \"response_type\": resp_type,\n",
    "        \"last_user_message\": ____ # user_message\n",
    "    }\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(MultiUpdateState) # StateGraph\n",
    "\n",
    "workflow.____(\"processor\", process_input) # add_node\n",
    "workflow.____(\"processor\") # set_entry_point\n",
    "workflow.____(\"processor\", ____) # add_edge, END\n",
    "\n",
    "app = workflow.____() # compile\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "test_inputs = [\n",
    "    {\"messages\": [HumanMessage(content=\"こんにちは\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"LangGraphについて教えてください\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"今日の天気は晴れですね\")]}\n",
    "]\n",
    "\n",
    "for i, inputs in enumerate(test_inputs):\n",
    "    print(f\"\\n--- テスト実行 {i+1} ---\")\n",
    "    for s in app.____(inputs, {\"recursion_limit\": 3}): # stream\n",
    "        print(s)\n",
    "    final_state = app.____(inputs, {\"recursion_limit\": 3}) # invoke\n",
    "    print(f\"Final State {i+1}: {final_state}\")\n",
    "    assert \"response_type\" in final_state\n",
    "    assert \"last_user_message\" in final_state\n",
    "    print(f\"Response Type: {final_state['response_type']}\")\n",
    "    print(f\"Last User Message: {final_state['last_user_message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答007</summary>\n",
    "\n",
    "``````python\n",
    "# 解答007\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "ResponseType = Literal[\"greeting\", \"question\", \"other\", \"unknown\"]\n",
    "\n",
    "class MultiUpdateState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    response_type: ResponseType # 応答の種類を保持するキー\n",
    "    last_user_message: str # 最後に入力されたユーザーメッセージ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def process_input(state: MultiUpdateState):\n",
    "    user_message = state[\"messages\"][-1].content.lower()\n",
    "    response_text = \"\"\n",
    "    resp_type: ResponseType = \"unknown\"\n",
    "\n",
    "    if \"こんにちは\" in user_message or \"こんばんは\" in user_message:\n",
    "        response_text = \"こんにちは！何かお手伝いできますか？\"\n",
    "        resp_type = \"greeting\"\n",
    "    elif \"?\" in user_message or \"教えて\" in user_message:\n",
    "        response_text = \"ご質問ありがとうございます。それについては現在調べています。\"\n",
    "        resp_type = \"question\"\n",
    "    else:\n",
    "        response_text = \"メッセージありがとうございます。\"\n",
    "        resp_type = \"other\"\n",
    "    \n",
    "    print(f\"process_input: User: '{user_message}', AI: '{response_text}', Type: '{resp_type}'\")\n",
    "    # 複数のキーを同時に更新して返す\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response_text)],\n",
    "        \"response_type\": resp_type,\n",
    "        \"last_user_message\": user_message # 元のユーザーメッセージを保存\n",
    "    }\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(MultiUpdateState)\n",
    "\n",
    "workflow.add_node(\"processor\", process_input)\n",
    "workflow.set_entry_point(\"processor\")\n",
    "workflow.add_edge(\"processor\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "test_inputs = [\n",
    "    {\"messages\": [HumanMessage(content=\"こんにちは\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"LangGraphについて教えてください\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"今日の天気は晴れですね\")]}\n",
    "]\n",
    "\n",
    "for i, inputs in enumerate(test_inputs):\n",
    "    print(f\"\\n--- テスト実行 {i+1} ---\")\n",
    "    # 初期状態としてresponse_typeやlast_user_messageを渡すことも可能だが、\n",
    "    # この問題ではノード内でこれらが設定されることを確認する\n",
    "    initial_state = inputs.copy()\n",
    "    # 必要であれば、初期値を設定\n",
    "    # initial_state.setdefault(\"response_type\", \"unknown\") \n",
    "    # initial_state.setdefault(\"last_user_message\", \"\")\n",
    "\n",
    "    for s in app.stream(initial_state, {\"recursion_limit\": 3}):\n",
    "        print(s)\n",
    "    final_state = app.invoke(initial_state, {\"recursion_limit\": 3})\n",
    "    print(f\"Final State {i+1}: {final_state}\")\n",
    "    assert \"response_type\" in final_state\n",
    "    assert \"last_user_message\" in final_state\n",
    "    print(f\"Response Type: {final_state['response_type']}\")\n",
    "    print(f\"Last User Message: {final_state['last_user_message']}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説007</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** 一つのノード関数から返す辞書に複数のキーと値のペアを含めることで、グラフの状態(`State`)の複数の属性を一度に更新する方法を学びます。また、`typing.Literal`を使って、状態キーが取りうる値を制限する方法も示します。\n",
    "*   **コード解説:**\n",
    "    *   `MultiUpdateState`に、`response_type: ResponseType` と `last_user_message: str` という2つの新しいキーを追加しました。`ResponseType`は`Literal[\"greeting\", \"question\", \"other\", \"unknown\"]`で定義され、`response_type`キーがこれらの文字列のうちのいずれかの値を取ることを示します（型ヒントであり、実行時の厳密な強制ではありませんが、開発時の可読性や静的解析に役立ちます）。\n",
    "    *   `process_input`ノードは、ユーザーのメッセージ内容に基づいて応答メッセージを生成し、それと同時に`response_type`（挨拶、質問、その他など）と`last_user_message`（処理対象となった元のユーザーメッセージ）も決定します。\n",
    "    *   ノードが返す辞書は `{\"messages\": ..., \"response_type\": ..., \"last_user_message\": ...}` のようになります。LangGraphは、この辞書に含まれる各キーに対応する状態を更新します。\n",
    "    *   実行時には、`messages`キーだけでなく、`response_type`と`last_user_message`も最終状態に含まれていることを確認できます。\n",
    "    *   このように、ノードはグラフの状態を柔軟に更新する役割を担います。返す辞書に含めるキーと値によって、どの状態属性をどのように変更するかを制御できます。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題008: ノードからの複数の出力先（静的エッジ）\n",
    "\n",
    "条件付きエッジ(`add_conditional_edges`)は強力ですが、場合によってはノードの処理結果に応じて、常に固定された異なる次のノードへ遷移させたいことがあります（実行前に分岐先が決定しているケース）。このような静的な分岐は、単に`add_edge`を複数回定義することで実現できます。ここでは、入力文字列に応じて異なる処理ノードへ遷移するグラフを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄008\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class StaticBranchState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    category: str # 入力カテゴリを保持\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def categorize_input(state: StaticBranchState):\n",
    "    last_message = state[\"messages\"][-1].content.lower()\n",
    "    cat = \"unknown\"\n",
    "    if \"fruit\" in last_message:\n",
    "        cat = \"fruit_handler\"\n",
    "    elif \"color\" in last_message:\n",
    "        cat = \"color_handler\"\n",
    "    print(f\"categorize_input: Message '{last_message}' categorized as '{cat}'\")\n",
    "    # このノードはカテゴリを返すだけで、次の遷移はエッジ定義に依存\n",
    "    # ただし、後続のノードがこのカテゴリ情報を使うかもしれないので状態に保存\n",
    "    return {\"category\": cat, \"messages\": [AIMessage(content=f\"Input categorized as {cat}\")]}\n",
    "\n",
    "def fruit_node(state: StaticBranchState):\n",
    "    print(\"fruit_node: Processing fruit-related input.\")\n",
    "    return {\"messages\": [AIMessage(content=\"This is about fruits.\")]}\n",
    "\n",
    "def color_node(state: StaticBranchState):\n",
    "    print(\"color_node: Processing color-related input.\")\n",
    "    return {\"messages\": [AIMessage(content=\"This is about colors.\")]}\n",
    "\n",
    "def unknown_node(state: StaticBranchState):\n",
    "    print(\"unknown_node: Processing unknown input.\")\n",
    "    return {\"messages\": [AIMessage(content=\"Input category unknown.\")]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 (問題003の形式) ---\n",
    "def route_by_category(state: StaticBranchState):\n",
    "    # この関数が返す文字列が、次のノード名になる\n",
    "    # カテゴリが fruit_handler なら fruit_node へ、など\n",
    "    # ここを埋めてください\n",
    "    category = state.get(\"category\", \"unknown_handler\") # カテゴリを取得、なければunknown_handler\n",
    "    print(f\"route_by_category: Routing based on category '{category}'\")\n",
    "    if category == \"fruit_handler\":\n",
    "        return ____ # \"fruit_node\"\n",
    "    elif category == \"color_handler\":\n",
    "        return ____ # \"color_node\"\n",
    "    else:\n",
    "        return ____ # \"unknown_node\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(StaticBranchState) # StateGraph\n",
    "\n",
    "workflow.____(\"categorizer\", categorize_input) # add_node\n",
    "workflow.____(\"fruit_node\", fruit_node) # add_node\n",
    "workflow.____(\"color_node\", color_node) # add_node\n",
    "workflow.____(\"unknown_node\", unknown_node) # add_node\n",
    "\n",
    "workflow.____(\"categorizer\") # set_entry_point\n",
    "\n",
    "# 条件付きエッジを使って分岐を定義\n",
    "workflow.____( # add_conditional_edges\n",
    "    \"categorizer\",\n",
    "    ____, # route_by_category\n",
    "    {\n",
    "        \"fruit_node\": \"fruit_node\",\n",
    "        \"color_node\": \"color_node\",\n",
    "        \"unknown_node\": \"unknown_node\",\n",
    "        # END を直接指定することも可能だが、ここでは専用ノードへ\n",
    "    }\n",
    ")\n",
    "\n",
    "# 各処理ノードから終了へ\n",
    "workflow.____(\"fruit_node\", ____) # add_edge, END\n",
    "workflow.____(\"color_node\", ____) # add_edge, END\n",
    "workflow.____(\"unknown_node\", ____) # add_edge, END\n",
    "\n",
    "app = workflow.____() # compile\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "test_data = [\n",
    "    {\"messages\": [HumanMessage(content=\"I like apples, it's a fruit.\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"Blue is my favorite color.\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me a joke.\")]}\n",
    "]\n",
    "\n",
    "for i, data in enumerate(test_data):\n",
    "    print(f\"\\n--- 静的分岐テスト {i+1} ---\")\n",
    "    # 初期カテゴリはcategorize_inputで設定されるので入力不要\n",
    "    for s in app.____(data, {\"recursion_limit\": 3}): # stream\n",
    "        print(s)\n",
    "    final_state = app.____(data, {\"recursion_limit\": 3}) # invoke\n",
    "    print(f\"Final State {i+1}: {final_state}\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答008</summary>\n",
    "\n",
    "``````python\n",
    "# 解答008\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display # 可視化のため\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class StaticBranchState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    category: str # 入力カテゴリを保持\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def categorize_input(state: StaticBranchState):\n",
    "    last_message = state[\"messages\"][-1].content.lower()\n",
    "    cat = \"unknown_handler\" # デフォルトをunknown_handlerに\n",
    "    if \"fruit\" in last_message:\n",
    "        cat = \"fruit_handler\"\n",
    "    elif \"color\" in last_message:\n",
    "        cat = \"color_handler\"\n",
    "    print(f\"categorize_input: Message '{last_message}' categorized as '{cat}'\")\n",
    "    return {\"category\": cat, \"messages\": [AIMessage(content=f\"Input categorized as {cat}\")]}\n",
    "\n",
    "def fruit_node(state: StaticBranchState):\n",
    "    print(\"fruit_node: Processing fruit-related input.\")\n",
    "    return {\"messages\": [AIMessage(content=\"This is about fruits.\")]}\n",
    "\n",
    "def color_node(state: StaticBranchState):\n",
    "    print(\"color_node: Processing color-related input.\")\n",
    "    return {\"messages\": [AIMessage(content=\"This is about colors.\")]}\n",
    "\n",
    "def unknown_node(state: StaticBranchState):\n",
    "    print(\"unknown_node: Processing unknown input.\")\n",
    "    return {\"messages\": [AIMessage(content=\"Input category unknown.\")]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def route_by_category(state: StaticBranchState):\n",
    "    category = state.get(\"category\", \"unknown_handler\") \n",
    "    print(f\"route_by_category: Routing based on category '{category}'\")\n",
    "    if category == \"fruit_handler\":\n",
    "        return \"fruit_node\"\n",
    "    elif category == \"color_handler\":\n",
    "        return \"color_node\"\n",
    "    else: # unknown_handler やその他の場合\n",
    "        return \"unknown_node\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(StaticBranchState)\n",
    "\n",
    "workflow.add_node(\"categorizer\", categorize_input)\n",
    "workflow.add_node(\"fruit_node\", fruit_node)\n",
    "workflow.add_node(\"color_node\", color_node)\n",
    "workflow.add_node(\"unknown_node\", unknown_node)\n",
    "\n",
    "workflow.set_entry_point(\"categorizer\")\n",
    "\n",
    "# 条件付きエッジを使って分岐を定義\n",
    "workflow.add_conditional_edges(\n",
    "    \"categorizer\",\n",
    "    route_by_category,\n",
    "    {\n",
    "        \"fruit_node\": \"fruit_node\",\n",
    "        \"color_node\": \"color_node\",\n",
    "        \"unknown_node\": \"unknown_node\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 各処理ノードから終了へ\n",
    "workflow.add_edge(\"fruit_node\", END)\n",
    "workflow.add_edge(\"color_node\", END)\n",
    "workflow.add_edge(\"unknown_node\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "test_data = [\n",
    "    {\"messages\": [HumanMessage(content=\"I like apples, it's a fruit.\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"Blue is my favorite color.\")]},\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me a joke.\")]}\n",
    "]\n",
    "\n",
    "for i, data in enumerate(test_data):\n",
    "    print(f\"\\n--- 静的分岐テスト {i+1} ---\")\n",
    "    for s in app.stream(data, {\"recursion_limit\": 3}):\n",
    "        print(s)\n",
    "    final_state = app.invoke(data, {\"recursion_limit\": 3})\n",
    "    print(f\"Final State {i+1}: {final_state}\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "    print(\"Graph visualized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説008</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** この問題では、問題003で学んだ`add_conditional_edges`とルーター関数を再利用して、ノードの処理結果（ここでは`category`状態）に基づいて、複数の固定的な次のノードへ分岐する方法を復習・確認します。問題文の意図は「`add_edge`を複数回使った静的分岐」でしたが、より一般的な条件分岐の実装方法である`add_conditional_edges`を用いた形としました。これにより、状態に基づいて動的に次の遷移先を決定する強力なパターンを再確認できます。\n",
    "*   **コード解説:**\n",
    "    *   `StaticBranchState`に`category`キーを追加し、入力がどのカテゴリに属するかを保持します。\n",
    "    *   `categorize_input`ノードは、入力メッセージの内容を解析し、`category`状態を更新します（例: `fruit_handler`, `color_handler`, `unknown_handler`）。\n",
    "    *   `fruit_node`, `color_node`, `unknown_node`は、それぞれのカテゴリに対応する処理を行うノードです。\n",
    "    *   `route_by_category`関数は、現在の`state['category']`の値を見て、次に実行すべきノード名（`\"fruit_node\"`、`\"color_node\"`、または`\"unknown_node\"`）を返します。\n",
    "    *   `workflow.add_conditional_edges(\"categorizer\", route_by_category, {...})`は、`categorizer`ノードの後に`route_by_category`関数を実行し、その戻り値に応じて指定されたノード（`fruit_node`, `color_node`, `unknown_node`のいずれか）に処理を分岐させます。\n",
    "    *   各処理ノード（`fruit_node`など）は、最終的に`END`に繋がっており、そこでグラフの実行が終了します。\n",
    "    *   このパターンは、状態に基づいて処理フローを制御する際の基本となり、より複雑なロジックを構築する上で非常に重要です。\n",
    "    *   グラフの可視化も行い、分岐構造を視覚的に確認できるようにしています。\n",
    "\n",
    "**補足:** もし「`add_edge`を複数回使った静的分岐」を厳密に表現したい場合、それは通常、あるノードが常に複数の特定の他のノードに「通知」を送るようなシナリオ（ファンアウト）で考えられますが、LangGraphの基本的な`add_edge`は一つの遷移先を指定します。一つのノードから複数のエッジを出す場合、それらは通常、異なる条件やイベントに対応するものです。上記の`add_conditional_edges`が、状態に基づいた分岐を実現する最も標準的で柔軟な方法です。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題009: `END` 以外の終了ノードの指定（概念理解）\n",
    "\n",
    "LangGraphでは、グラフの終点は通常、特別な `END` ノードに接続することで示されます。しかし、概念的には、あるノードが処理の最終ステップであり、それ以上後続のノードが存在しない場合、そのノードが事実上の「終了ノード」として機能すると考えることもできます。この問題では、特定のノードを実行した後、明示的に `END` に接続せず、グラフがそこで停止することを確認します。ただし、LangGraphのベストプラクティスとしては、可能な限り `END` を使用することが推奨されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄009\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class FinalNodeState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    status: str\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def start_process(state: FinalNodeState):\n",
    "    print(\"start_process: Process started.\")\n",
    "    return {\"status\": \"Processing\", \"messages\": [AIMessage(content=\"Process initiated.\")]}\n",
    "\n",
    "def final_processing_node(state: FinalNodeState):\n",
    "    # このノードが処理の最後とする\n",
    "    final_message = \"Process completed at final_processing_node.\"\n",
    "    print(f\"final_processing_node: {final_message}\")\n",
    "    return {\"status\": \"Completed\", \"messages\": [AIMessage(content=final_message)]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(FinalNodeState) # StateGraph\n",
    "\n",
    "workflow.____(\"start\", start_process) # add_node\n",
    "workflow.____(\"final_step\", final_processing_node) # add_node\n",
    "\n",
    "workflow.____(\"start\") # set_entry_point\n",
    "\n",
    "# startノードからfinal_stepノードへのエッジ\n",
    "workflow.____(\"start\", \"final_step\") # add_edge\n",
    "\n",
    "# final_stepノードからENDへのエッジを意図的に作成しない\n",
    "# workflow.add_edge(\"final_step\", END) # ← これをコメントアウトまたは削除\n",
    "\n",
    "# グラフのコンパイル\n",
    "# check_interruptions=True をつけると、ENDに到達しない場合にエラーになるため、\n",
    "# この例では明示的にENDに繋がないことを示すために compile() の引数なし、\n",
    "# または check_interruptions=False (デフォルト) を利用します。\n",
    "# Langfuseなどのトレーシングツールと連携する場合、ENDに到達しないとトレースが終了しないことがあるため注意。\n",
    "app = workflow.____() # compile\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- 最終ノードテスト ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Begin process\")], \"status\": \"Initial\"}\n",
    "final_state = None\n",
    "for s in app.____(inputs, {\"recursion_limit\": 5}): # stream\n",
    "    print(s)\n",
    "    # streamの最後の要素が最終状態となる\n",
    "    # (invokeと異なり、ENDに到達しなくても最後に実行されたノードの更新後状態が返る)\n",
    "    if \"final_step\" in s: # final_stepノードの出力が得られたら、それが最終状態\n",
    "        final_state = s[\"final_step\"]\n",
    "\n",
    "print(f\"Final State from stream: {final_state}\")\n",
    "\n",
    "# invokeを呼び出す場合、ENDに到達しないとエラーになるか、最後のノードの結果が返るかは\n",
    "# LangGraphのバージョンや設定に依存する可能性がある。\n",
    "# ここではstreamの挙動を中心に確認。\n",
    "try:\n",
    "    invoked_state = app.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"Final State from invoke: {invoked_state}\")\n",
    "    # invokeがエラーなく値を返す場合、その値を最終状態として扱う\n",
    "    # 基本的にはENDに到達することが期待される\n",
    "except Exception as e:\n",
    "    print(f\"Invoke failed as expected or due to other reasons: {e}\")\n",
    "    print(\"InvokeはENDに到達しない場合、エラーを発生させることがあります。streamの最後の出力で状態を確認してください。\")\n",
    "\n",
    "assert final_state is not None, \"Final state was not captured from stream\"\n",
    "assert final_state[\"status\"] == \"Completed\"\n",
    "assert \"Process completed at final_processing_node.\" in final_state[\"messages\"][-1].content\n",
    "print(\"Assertion for final_state from stream passed.\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答009</summary>\n",
    "\n",
    "``````python\n",
    "# 解答009\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class FinalNodeState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    status: str\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def start_process(state: FinalNodeState):\n",
    "    print(\"start_process: Process started.\")\n",
    "    return {\"status\": \"Processing\", \"messages\": [AIMessage(content=\"Process initiated.\")]}\n",
    "\n",
    "def final_processing_node(state: FinalNodeState):\n",
    "    # このノードが処理の最後とする\n",
    "    final_message = \"Process completed at final_processing_node.\"\n",
    "    print(f\"final_processing_node: {final_message}\")\n",
    "    return {\"status\": \"Completed\", \"messages\": [AIMessage(content=final_message)]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(FinalNodeState)\n",
    "\n",
    "workflow.add_node(\"start\", start_process)\n",
    "workflow.add_node(\"final_step\", final_processing_node)\n",
    "\n",
    "workflow.set_entry_point(\"start\")\n",
    "\n",
    "# startノードからfinal_stepノードへのエッジ\n",
    "workflow.add_edge(\"start\", \"final_step\")\n",
    "\n",
    "# final_stepノードからENDへのエッジを意図的に作成しない\n",
    "# workflow.add_edge(\"final_step\", END) # ← コメントアウトまたは削除\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "print(\"\\n--- 最終ノードテスト ---\")\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Begin process\")], \"status\": \"Initial\"}\n",
    "final_state_from_stream = None\n",
    "\n",
    "print(\"Streaming execution:\")\n",
    "for s in app.stream(inputs, {\"recursion_limit\": 5}):\n",
    "    print(s)\n",
    "    # streamの各要素は {ノード名: 更新された状態} または {ノード名: 更新内容} の形式\n",
    "    # 最後の \"final_step\" ノードの出力が、この場合の最終状態を示す\n",
    "    if \"final_step\" in s:\n",
    "        final_state_from_stream = s[\"final_step\"]\n",
    "\n",
    "print(f\"Final State from stream: {final_state_from_stream}\")\n",
    "\n",
    "# invokeの挙動確認\n",
    "invoked_state = None\n",
    "try:\n",
    "    # ENDに繋がっていない場合、invokeはエラーを出すか、最後に実行されたノードの状態を返す\n",
    "    # LangGraphのバージョンや内部実装により挙動が変わりうるため注意\n",
    "    # ここでは、streamの結果を正として扱う\n",
    "    invoked_state = app.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"Final State from invoke: {invoked_state}\")\n",
    "except Exception as e:\n",
    "    print(f\"Invoke call resulted in an error or unexpected behavior: {e}\")\n",
    "    print(\"This might be expected if the graph doesn't explicitly reach END.\")\n",
    "\n",
    "# streamから取得した最終状態でアサーションを行う\n",
    "assert final_state_from_stream is not None, \"Final state was not captured from stream\"\n",
    "assert final_state_from_stream[\"status\"] == \"Completed\"\n",
    "assert \"Process completed at final_processing_node.\" in final_state_from_stream[\"messages\"][-1].content\n",
    "print(\"Assertion for final_state_from_stream passed.\")\n",
    "\n",
    "# もしinvokeが値を返した場合、それも確認 (参考程度)\n",
    "if invoked_state:\n",
    "    print(f\"Invoked state status: {invoked_state.get('status')}\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "    print(\"Graph visualized. Note that 'final_step' does not point to END.\")\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説009</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** グラフの終点として必ずしも `END` を明示的に指定する必要はなく、あるノードから先に遷移するエッジがなければ、そのノードの処理が終わった時点でグラフの実行が停止し、その時点での状態が最終状態となることを理解します。ただし、これはLangGraphの挙動の一つであり、デバッグや可視化、他のツールとの連携（例: Langfuse）を考慮すると、可能な限りグラフの終点を `END` に接続することが推奨されます。\n",
    "*   **コード解説:**\n",
    "    *   `final_processing_node`を作成し、このノードから `END` へのエッジ（`workflow.add_edge(\"final_step\", END)`）を定義していません。\n",
    "    *   `app.stream()` を使ってグラフを実行すると、`final_processing_node` が実行された後、それ以上進むべきノードがないため、処理が停止します。`stream()` の最後の出力（この場合は `final_processing_node` の出力）が、その実行における最終的な状態を示します。\n",
    "    *   `app.invoke()` の場合、グラフが明示的に `END` に到達しないと、バージョンや設定によってはエラーが発生したり、予期しない挙動をしたりする可能性があります。一般的に `invoke()` はグラフが `END` に到達し、完全な最終状態が確定することを期待します。この問題では、主に `stream()` での挙動を確認し、`invoke()` は参考として示しています。\n",
    "    *   可視化すると、`final_step` ノードから `END` (または他のノード) への矢印がないことが確認できます。\n",
    "*   **重要な注意点:**\n",
    "    *   **`END` の使用推奨:** LangGraphでは、グラフの論理的な終了点を明確にするために `END` を使用することが強く推奨されます。これにより、グラフの構造が理解しやすくなり、デバッグも容易になります。また、LangSmith/Langfuseのようなトレースツールは、`END` への到達をもって一連の処理の完了とみなすことが多いため、連携時にも重要です。\n",
    "    *   **`compile(check_interruptions=True)`:** グラフをコンパイルする際に `check_interruptions=True` を指定すると、中断（Interrupt）が発生しない限り、グラフが必ず `END` に到達することを強制できます。`END` に到達しないパスがある場合、コンパイル時または実行時にエラーが発生します。\n",
    "    *   この問題は、`END` を使わない場合の挙動を理解するためのものであり、実際の開発では `END` を適切に配置する設計を心がけてください。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題010: LLMノードと非LLMノードの連携強化\n",
    "\n",
    "LLM（大規模言語モデル）を組み込んだノードと、LLM以外の処理を行うノード（例: 文字列操作、データ抽出など）を連携させる方法を学びましょう。ここでは、LLMに質問を投げて得られた応答（文字列）から、特定の情報を抽出・加工して状態を更新する、より実践的なグラフを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄010\n",
    "from typing import TypedDict, Annotated\n",
    "import re # 正規表現モジュールをインポート\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ExtractionState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_question: str # ユーザーの元の質問\n",
    "    llm_response_text: str # LLMの生の応答テキスト\n",
    "    extracted_info: str | None # LLMの応答から抽出された情報\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def get_user_question(state: ExtractionState):\n",
    "    # ユーザーの質問を状態に保存\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"get_user_question: Received question: '{last_message_content}'\")\n",
    "    return {\"user_question\": ____} # last_message_content\n",
    "\n",
    "def llm_responder_node(state: ExtractionState):\n",
    "    # LLMに質問を投げるノード\n",
    "    question = state[\"user_question\"]\n",
    "    print(f\"llm_responder_node: Asking LLM: '{question}'\")\n",
    "    # LLMに渡すメッセージは、過去の履歴全体でも、最新の質問だけでも良い\n",
    "    # ここでは簡単のため、最新の質問のみをHumanMessageとして渡す\n",
    "    response = llm.invoke([HumanMessage(content=question)])\n",
    "    response_content = response.content\n",
    "    print(f\"llm_responder_node: LLM raw response: '{response_content}'\")\n",
    "    return {\"messages\": [response], \"llm_response_text\": ____} # response_content\n",
    "\n",
    "def data_extractor_node(state: ExtractionState):\n",
    "    # LLMの応答から情報を抽出するノード\n",
    "    raw_response = state[\"llm_response_text\"]\n",
    "    # 例: LLMが「日本の首都は東京です。」と答えたら「東京」を抽出\n",
    "    # ここでは簡単な正規表現で「XXはYYです」のYY部分を抽出試行\n",
    "    extracted = None\n",
    "    match = re.search(r\"(?:は|is)\\s*([^。.]+)[.。]?\", raw_response) # 簡易的な抽出\n",
    "    if match:\n",
    "        extracted = match.group(1).strip()\n",
    "    \n",
    "    print(f\"data_extractor_node: Raw response: '{raw_response}', Extracted: '{extracted}'\")\n",
    "    return {\"extracted_info\": extracted, \"messages\": [AIMessage(content=f\"Extracted: {extracted}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(ExtractionState) # StateGraph\n",
    "\n",
    "workflow.____(\"capture_question\", get_user_question) # add_node\n",
    "workflow.____(\"ask_llm\", llm_responder_node) # add_node\n",
    "workflow.____(\"extract_data\", data_extractor_node) # add_node\n",
    "\n",
    "workflow.____(\"capture_question\") # set_entry_point\n",
    "\n",
    "workflow.____(\"capture_question\", \"ask_llm\") # add_edge\n",
    "workflow.____(\"ask_llm\", \"extract_data\") # add_edge\n",
    "workflow.____(\"extract_data\", ____) # add_edge, END\n",
    "\n",
    "app = workflow.____() # compile\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "questions = [\n",
    "    \"日本の首都は何ですか？\",\n",
    "    \"フランスの有名な画家の名前を一人教えてください。\",\n",
    "    \"今日の天気は？\" # これは抽出パターンにマッチしない可能性\n",
    "]\n",
    "\n",
    "for q_text in questions:\n",
    "    print(f\"\\n--- LLM連携と情報抽出テスト (質問: {q_text}) ---\")\n",
    "    inputs = {\"messages\": [HumanMessage(content=q_text)]}\n",
    "    final_state = None\n",
    "    for s in app.____(inputs, {\"recursion_limit\": 5}): # stream\n",
    "        print(s)\n",
    "        if \"extract_data\" in s:\n",
    "            final_state = s[\"extract_data\"]\n",
    "\n",
    "    if final_state:\n",
    "        print(f\"Final State: {final_state}\")\n",
    "        print(f\"  User Question: {final_state.get('user_question')}\")\n",
    "        print(f\"  LLM Response: {final_state.get('llm_response_text')}\")\n",
    "        print(f\"  Extracted Info: {final_state.get('extracted_info')}\")\n",
    "    else:\n",
    "        # streamの最後がENDだった場合など、特定のノード名で状態が取れない場合がある\n",
    "        # その場合はinvokeで最終状態を取得\n",
    "        final_state_invoked = app.invoke(inputs, {\"recursion_limit\": 5})\n",
    "        print(f\"Final State (from invoke): {final_state_invoked}\")\n",
    "        print(f\"  User Question: {final_state_invoked.get('user_question')}\")\n",
    "        print(f\"  LLM Response: {final_state_invoked.get('llm_response_text')}\")\n",
    "        print(f\"  Extracted Info: {final_state_invoked.get('extracted_info')}\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答010</summary>\n",
    "\n",
    "``````python\n",
    "# 解答010\n",
    "from typing import TypedDict, Annotated\n",
    "import re # 正規表現モジュールをインポート\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ExtractionState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_question: str # ユーザーの元の質問\n",
    "    llm_response_text: str # LLMの生の応答テキスト\n",
    "    extracted_info: str | None # LLMの応答から抽出された情報\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def get_user_question(state: ExtractionState):\n",
    "    # ユーザーの質問を状態に保存\n",
    "    last_message_content = state[\"messages\"][-1].content\n",
    "    print(f\"get_user_question: Received question: '{last_message_content}'\")\n",
    "    return {\"user_question\": last_message_content}\n",
    "\n",
    "def llm_responder_node(state: ExtractionState):\n",
    "    # LLMに質問を投げるノード\n",
    "    question = state[\"user_question\"]\n",
    "    print(f\"llm_responder_node: Asking LLM: '{question}'\")\n",
    "    response = llm.invoke([HumanMessage(content=question)])\n",
    "    response_content = response.content\n",
    "    print(f\"llm_responder_node: LLM raw response: '{response_content}'\")\n",
    "    return {\"messages\": [response], \"llm_response_text\": response_content}\n",
    "\n",
    "def data_extractor_node(state: ExtractionState):\n",
    "    # LLMの応答から情報を抽出するノード\n",
    "    raw_response = state[\"llm_response_text\"]\n",
    "    extracted = None\n",
    "    # 改善された正規表現: 「XXはYYです」「XX is YY」のようなパターンや、単に「YYです」のような応答にも対応試行\n",
    "    # 質問が「日本の首都は何ですか？」で応答が「東京です。」の場合「東京」を抽出\n",
    "    # 質問が「日本の首都は？」で応答が「東京」の場合「東京」を抽出\n",
    "    patterns = [\n",
    "        r\"(?:.+は|.+\\s*is)\\s*(.+?)(?:です|。|\\.|\\s*for|$)\", #「～はXです」「～ is X」\n",
    "        r\"^([^。.]+?)(?:です|。|\\.|\\s*for|$)\" # 文頭から「Xです」\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, raw_response)\n",
    "        if match:\n",
    "            extracted = match.group(1).strip()\n",
    "            if extracted.lower() == state[\"user_question\"].lower().replace(\"何ですか\",\"\").replace(\"何\",\"зиру\").strip(\"?？\") : # 質問自体が答えになるような場合を除外\n",
    "                 extracted = None # 例：「天気は？」->「晴れです」はOKだが、「天気は？」->「天気」はNG\n",
    "                 continue\n",
    "            break\n",
    "    \n",
    "    # もし上記で抽出できなかった場合、LLMの応答が単語やフレーズそのものである可能性を考慮\n",
    "    if not extracted and len(raw_response.split()) < 5 and not state[\"user_question\"].startswith(raw_response): # 短い応答で、質問の繰り返しでない場合\n",
    "        extracted = raw_response.strip()\n",
    "\n",
    "    print(f\"data_extractor_node: Raw response: '{raw_response}', Extracted: '{extracted}'\")\n",
    "    return {\"extracted_info\": extracted, \"messages\": [AIMessage(content=f\"Extracted info: {extracted if extracted else 'N/A'}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(ExtractionState)\n",
    "\n",
    "workflow.add_node(\"capture_question\", get_user_question)\n",
    "workflow.add_node(\"ask_llm\", llm_responder_node)\n",
    "workflow.add_node(\"extract_data\", data_extractor_node)\n",
    "\n",
    "workflow.set_entry_point(\"capture_question\")\n",
    "\n",
    "workflow.add_edge(\"capture_question\", \"ask_llm\")\n",
    "workflow.add_edge(\"ask_llm\", \"extract_data\")\n",
    "workflow.add_edge(\"extract_data\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "questions = [\n",
    "    \"日本の首都は何ですか？\",\n",
    "    \"フランスの有名な画家の名前を一人教えてください。\", # LLMの回答次第で抽出成功/失敗が変わる\n",
    "    \"1+1は何？\", # LLMが「2です」と答えれば抽出できるかも\n",
    "    \"今日の天気は？\" # 「晴れです」なら「晴れ」を抽出期待\n",
    "]\n",
    "\n",
    "for q_text in questions:\n",
    "    print(f\"\\n--- LLM連携と情報抽出テスト (質問: {q_text}) ---\")\n",
    "    inputs = {\"messages\": [HumanMessage(content=q_text)], \"llm_response_text\": \"\", \"extracted_info\": None} # 初期値を設定\n",
    "    \n",
    "    # streamで実行し、各ステップの状態変化を確認\n",
    "    # final_state_from_stream = {} # streamの最終的な状態を保持する辞書\n",
    "    # for s_chunk in app.stream(inputs, {\"recursion_limit\": 5}):\n",
    "    #     print(s_chunk)\n",
    "    #     final_state_from_stream.update(s_chunk)\n",
    "    # final_state_data = final_state_from_stream.get(list(final_state_from_stream.keys())[-1]) # 最後のキーのデータ\n",
    "\n",
    "    # invokeで最終状態を取得する方がシンプル\n",
    "    final_state_data = app.invoke(inputs, {\"recursion_limit\": 5})\n",
    "\n",
    "    if final_state_data:\n",
    "        print(f\"Final State: {final_state_data}\")\n",
    "        print(f\"  User Question: {final_state_data.get('user_question')}\")\n",
    "        print(f\"  LLM Response: {final_state_data.get('llm_response_text')}\")\n",
    "        print(f\"  Extracted Info: {final_state_data.get('extracted_info')}\")\n",
    "    else:\n",
    "        print(\"Could not retrieve final state.\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説010</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** LLMを呼び出すノードと、その出力を処理する非LLMノードを組み合わせることで、より高度な情報処理パイプラインを構築する方法を学びます。具体的には、LLMの自然言語応答から正規表現などを用いて構造化された情報を抽出し、グラフの状態を更新します。\n",
    "*   **コード解説:**\n",
    "    *   `ExtractionState`には、ユーザーの質問 (`user_question`)、LLMの生の応答テキスト (`llm_response_text`)、そして抽出された情報 (`extracted_info`) を保持するためのキーが定義されています。\n",
    "    *   `get_user_question`ノード: ユーザーからの最初のメッセージを `user_question` として状態に保存します。\n",
    "    *   `llm_responder_node`: 保存された `user_question` を使ってLLMに問い合わせを行い、得られた応答を `messages` (AIMessageとして) と `llm_response_text` (生の文字列として) 状態に保存します。\n",
    "    *   `data_extractor_node`: `llm_response_text` から情報を抽出します。この例では、簡単な正規表現 `re.search(r\"(?:は|is)\\s*([^。.]+)[.。]?\", raw_response)` を使用して、「AはBです」や「A is B」といった形式の文からBの部分を抽出しようと試みています。抽出結果は `extracted_info` として状態に保存されます。正規表現は完璧ではなく、LLMの応答形式によってはうまく抽出できない場合もありますが、ここではLLMの出力後処理の一例として示しています。\n",
    "    *   グラフは `capture_question` -> `ask_llm` -> `extract_data` -> `END` というシーケンシャルな流れで定義されています。\n",
    "    *   実行時には、異なる質問を投げ、LLMの応答とそこから抽出された情報（または抽出できなかった場合は `None` や `N/A`）が最終状態に含まれることを確認します。\n",
    "*   **発展:**\n",
    "    *   情報抽出の方法は正規表現に限らず、より高度なNLPライブラリ（例: spaCy）や、LangChainが提供するOutput Parsers、あるいは別のLLMコール（Function Calling/Tool Calling対応モデルならより高精度）を使って行うことも考えられます。\n",
    "    *   抽出に失敗した場合のフォールバック処理（例: ユーザーに再確認を求める、デフォルト値を設定するなど）をグラフに追加することもできます。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題011: 状態を使ったループ（シンプルなカウンター制御）\n",
    "\n",
    "グラフ内で特定の条件が満たされるまで処理を繰り返す「ループ」構造を、状態と条件付きエッジを使って実現する方法を学びましょう。ここでは、カウンターが指定した上限値に達するまで、カウンターを増やし続けるシンプルなループ処理を持つグラフを作成します。これは、より複雑な自己修正ループや反復処理の基礎となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄011\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class LoopState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    current_count: int # 現在のカウンター値\n",
    "    max_count: int     # ループを終了するカウンターの上限値\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def increment_and_log(state: LoopState):\n",
    "    # カウンターを1増やし、メッセージをログに記録する\n",
    "    count = state.get(\"current_count\", 0) + 1\n",
    "    log_message = f\"Counter incremented to {count}.\"\n",
    "    print(f\"increment_and_log: {log_message}\")\n",
    "    return {\"current_count\": count, \"messages\": [AIMessage(content=log_message)]}\n",
    "\n",
    "def final_log(state: LoopState):\n",
    "    # ループ終了後に最終メッセージをログに記録する\n",
    "    log_message = f\"Loop finished. Final count is {state['current_count']}. Max count was {state['max_count']}.\"\n",
    "    print(f\"final_log: {log_message}\")\n",
    "    return {\"messages\": [AIMessage(content=log_message)]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def should_continue_loop(state: LoopState):\n",
    "    # current_countがmax_count未満ならループを継続、そうでなければ終了\n",
    "    current = state.get(\"current_count\", 0)\n",
    "    max_c = state.get(\"max_count\", 0)\n",
    "    print(f\"should_continue_loop: Current: {current}, Max: {max_c}\")\n",
    "    if current < max_c:\n",
    "        print(\"  -> Routing to 'increment_and_log' (continue loop)\")\n",
    "        return ____ # \"continue_loop\"\n",
    "    else:\n",
    "        print(\"  -> Routing to 'exit_loop' (end loop)\")\n",
    "        return ____ # \"exit_loop\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(LoopState) # StateGraph\n",
    "\n",
    "workflow.____(\"increment_logger\", increment_and_log) # add_node\n",
    "workflow.____(\"final_logger\", final_log) # add_node\n",
    "\n",
    "# エントリポイントは直接 increment_logger とする (初期カウントは入力で設定)\n",
    "workflow.____(\"increment_logger\") # set_entry_point\n",
    "\n",
    "# 条件付きエッジでループを制御\n",
    "workflow.____( # add_conditional_edges\n",
    "    \"increment_logger\", # このノードの後に条件分岐\n",
    "    ____, # should_continue_loop (ルーター関数)\n",
    "    {\n",
    "        \"continue_loop\": \"increment_logger\", # \"continue_loop\"なら再度increment_loggerへ (ループバック)\n",
    "        \"exit_loop\": \"final_logger\"      # \"exit_loop\"ならfinal_loggerへ\n",
    "    }\n",
    ")\n",
    "\n",
    "# final_loggerノードからENDへ\n",
    "workflow.____(\"final_logger\", ____) # add_edge, END\n",
    "\n",
    "app = workflow.____() # compile\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "max_iterations = 3\n",
    "print(f\"\\n--- ループテスト (max_count = {max_iterations}) ---\")\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=f\"Start loop up to {max_iterations}\")],\n",
    "    \"current_count\": 0, # 初期カウント\n",
    "    \"max_count\": max_iterations\n",
    "}\n",
    "\n",
    "for s in app.____(inputs, {\"recursion_limit\": 10}): # stream (recursion_limitに注意)\n",
    "    print(s)\n",
    "\n",
    "final_state = app.____(inputs, {\"recursion_limit\": 10}) # invoke\n",
    "print(f\"Final State: {final_state}\")\n",
    "assert final_state[\"current_count\"] == max_iterations\n",
    "assert f\"Loop finished. Final count is {max_iterations}.\" in final_state[\"messages\"][-1].content\n",
    "print(\"Loop test passed.\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答011</summary>\n",
    "\n",
    "``````python\n",
    "# 解答011\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class LoopState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    current_count: int # 現在のカウンター値\n",
    "    max_count: int     # ループを終了するカウンターの上限値\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def increment_and_log(state: LoopState):\n",
    "    # カウンターを1増やし、メッセージをログに記録する\n",
    "    count = state.get(\"current_count\", 0) + 1\n",
    "    log_message = f\"Counter incremented to {count}.\"\n",
    "    print(f\"increment_and_log: {log_message}\")\n",
    "    return {\"current_count\": count, \"messages\": [AIMessage(content=log_message)]}\n",
    "\n",
    "def final_log(state: LoopState):\n",
    "    # ループ終了後に最終メッセージをログに記録する\n",
    "    log_message = f\"Loop finished. Final count is {state['current_count']}. Max count was {state['max_count']}.\"\n",
    "    print(f\"final_log: {log_message}\")\n",
    "    return {\"messages\": [AIMessage(content=log_message)]}\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def should_continue_loop(state: LoopState):\n",
    "    # current_countがmax_count未満ならループを継続、そうでなければ終了\n",
    "    current = state.get(\"current_count\", 0)\n",
    "    max_c = state.get(\"max_count\", 0)\n",
    "    print(f\"should_continue_loop: Current: {current}, Max: {max_c}\")\n",
    "    if current < max_c:\n",
    "        print(\"  -> Routing to 'increment_and_log' (continue loop)\")\n",
    "        return \"continue_loop\"\n",
    "    else:\n",
    "        print(\"  -> Routing to 'exit_loop' (end loop)\")\n",
    "        return \"exit_loop\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(LoopState)\n",
    "\n",
    "workflow.add_node(\"increment_logger\", increment_and_log)\n",
    "workflow.add_node(\"final_logger\", final_log)\n",
    "\n",
    "# エントリポイントは直接 increment_logger とする (初期カウントは入力で設定)\n",
    "workflow.set_entry_point(\"increment_logger\")\n",
    "\n",
    "# 条件付きエッジでループを制御\n",
    "workflow.add_conditional_edges(\n",
    "    \"increment_logger\", # このノードの後に条件分岐\n",
    "    should_continue_loop, # ルーター関数\n",
    "    {\n",
    "        \"continue_loop\": \"increment_logger\", # \"continue_loop\"なら再度increment_loggerへ (ループバック)\n",
    "        \"exit_loop\": \"final_logger\"      # \"exit_loop\"ならfinal_loggerへ\n",
    "    }\n",
    ")\n",
    "\n",
    "# final_loggerノードからENDへ\n",
    "workflow.add_edge(\"final_logger\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "max_iterations = 3\n",
    "print(f\"\\n--- ループテスト (max_count = {max_iterations}) ---\")\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=f\"Start loop up to {max_iterations}\")],\n",
    "    \"current_count\": 0, # 初期カウント\n",
    "    \"max_count\": max_iterations\n",
    "}\n",
    "\n",
    "for s in app.stream(inputs, {\"recursion_limit\": 10}): # stream (recursion_limitに注意)\n",
    "    print(s)\n",
    "\n",
    "final_state = app.invoke(inputs, {\"recursion_limit\": 10}) # invoke\n",
    "print(f\"Final State: {final_state}\")\n",
    "assert final_state[\"current_count\"] == max_iterations\n",
    "assert f\"Loop finished. Final count is {max_iterations}.\" in final_state[\"messages\"][-1].content\n",
    "print(\"Loop test passed.\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "    print(\"Graph visualized. Note the loop from 'increment_logger' back to itself.\")\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説011</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** 状態(`State`)と条件付きエッジ(`add_conditional_edges`)を組み合わせて、グラフ内にループ構造（繰り返し処理）を実装する方法を学びます。具体的には、カウンターが上限に達するまで特定のノード（ここでは`increment_logger`）を繰り返し実行し、条件を満たしたら別のノード（`final_logger`）に遷移して終了します。\n",
    "*   **コード解説:**\n",
    "    *   `LoopState`には、現在のカウンター値 `current_count` とループの終了条件となる `max_count` が定義されています。\n",
    "    *   `increment_and_log`ノード: `current_count`を1増やし、その結果をログ（`messages`）に記録します。\n",
    "    *   `final_log`ノード: ループが終了した際に最終的なメッセージをログに記録します。\n",
    "    *   `should_continue_loop`ルーター関数: `current_count`が`max_count`未満であれば `\"continue_loop\"` を返し、そうでなければ `\"exit_loop\"` を返します。\n",
    "    *   グラフ構築部分:\n",
    "        *   エントリポイントは `increment_logger` です。\n",
    "        *   `add_conditional_edges` を使って `increment_logger` の後に `should_continue_loop` ルーターを実行します。\n",
    "        *   ルーターが `\"continue_loop\"` を返した場合、処理は再び `increment_logger` に戻ります（これがループバックエッジとなり、ループを形成します）。\n",
    "        *   ルーターが `\"exit_loop\"` を返した場合、処理は `final_logger` に進み、その後 `END` に到達してグラフが終了します。\n",
    "    *   実行時には、`inputs`で `current_count` の初期値（例: 0）と `max_count` を設定します。グラフは `increment_logger` を `max_count` 回実行し、その後 `final_logger` を実行して終了します。\n",
    "    *   `app.stream()` や `app.invoke()` を呼び出す際には、`recursion_limit`（再帰深度の上限）を適切に設定する必要があります。ループが深い場合、デフォルトの再帰上限を超える可能性があるためです。\n",
    "*   **応用:** このループ構造は、例えばLLMに繰り返し改善を指示したり、特定の条件が満たされるまでツールを実行し続けたりするなど、より複雑なエージェント的挙動の基礎となります。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題012: グラフの入力と出力のカスタマイズと明確化\n",
    "\n",
    "LangGraphのグラフを実行する際、`invoke()` や `stream()` メソッドに渡す初期状態の構造と、グラフ全体の最終的な出力状態の構造を意識することが重要です。`StateGraph` に渡す状態クラス（例: `TypedDict`）の定義が、実質的にグラフの入力と出力のスキーマ（型定義）となります。この問題では、入力として複数の情報を受け取り、それらを処理して特定の構造で出力するグラフを作成し、入出力の対応関係を明確に意識します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄012\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (入力と出力のスキーマを兼ねる) ---\n",
    "class ProcessedData(TypedDict):\n",
    "    item_id: str\n",
    "    description: str\n",
    "    is_processed: bool\n",
    "\n",
    "class ComplexIOState(TypedDict):\n",
    "    # 入力として期待されるキー\n",
    "    raw_item_name: str\n",
    "    raw_item_details: List[str]\n",
    "    processing_threshold: Optional[int]\n",
    "\n",
    "    # 処理中に使われるキー\n",
    "    messages: Annotated[list, add_messages] # 処理ログ用\n",
    "    internal_counter: int\n",
    "\n",
    "    # 出力として期待される主要なキー\n",
    "    processed_data: Optional[ProcessedData] # 処理結果\n",
    "    error_message: Optional[str] # エラー発生時のメッセージ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def initialize_processing(state: ComplexIOState):\n",
    "    print(f\"initialize_processing: Received item '{state['raw_item_name']}' with details {state['raw_item_details']}\")\n",
    "    threshold = state.get(\"processing_threshold\", 0)\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Initializing processing for {state['raw_item_name']}\")],\n",
    "        \"internal_counter\": 0,\n",
    "        \"processed_data\": None, # 出力キーを初期化\n",
    "        \"error_message\": None   # 出力キーを初期化\n",
    "        # processing_threshold は入力からそのまま引き継がれる\n",
    "    }\n",
    "\n",
    "def main_processor(state: ComplexIOState):\n",
    "    name = state[\"raw_item_name\"]\n",
    "    details_count = len(state[\"raw_item_details\"])\n",
    "    counter = state[\"internal_counter\"] + 1\n",
    "    threshold = state.get(\"processing_threshold\", 0)\n",
    "    \n",
    "    log_msg = f\"Processing '{name}', detail count: {details_count}, attempt: {counter}, threshold: {threshold}\"\n",
    "    print(f\"main_processor: {log_msg}\")\n",
    "\n",
    "    if details_count == 0:\n",
    "        err_msg = \"No details provided.\"\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {err_msg}\")],\n",
    "            \"error_message\": err_msg,\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "    \n",
    "    # 簡単な処理のシミュレーション\n",
    "    if counter > threshold: # processing_threshold を超えたら処理成功とするデモ\n",
    "        processed_item = ProcessedData(\n",
    "            item_id=f\"PROC_{name.upper()}_{counter}\",\n",
    "            description=f\"Successfully processed {name} with {details_count} details after {counter} attempts.\",\n",
    "            is_processed=True\n",
    "        )\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Successfully processed {name}\")],\n",
    "            \"processed_data\": ____, # processed_item\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Attempt {counter} for {name} did not meet threshold.\")],\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def check_status(state: ComplexIOState):\n",
    "    if state.get(\"error_message\"):\n",
    "        print(\"check_status: Error detected, routing to END.\")\n",
    "        return \"__end__\" # エラーなら即終了 (ENDに直接マッピングされる特別な名前)\n",
    "    if state.get(\"processed_data\") and state[\"processed_data\"][\"is_processed\"]:\n",
    "        print(\"check_status: Successfully processed, routing to END.\")\n",
    "        return ____ # \"__end__\"\n",
    "    else:\n",
    "        print(\"check_status: Not yet processed or error, routing back to main_processor.\")\n",
    "        return ____ # \"retry_processing\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(ComplexIOState) # StateGraph\n",
    "\n",
    "workflow.____(\"initializer\", initialize_processing) # add_node\n",
    "workflow.____(\"processor\", main_processor) # add_node\n",
    "\n",
    "workflow.____(\"initializer\") # set_entry_point\n",
    "workflow.____(\"initializer\", \"processor\") # add_edge\n",
    "\n",
    "workflow.____( # add_conditional_edges\n",
    "    \"processor\",\n",
    "    check_status,\n",
    "    {\n",
    "        \"__end__\": END, # ルーターが\"__end__\"を返したらグラフ終了\n",
    "        \"retry_processing\": \"processor\" # \"retry_processing\"なら再度processorへ (ループ)\n",
    "    }\n",
    ")\n",
    "\n",
    "app = workflow.____() # compile\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs_success = {\n",
    "    \"raw_item_name\": \"TestItem1\",\n",
    "    \"raw_item_details\": [\"detail A\", \"detail B\"],\n",
    "    \"processing_threshold\": 2 # 3回目のprocessor呼び出しで成功するはず\n",
    "}\n",
    "\n",
    "inputs_fail_no_details = {\n",
    "    \"raw_item_name\": \"TestItem2\",\n",
    "    \"raw_item_details\": [], # 詳細なし -> エラー\n",
    "    \"processing_threshold\": 1\n",
    "}\n",
    "\n",
    "test_cases = {\"Success Case\": inputs_success, \"Failure Case (No Details)\": inputs_fail_no_details}\n",
    "\n",
    "for case_name, inputs_data in test_cases.items():\n",
    "    print(f\"\\n--- I/Oカスタマイズテスト: {case_name} ---\")\n",
    "    # 入力時には、Stateで定義された必須キーとオプショナルなキーを渡す\n",
    "    # messages, internal_counter, processed_data, error_message は処理中に設定されるので入力不要\n",
    "    current_inputs = inputs_data.copy()\n",
    "    # messagesはadd_messagesの仕様上、リストでないとエラーになるため、空リストで初期化することが安全\n",
    "    current_inputs.setdefault(\"messages\", []) \n",
    "\n",
    "    final_output_state = app.invoke(current_inputs, {\"recursion_limit\": 10})\n",
    "    print(f\"Final Output State for {case_name}: {final_output_state}\")\n",
    "\n",
    "    # 出力状態の確認 (processed_data または error_message が期待通りか)\n",
    "    if final_output_state.get(\"processed_data\"):\n",
    "        print(f\"  Processed Item ID: {final_output_state['processed_data']['item_id']}\")\n",
    "        print(f\"  Processed: {final_output_state['processed_data']['is_processed']}\")\n",
    "    if final_output_state.get(\"error_message\"):\n",
    "        print(f\"  Error: {final_output_state['error_message']}\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答012</summary>\n",
    "\n",
    "``````python\n",
    "# 解答012\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (入力と出力のスキーマを兼ねる) ---\n",
    "class ProcessedData(TypedDict):\n",
    "    item_id: str\n",
    "    description: str\n",
    "    is_processed: bool\n",
    "\n",
    "class ComplexIOState(TypedDict):\n",
    "    # 入力として期待されるキー\n",
    "    raw_item_name: str\n",
    "    raw_item_details: List[str]\n",
    "    processing_threshold: Optional[int]\n",
    "\n",
    "    # 処理中に使われるキー\n",
    "    messages: Annotated[list, add_messages] # 処理ログ用\n",
    "    internal_counter: int\n",
    "\n",
    "    # 出力として期待される主要なキー\n",
    "    processed_data: Optional[ProcessedData] # 処理結果\n",
    "    error_message: Optional[str] # エラー発生時のメッセージ\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def initialize_processing(state: ComplexIOState):\n",
    "    print(f\"initialize_processing: Received item '{state['raw_item_name']}' with details {state['raw_item_details']}\")\n",
    "    # processing_thresholdが入力で与えられていない場合のデフォルト値を設定\n",
    "    # この例では、state.getで取得する際にデフォルト値を設定するアプローチではなく、\n",
    "    # 入力時に processing_threshold が Optional であることを明示し、\n",
    "    # main_processor で利用する際に .get でデフォルト値を扱う。\n",
    "    # ここでは、入力された値をそのまま引き継ぎ、他の内部状態を初期化。\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Initializing processing for {state['raw_item_name']}\")],\n",
    "        \"internal_counter\": 0,\n",
    "        \"processed_data\": None, # 出力キーを初期化\n",
    "        \"error_message\": None   # 出力キーを初期化\n",
    "    }\n",
    "\n",
    "def main_processor(state: ComplexIOState):\n",
    "    name = state[\"raw_item_name\"]\n",
    "    details_count = len(state[\"raw_item_details\"])\n",
    "    counter = state[\"internal_counter\"] + 1\n",
    "    # processing_threshold が None の場合はデフォルト値 0 を使用\n",
    "    threshold = state.get(\"processing_threshold\") if state.get(\"processing_threshold\") is not None else 0\n",
    "    \n",
    "    log_msg = f\"Processing '{name}', detail count: {details_count}, attempt: {counter}, threshold: {threshold}\"\n",
    "    print(f\"main_processor: {log_msg}\")\n",
    "\n",
    "    if details_count == 0:\n",
    "        err_msg = \"No details provided.\"\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {err_msg}\")],\n",
    "            \"error_message\": err_msg,\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "    \n",
    "    if counter > threshold:\n",
    "        processed_item = ProcessedData(\n",
    "            item_id=f\"PROC_{name.upper()}_{counter}\",\n",
    "            description=f\"Successfully processed {name} with {details_count} details after {counter} attempts.\",\n",
    "            is_processed=True\n",
    "        )\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Successfully processed {name}\")],\n",
    "            \"processed_data\": processed_item,\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Attempt {counter} for {name} did not meet threshold.\")],\n",
    "            \"internal_counter\": counter\n",
    "        }\n",
    "\n",
    "# --- 条件付きエッジのルーター関数 ---\n",
    "def check_status(state: ComplexIOState):\n",
    "    if state.get(\"error_message\"):\n",
    "        print(\"check_status: Error detected, routing to END.\")\n",
    "        return \"__end__\" \n",
    "    if state.get(\"processed_data\") and state[\"processed_data\"][\"is_processed\"]:\n",
    "        print(\"check_status: Successfully processed, routing to END.\")\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        print(\"check_status: Not yet processed or error, routing back to main_processor.\")\n",
    "        return \"retry_processing\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(ComplexIOState)\n",
    "\n",
    "workflow.add_node(\"initializer\", initialize_processing)\n",
    "workflow.add_node(\"processor\", main_processor)\n",
    "\n",
    "workflow.set_entry_point(\"initializer\")\n",
    "workflow.add_edge(\"initializer\", \"processor\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"processor\",\n",
    "    check_status,\n",
    "    {\n",
    "        \"__end__\": END,\n",
    "        \"retry_processing\": \"processor\"\n",
    "    }\n",
    ")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "inputs_success = {\n",
    "    \"raw_item_name\": \"TestItem1\",\n",
    "    \"raw_item_details\": [\"detail A\", \"detail B\"],\n",
    "    \"processing_threshold\": 2 \n",
    "}\n",
    "\n",
    "inputs_fail_no_details = {\n",
    "    \"raw_item_name\": \"TestItem2\",\n",
    "    \"raw_item_details\": [], \n",
    "    \"processing_threshold\": 1\n",
    "}\n",
    "\n",
    "inputs_optional_threshold_not_provided = {\n",
    "    \"raw_item_name\": \"TestItem3\",\n",
    "    \"raw_item_details\": [\"detail C\"],\n",
    "    # processing_threshold は提供しない (OptionalなのでOK)\n",
    "}\n",
    "\n",
    "test_cases = {\n",
    "    \"Success Case\": inputs_success,\n",
    "    \"Failure Case (No Details)\": inputs_fail_no_details,\n",
    "    \"Success Case (Threshold Not Provided)\": inputs_optional_threshold_not_provided\n",
    "}\n",
    "\n",
    "for case_name, inputs_data in test_cases.items():\n",
    "    print(f\"\\n--- I/Oカスタマイズテスト: {case_name} ---\")\n",
    "    current_inputs = inputs_data.copy()\n",
    "    current_inputs.setdefault(\"messages\", []) \n",
    "    # Optionalでない内部状態キーで、初期化ノードで設定されるものは入力時に含めなくても良い\n",
    "    # 例: internal_counter, processed_data, error_message\n",
    "\n",
    "    final_output_state = app.invoke(current_inputs, {\"recursion_limit\": 10})\n",
    "    print(f\"Final Output State for {case_name}: {final_output_state}\")\n",
    "\n",
    "    if final_output_state.get(\"processed_data\"):\n",
    "        print(f\"  Processed Item ID: {final_output_state['processed_data']['item_id']}\")\n",
    "        print(f\"  Processed: {final_output_state['processed_data']['is_processed']}\")\n",
    "    if final_output_state.get(\"error_message\"):\n",
    "        print(f\"  Error: {final_output_state['error_message']}\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説012</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** `TypedDict` を使ってグラフの状態スキーマを定義する際、どのキーがグラフへの主要な「入力」として期待され、どのキーがグラフからの主要な「出力」として扱われるのかを明確に意識することの重要性を学びます。また、`Optional`型やデフォルト値の扱い方についても触れます。\n",
    "*   **コード解説:**\n",
    "    *   `ComplexIOState` (状態スキーマ):\n",
    "        *   **入力想定キー:** `raw_item_name`, `raw_item_details`, `processing_threshold` (これは `Optional` なので、入力時に省略可能)。これらはグラフ実行時に `invoke` や `stream` の `inputs` 引数で渡されることが期待されます。\n",
    "        *   **処理中キー:** `messages`, `internal_counter`。これらは主にグラフ内部の処理やログのために使われ、通常は入力時に指定しません（`messages` は `add_messages` のために空リストで初期化することがあります）。\n",
    "        *   **出力想定キー:** `processed_data` (処理成功時の結果), `error_message` (エラー発生時の情報)。これらのキーの値が、グラフ実行後の最終的な成果物となります。\n",
    "    *   `initialize_processing`ノード: 入力値を受け取り、処理に必要な内部状態（`internal_counter`など）や出力用キー（`processed_data`, `error_message`）を初期化します。\n",
    "    *   `main_processor`ノード: 主要な処理ロジックを担当します。入力された `raw_item_name` や `raw_item_details`、そして `processing_threshold` (入力されなければデフォルト値を使用) に基づいて処理を行い、成功すれば `processed_data` を、失敗すれば `error_message` を更新します。また、処理試行回数を `internal_counter` で管理します。\n",
    "    *   `check_status`ルーター関数: `error_message` があれば終了。`processed_data` があれば終了。それ以外（まだ処理が完了していない、またはエラーではないが成功もしていない）場合は `main_processor` に戻って処理を続行（リトライ/ループ）します。\n",
    "    *   グラフ実行時:\n",
    "        *   `inputs`辞書には、`ComplexIOState`で定義した入力想定キー（`raw_item_name`など）を指定します。\n",
    "        *   `invoke()` から返される `final_output_state` は、`ComplexIOState` と同じ構造の辞書です。この中から出力想定キー（`processed_data` や `error_message`）の値を確認することで、グラフの実行結果を得ます。\n",
    "*   **重要な点:**\n",
    "    *   状態スキーマ（`TypedDict`）は、グラフのインターフェース（入力と出力の形式）を定義する上で中心的な役割を果たします。\n",
    "    *   入力時にどのキーが必要で、どのキーがオプショナルか、そしてグラフ実行後にどのキーに出力結果が格納されるのかを明確に設計することが、再利用可能で理解しやすいグラフを作る上で重要です。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題013: 複数のLLM呼び出しを含むグラフ\n",
    "\n",
    "一つのグラフ内で、異なる役割やプロンプトを持つ複数のLLM呼び出しノードを組み込む方法を学びましょう。例えば、最初のLLMがアイデアを生成し、次のLLMがそのアイデアを評価・洗練する、といった連携が考えられます。この問題では、簡単な役割分担を持つ2つのLLMノードを直列に接続します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄013\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class MultiLLMState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    original_topic: str # ユーザーからの最初のトピック\n",
    "    generated_idea: str | None # アイデア生成LLMの出力\n",
    "    evaluated_idea: str | None # アイデア評価LLMの出力\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def get_topic(state: MultiLLMState):\n",
    "    topic = state[\"messages\"][-1].content\n",
    "    print(f\"get_topic: Original topic is '{topic}'\")\n",
    "    return {\"original_topic\": topic, \"messages\": [AIMessage(content=f\"Topic received: {topic}\")]}\n",
    "\n",
    "def idea_generation_node(state: MultiLLMState):\n",
    "    topic = state[\"original_topic\"]\n",
    "    print(f\"idea_generation_node: Generating idea for topic: '{topic}'\")\n",
    "    \n",
    "    # このLLM用のプロンプトテンプレート\n",
    "    prompt_template_idea = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたは新しいアイデアを生み出すのが得意なAIです。与えられたトピックに関して、ユニークで面白いアイデアを一つ提案してください。アイデアは簡潔に一行で述べてください。\"),\n",
    "        (\"human\", \"トピック: {topic}\")\n",
    "    ])\n",
    "    \n",
    "    # llm変数を使ってプロンプトを実行 (ノートブック冒頭で初期化された共通のllm)\n",
    "    chain = prompt_template_idea | llm \n",
    "    response = chain.invoke({\"topic\": topic})\n",
    "    idea = response.content.strip()\n",
    "    \n",
    "    print(f\"idea_generation_node: Generated idea: '{idea}'\")\n",
    "    return {\"generated_idea\": idea, \"messages\": [AIMessage(content=f\"Generated Idea: {idea}\")]}\n",
    "\n",
    "def idea_evaluation_node(state: MultiLLMState):\n",
    "    idea = state[\"generated_idea\"]\n",
    "    if not idea:\n",
    "        return {\"messages\": [AIMessage(content=\"No idea to evaluate.\")], \"evaluated_idea\": \"N/A\"}\n",
    "        \n",
    "    print(f\"idea_evaluation_node: Evaluating idea: '{idea}'\")\n",
    "    \n",
    "    # このLLM用のプロンプトテンプレート\n",
    "    prompt_template_eval = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたはアイデアを客観的に評価するのが得意なAIです。与えられたアイデアについて、その実現可能性と面白さを評価し、短いコメントを述べてください。\"),\n",
    "        (\"human\", \"評価対象のアイデア: {idea}\")\n",
    "    ])\n",
    "    \n",
    "    chain = ____ | llm # prompt_template_eval\n",
    "    response = chain.invoke({\"idea\": idea})\n",
    "    evaluation = response.content.strip()\n",
    "    \n",
    "    print(f\"idea_evaluation_node: Evaluation: '{evaluation}'\")\n",
    "    return {\"evaluated_idea\": evaluation, \"messages\": [AIMessage(content=f\"Evaluation: {evaluation}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(MultiLLMState) # StateGraph\n",
    "\n",
    "workflow.____(\"capture_topic\", get_topic) # add_node\n",
    "workflow.____(\"generate_idea\", idea_generation_node) # add_node\n",
    "workflow.____(\"evaluate_idea\", idea_evaluation_node) # add_node\n",
    "\n",
    "workflow.____(\"capture_topic\") # set_entry_point\n",
    "\n",
    "workflow.____(\"capture_topic\", \"generate_idea\") # add_edge\n",
    "workflow.____(\"generate_idea\", \"evaluate_idea\") # add_edge\n",
    "workflow.____(\"evaluate_idea\", ____) # add_edge, END\n",
    "\n",
    "app = workflow.____() # compile\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "topics_to_test = [\n",
    "    \"新しい料理のレシピ\",\n",
    "    \"未来の交通手段\",\n",
    "    \"週末の過ごし方\"\n",
    "]\n",
    "\n",
    "for topic_text in topics_to_test:\n",
    "    print(f\"\\n--- 複数LLM連携テスト (トピック: {topic_text}) ---\")\n",
    "    inputs = {\n",
    "        \"messages\": [HumanMessage(content=topic_text)],\n",
    "        \"original_topic\": \"\", # 初期化ノードで設定される\n",
    "        \"generated_idea\": None,\n",
    "        \"evaluated_idea\": None\n",
    "    }\n",
    "    final_state = app.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"Final State for topic '{topic_text}':\")\n",
    "    print(f\"  Original Topic: {final_state.get('original_topic')}\")\n",
    "    print(f\"  Generated Idea: {final_state.get('generated_idea')}\")\n",
    "    print(f\"  Evaluated Idea: {final_state.get('evaluated_idea')}\")\n",
    "    # print(f\"  Message History: {final_state.get('messages')}\") # 必要なら表示\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答013</summary>\n",
    "\n",
    "``````python\n",
    "# 解答013\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class MultiLLMState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    original_topic: str # ユーザーからの最初のトピック\n",
    "    generated_idea: str | None # アイデア生成LLMの出力\n",
    "    evaluated_idea: str | None # アイデア評価LLMの出力\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def get_topic(state: MultiLLMState):\n",
    "    topic = state[\"messages\"][-1].content\n",
    "    print(f\"get_topic: Original topic is '{topic}'\")\n",
    "    return {\"original_topic\": topic, \"messages\": [AIMessage(content=f\"Topic received: {topic}\")]}\n",
    "\n",
    "def idea_generation_node(state: MultiLLMState):\n",
    "    topic = state[\"original_topic\"]\n",
    "    print(f\"idea_generation_node: Generating idea for topic: '{topic}'\")\n",
    "    \n",
    "    prompt_template_idea = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたは新しいアイデアを生み出すのが得意なAIです。与えられたトピックに関して、ユニークで面白いアイデアを一つ提案してください。アイデアは簡潔に一行で述べてください。\"),\n",
    "        (\"human\", \"トピック: {topic}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt_template_idea | llm \n",
    "    response = chain.invoke({\"topic\": topic})\n",
    "    idea = response.content.strip()\n",
    "    \n",
    "    print(f\"idea_generation_node: Generated idea: '{idea}'\")\n",
    "    return {\"generated_idea\": idea, \"messages\": [AIMessage(content=f\"Generated Idea: {idea}\")]}\n",
    "\n",
    "def idea_evaluation_node(state: MultiLLMState):\n",
    "    idea = state[\"generated_idea\"]\n",
    "    if not idea:\n",
    "        return {\"messages\": [AIMessage(content=\"No idea to evaluate.\")], \"evaluated_idea\": \"N/A\"}\n",
    "        \n",
    "    print(f\"idea_evaluation_node: Evaluating idea: '{idea}'\")\n",
    "    \n",
    "    prompt_template_eval = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたはアイデアを客観的に評価するのが得意なAIです。与えられたアイデアについて、その実現可能性と面白さを評価し、短いコメントを述べてください。\"),\n",
    "        (\"human\", \"評価対象のアイデア: {idea}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt_template_eval | llm \n",
    "    response = chain.invoke({\"idea\": idea})\n",
    "    evaluation = response.content.strip()\n",
    "    \n",
    "    print(f\"idea_evaluation_node: Evaluation: '{evaluation}'\")\n",
    "    return {\"evaluated_idea\": evaluation, \"messages\": [AIMessage(content=f\"Evaluation: {evaluation}\")]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(MultiLLMState)\n",
    "\n",
    "workflow.add_node(\"capture_topic\", get_topic)\n",
    "workflow.add_node(\"generate_idea\", idea_generation_node)\n",
    "workflow.add_node(\"evaluate_idea\", idea_evaluation_node)\n",
    "\n",
    "workflow.set_entry_point(\"capture_topic\")\n",
    "\n",
    "workflow.add_edge(\"capture_topic\", \"generate_idea\")\n",
    "workflow.add_edge(\"generate_idea\", \"evaluate_idea\")\n",
    "workflow.add_edge(\"evaluate_idea\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "topics_to_test = [\n",
    "    \"新しい料理のレシピ\",\n",
    "    \"未来の交通手段\",\n",
    "    \"週末の過ごし方\"\n",
    "]\n",
    "\n",
    "for topic_text in topics_to_test:\n",
    "    print(f\"\\n--- 複数LLM連携テスト (トピック: {topic_text}) ---\")\n",
    "    inputs = {\n",
    "        \"messages\": [HumanMessage(content=topic_text)],\n",
    "        # original_topic, generated_idea, evaluated_idea はグラフ内で設定されるので初期値はNoneや空文字でOK\n",
    "        \"original_topic\": \"\", \n",
    "        \"generated_idea\": None,\n",
    "        \"evaluated_idea\": None\n",
    "    }\n",
    "    final_state = app.invoke(inputs, {\"recursion_limit\": 5})\n",
    "    print(f\"Final State for topic '{topic_text}':\")\n",
    "    print(f\"  Original Topic: {final_state.get('original_topic')}\")\n",
    "    print(f\"  Generated Idea: {final_state.get('generated_idea')}\")\n",
    "    print(f\"  Evaluated Idea: {final_state.get('evaluated_idea')}\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説013</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** 一つのグラフ内に、それぞれ異なるプロンプトや役割を持つ複数のLLM呼び出しノードを配置し、それらを連携させる方法を学びます。これにより、より複雑で多段階の思考や処理を行うエージェントやパイプラインを構築できます。\n",
    "*   **コード解説:**\n",
    "    *   `MultiLLMState`には、ユーザーからの最初のトピック (`original_topic`)、最初のLLMが生成したアイデア (`generated_idea`)、そして二番目のLLMが評価した結果 (`evaluated_idea`) を保持するキーが定義されています。\n",
    "    *   `get_topic`ノード: ユーザーの入力を `original_topic` として状態に保存します。\n",
    "    *   `idea_generation_node`: `original_topic` に基づいて、アイデア生成用のプロンプト (`prompt_template_idea`) を使用してLLMを呼び出し、結果を `generated_idea` に保存します。\n",
    "    *   `idea_evaluation_node`: `generated_idea` に基づいて、アイデア評価用のプロンプト (`prompt_template_eval`) を使用してLLMを呼び出し（ここでも同じ `llm` インスタンスを使用していますが、プロンプトが異なるため役割が変わります）、結果を `evaluated_idea` に保存します。\n",
    "    *   グラフは `capture_topic` -> `generate_idea` -> `evaluate_idea` -> `END` という直列な流れで、各ステップで状態が更新されていきます。\n",
    "    *   各LLM呼び出しノード内では、`langchain_core.prompts.ChatPromptTemplate` を使ってそのノード専用のプロンプトを定義し、共通の `llm` インスタンスと組み合わせて (例: `chain = prompt_template | llm`) LLM呼び出しを行っています。これにより、同じLLMモデルでも異なる指示を与えることで、多様な処理を実現できます。\n",
    "*   **応用例:**\n",
    "    *   リサーチアシスタント: 質問受け付け -> 情報検索プロンプトでLLM -> 要約プロンプトでLLM -> 報告書作成プロンプトでLLM。\n",
    "    *   コード生成・レビュー: 要件定義 -> コード生成LLM -> 生成コード評価LLM -> 修正指示LLM。\n",
    "    *   このように、タスクを細分化し、各サブタスクに特化したプロンプトを持つLLMノードを連携させることで、より高品質な結果を得ることが期待できます。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題014: エラーハンドリングの準備 (シンプルなtry-except)\n",
    "\n",
    "グラフ内のノード処理中にエラーが発生する可能性は常にあります（例: 外部API呼び出しの失敗、予期しないデータ形式など）。この問題では、ノード関数内で基本的な`try-except`ブロックを使い、エラーを捕捉して状態にエラー情報を記録し、処理を継続または安全に終了させる方法の基礎を学びます。これは第2章でより詳しく扱う「エラーハンドリング」の準備となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄014\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ErrorHandlingState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    input_value: str\n",
    "    processed_value: Optional[int] # 処理結果 (数値のはず)\n",
    "    error_info: Optional[str]      # エラー発生時の情報\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def risky_converter_node(state: ErrorHandlingState):\n",
    "    raw_value = state[\"input_value\"]\n",
    "    print(f\"risky_converter_node: Attempting to convert '{raw_value}' to integer.\")\n",
    "    \n",
    "    try:\n",
    "        # ここでエラーが発生する可能性のある処理\n",
    "        converted_int = int(raw_value)\n",
    "        print(f\"  Successfully converted to {converted_int}\")\n",
    "        return {\n",
    "            \"processed_value\": ____, # converted_int\n",
    "            \"error_info\": None, # エラーなし\n",
    "            \"messages\": [AIMessage(content=f\"Successfully converted '{raw_value}' to {converted_int}.\")]\n",
    "        }\n",
    "    except ValueError as e:\n",
    "        # エラーを捕捉した場合\n",
    "        error_message = f\"Conversion failed: {str(e)}\"\n",
    "        print(f\"  Error: {error_message}\")\n",
    "        return {\n",
    "            \"processed_value\": None,\n",
    "            \"error_info\": ____, # error_message\n",
    "            \"messages\": [AIMessage(content=f\"Error converting '{raw_value}': {error_message}\")]\n",
    "        }\n",
    "\n",
    "def result_router(state: ErrorHandlingState):\n",
    "    # error_info があればエラー処理へ、なければ成功処理へ\n",
    "    if state.get(\"error_info\"):\n",
    "        print(\"result_router: Error detected, routing to error_handler.\")\n",
    "        return \"handle_error\"\n",
    "    else:\n",
    "        print(\"result_router: No error, routing to success_logger.\")\n",
    "        return \"log_success\"\n",
    "\n",
    "def success_log_node(state: ErrorHandlingState):\n",
    "    log_msg = f\"Success! Processed value: {state['processed_value']}\"\n",
    "    print(f\"success_log_node: {log_msg}\")\n",
    "    return {\"messages\": [AIMessage(content=log_msg)]}\n",
    "\n",
    "def error_handler_node(state: ErrorHandlingState):\n",
    "    log_msg = f\"Handling error: {state['error_info']}. Input was '{state['input_value']}'.\"\n",
    "    print(f\"error_handler_node: {log_msg}\")\n",
    "    # ここでさらにエラーに応じた処理（例: デフォルト値設定、ユーザー通知など）も可能\n",
    "    return {\"messages\": [AIMessage(content=log_msg)]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(ErrorHandlingState) # StateGraph\n",
    "\n",
    "workflow.____(\"converter\", risky_converter_node) # add_node\n",
    "workflow.____(\"success_logger\", success_log_node) # add_node\n",
    "workflow.____(\"error_handler\", error_handler_node) # add_node\n",
    "\n",
    "workflow.____(\"converter\") # set_entry_point\n",
    "\n",
    "workflow.____( # add_conditional_edges\n",
    "    \"converter\",\n",
    "    result_router,\n",
    "    {\n",
    "        \"log_success\": \"success_logger\",\n",
    "        \"handle_error\": \"error_handler\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.____(\"success_logger\", ____) # add_edge, END\n",
    "workflow.____(\"error_handler\", ____) # add_edge, END\n",
    "\n",
    "app = workflow.____() # compile\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "test_inputs_map = {\n",
    "    \"Valid Input\": {\"input_value\": \"123\"},\n",
    "    \"Invalid Input\": {\"input_value\": \"abc\"},\n",
    "    \"Empty Input\": {\"input_value\": \"\"}\n",
    "}\n",
    "\n",
    "for test_name, data in test_inputs_map.items():\n",
    "    print(f\"\\n--- エラーハンドリングテスト: {test_name} (Input: '{data['input_value']}') ---\")\n",
    "    # messages は add_messages のために空リストで初期化\n",
    "    current_data = data.copy()\n",
    "    current_data.setdefault(\"messages\", [])\n",
    "    current_data.setdefault(\"processed_value\", None)\n",
    "    current_data.setdefault(\"error_info\", None)\n",
    "\n",
    "    final_state = app.invoke(current_data, {\"recursion_limit\": 5})\n",
    "    print(f\"Final State for {test_name}: {final_state}\")\n",
    "    if final_state.get(\"error_info\"):\n",
    "        print(f\"  Error Info: {final_state['error_info']}\")\n",
    "    else:\n",
    "        print(f\"  Processed Value: {final_state.get('processed_value')}\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答014</summary>\n",
    "\n",
    "``````python\n",
    "# 解答014\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "class ErrorHandlingState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    input_value: str\n",
    "    processed_value: Optional[int] # 処理結果 (数値のはず)\n",
    "    error_info: Optional[str]      # エラー発生時の情報\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def risky_converter_node(state: ErrorHandlingState):\n",
    "    raw_value = state[\"input_value\"]\n",
    "    print(f\"risky_converter_node: Attempting to convert '{raw_value}' to integer.\")\n",
    "    \n",
    "    try:\n",
    "        converted_int = int(raw_value)\n",
    "        print(f\"  Successfully converted to {converted_int}\")\n",
    "        return {\n",
    "            \"processed_value\": converted_int,\n",
    "            \"error_info\": None, \n",
    "            \"messages\": [AIMessage(content=f\"Successfully converted '{raw_value}' to {converted_int}.\")]\n",
    "        }\n",
    "    except ValueError as e:\n",
    "        error_message = f\"Conversion failed: {str(e)}\"\n",
    "        print(f\"  Error: {error_message}\")\n",
    "        return {\n",
    "            \"processed_value\": None,\n",
    "            \"error_info\": error_message,\n",
    "            \"messages\": [AIMessage(content=f\"Error converting '{raw_value}': {error_message}\")]\n",
    "        }\n",
    "\n",
    "def result_router(state: ErrorHandlingState):\n",
    "    if state.get(\"error_info\"):\n",
    "        print(\"result_router: Error detected, routing to error_handler.\")\n",
    "        return \"handle_error\"\n",
    "    else:\n",
    "        print(\"result_router: No error, routing to success_logger.\")\n",
    "        return \"log_success\"\n",
    "\n",
    "def success_log_node(state: ErrorHandlingState):\n",
    "    log_msg = f\"Success! Processed value: {state['processed_value']}\"\n",
    "    print(f\"success_log_node: {log_msg}\")\n",
    "    return {\"messages\": [AIMessage(content=log_msg)]}\n",
    "\n",
    "def error_handler_node(state: ErrorHandlingState):\n",
    "    log_msg = f\"Handling error: {state['error_info']}. Input was '{state['input_value']}'.\"\n",
    "    print(f\"error_handler_node: {log_msg}\")\n",
    "    return {\"messages\": [AIMessage(content=log_msg)]}\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(ErrorHandlingState)\n",
    "\n",
    "workflow.add_node(\"converter\", risky_converter_node)\n",
    "workflow.add_node(\"success_logger\", success_log_node)\n",
    "workflow.add_node(\"error_handler\", error_handler_node)\n",
    "\n",
    "workflow.set_entry_point(\"converter\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"converter\",\n",
    "    result_router,\n",
    "    {\n",
    "        \"log_success\": \"success_logger\",\n",
    "        \"handle_error\": \"error_handler\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"success_logger\", END)\n",
    "workflow.add_edge(\"error_handler\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "test_inputs_map = {\n",
    "    \"Valid Input\": {\"input_value\": \"123\"},\n",
    "    \"Invalid Input\": {\"input_value\": \"abc\"},\n",
    "    \"Empty Input\": {\"input_value\": \"\"}\n",
    "}\n",
    "\n",
    "for test_name, data in test_inputs_map.items():\n",
    "    print(f\"\\n--- エラーハンドリングテスト: {test_name} (Input: '{data['input_value']}') ---\")\n",
    "    current_data = data.copy()\n",
    "    current_data.setdefault(\"messages\", [])\n",
    "    current_data.setdefault(\"processed_value\", None)\n",
    "    current_data.setdefault(\"error_info\", None)\n",
    "\n",
    "    final_state = app.invoke(current_data, {\"recursion_limit\": 5})\n",
    "    print(f\"Final State for {test_name}: {final_state}\")\n",
    "    if final_state.get(\"error_info\"):\n",
    "        print(f\"  Error Info: {final_state['error_info']}\")\n",
    "    else:\n",
    "        print(f\"  Processed Value: {final_state.get('processed_value')}\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説014</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** ノード処理中の潜在的なエラー（この例では文字列から整数への型変換失敗）を`try-except`ブロックで捕捉し、その結果（成功か失敗か、エラー情報など）をグラフの状態(`State`)に記録する方法を学びます。さらに、状態に記録されたエラー情報に基づいて、条件付きエッジを使って処理を分岐させ、成功時とエラー時で異なる後続処理を行う方法も理解します。\n",
    "*   **コード解説:**\n",
    "    *   `ErrorHandlingState`には、処理対象の入力 `input_value`、処理後の値 `processed_value`、そしてエラー情報を格納する `error_info` が定義されています。\n",
    "    *   `risky_converter_node`: `input_value` を整数に変換しようとします。\n",
    "        *   成功した場合: `processed_value` に変換後の整数を、`error_info` に `None` を設定して返します。\n",
    "        *   `ValueError` が発生した場合 (例: \"abc\"を整数に変換しようとした): `except` ブロックでエラーを捕捉し、`processed_value` を `None` に、`error_info` にエラーメッセージ文字列を設定して返します。\n",
    "    *   `result_router`ルーター関数: `state['error_info']` が存在するかどうか（つまり、エラーが発生したかどうか）を確認します。\n",
    "        *   エラーがあれば `\"handle_error\"` を返し、`error_handler_node` に処理を分岐させます。\n",
    "        *   エラーがなければ `\"log_success\"` を返し、`success_log_node` に処理を分岐させます。\n",
    "    *   `success_log_node`: 成功メッセージをログに記録します。\n",
    "    *   `error_handler_node`: エラー発生時の情報をログに記録します。実際のアプリケーションでは、ここでエラーに応じたフォールバック処理やユーザーへの通知などを行うことができます。\n",
    "    *   グラフは、入力 -> `converter` -> (成功なら `success_logger` / エラーなら `error_handler`) -> `END` という流れになります。\n",
    "*   **重要性:**\n",
    "    *   堅牢なアプリケーションを構築するためには、予期せぬエラーへの対処が不可欠です。LangGraphにおいても、各ノードの処理でエラーが発生する可能性を考慮し、適切にハンドリングすることで、グラフ全体の安定性を高めることができます。\n",
    "    *   この問題で示した方法は基本的なエラーハンドリングですが、より高度なエラー処理（リトライ、サーキットブレーカーなど）もLangGraphのループ構造や条件分岐を応用して実装可能です（これらは第2章以降のトピックとなります）。\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題015: 第1章のまとめ - 簡単なQ&Aボットの改善\n",
    "\n",
    "第1章で学んだ様々な要素（状態管理、LLM連携、条件分岐、ループ、情報抽出など）を組み合わせて、問題004で作成したシンプルなチャットボットを改善してみましょう。このQ&Aボットは、ユーザーの質問のタイプ（例: 単純な挨拶、知識を問う質問、不明な質問）を判別し、応答を変化させたり、会話の回数をカウントしたりする機能を持つようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄015\n",
    "from typing import TypedDict, Annotated, Literal, Optional\n",
    "import re\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "QuestionCategory = Literal[\"greeting\", \"knowledge_q\", \"opinion_q\", \"unknown_q\"]\n",
    "\n",
    "class AdvancedQABotState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_input: str # 最新のユーザー入力\n",
    "    question_category: Optional[QuestionCategory] # 質問のカテゴリ\n",
    "    llm_response: Optional[str] # LLMからの最終的な応答\n",
    "    conversation_turns: int # 会話のターン数\n",
    "    max_turns: int # 最大許容ターン数\n",
    "    error_message: Optional[str]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def capture_input_and_increment_turn(state: AdvancedQABotState):\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    current_turns = state.get(\"conversation_turns\", 0) + 1\n",
    "    print(f\"capture_input_and_increment_turn: User: '{user_message}', Turn: {current_turns}\")\n",
    "    return {\n",
    "        \"user_input\": user_message,\n",
    "        \"conversation_turns\": current_turns,\n",
    "        \"question_category\": None, # 各処理の前に初期化\n",
    "        \"llm_response\": None, # 各処理の前に初期化\n",
    "        \"error_message\": None # 各処理の前に初期化\n",
    "    }\n",
    "\n",
    "def categorize_question_node(state: AdvancedQABotState):\n",
    "    text = state[\"user_input\"].lower()\n",
    "    category: QuestionCategory = \"unknown_q\"\n",
    "    \n",
    "    # 簡単なキーワードベースのカテゴリ分け（LLMを使っても良い）\n",
    "    if any(greet in text for greet in [\"こんにちは\", \"やあ\", \"どうも\"]):\n",
    "        category = \"greeting\"\n",
    "    elif any(q_word in text for q_word in [\"何ですか\", \"教えて\", \"とは\", \"なぜ\"]) or \"?\" in text:\n",
    "        # ここでは知識を問う質問と意見を問う質問を単純に分けるのは難しいので、一旦「知識」としておく\n",
    "        category = \"knowledge_q\"\n",
    "        if any(opinion_word in text for opinion_word in [\"どう思う\", \"意見は\"]):\n",
    "             category = \"opinion_q\"\n",
    "    \n",
    "    print(f\"categorize_question_node: Input '{text}' categorized as '{category}'\")\n",
    "    return {\"question_category\": category, \"messages\": [AIMessage(content=f\"Category: {category}\")]}\n",
    "\n",
    "def greeting_responder_node(state: AdvancedQABotState):\n",
    "    response = \"こんにちは！ご用件は何でしょうか？\"\n",
    "    print(f\"greeting_responder_node: Responding with '{response}'\")\n",
    "    return {\"llm_response\": response}\n",
    "\n",
    "def knowledge_llm_node(state: AdvancedQABotState):\n",
    "    question = state[\"user_input\"]\n",
    "    print(f\"knowledge_llm_node: Asking LLM (knowledge): '{question}'\")\n",
    "    # 知識ベースの質問応答用プロンプト (問題013のように専用プロンプトも可)\n",
    "    # ここではシンプルにそのまま質問\n",
    "    response_obj = llm.invoke([HumanMessage(content=question)])\n",
    "    response_text = response_obj.content.strip()\n",
    "    print(f\"  LLM response: {response_text}\")\n",
    "    return {\"llm_response\": response_text}\n",
    "\n",
    "def opinion_llm_node(state: AdvancedQABotState):\n",
    "    question = state[\"user_input\"]\n",
    "    print(f\"opinion_llm_node: Asking LLM (opinion): '{question}'\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたは様々なトピックについて個人的な意見を述べることができるAIです。ただし、客観的な事実と意見を区別して話してください。\"),\n",
    "        (\"human\", \"{user_question}\")\n",
    "    ])\n",
    "    chain = prompt | llm\n",
    "    response_obj = chain.invoke({\"user_question\": question})\n",
    "    response_text = response_obj.content.strip()\n",
    "    print(f\"  LLM response: {response_text}\")\n",
    "    return {\"llm_response\": response_text}\n",
    "\n",
    "def unknown_question_node(state: AdvancedQABotState):\n",
    "    response = \"申し訳ありません、よく理解できませんでした。別の言葉で質問していただけますか？\"\n",
    "    print(f\"unknown_question_node: Responding with '{response}'\")\n",
    "    return {\"llm_response\": response}\n",
    "\n",
    "def final_response_node(state: AdvancedQABotState):\n",
    "    # 最終的な応答をmessagesに追加\n",
    "    final_resp = state.get(\"llm_response\", \"(No response generated)\")\n",
    "    print(f\"final_response_node: Final AI response: '{final_resp}'\")\n",
    "    return {\"messages\": [AIMessage(content=final_resp)]}\n",
    "\n",
    "# --- ルーター関数 ---\n",
    "def route_by_category(state: AdvancedQABotState):\n",
    "    category = state.get(\"question_category\")\n",
    "    print(f\"route_by_category: Routing based on category '{category}'\")\n",
    "    if category == \"greeting\":\n",
    "        return \"greeting_responder\"\n",
    "    elif category == \"knowledge_q\":\n",
    "        return \"knowledge_llm\"\n",
    "    elif category == \"opinion_q\":\n",
    "        return \"opinion_llm\"\n",
    "    else: # unknown_q or None\n",
    "        return \"unknown_responder\"\n",
    "\n",
    "def check_conversation_limit(state: AdvancedQABotState):\n",
    "    current_turns = state.get(\"conversation_turns\", 0)\n",
    "    max_t = state.get(\"max_turns\", 3)\n",
    "    if current_turns >= max_t:\n",
    "        print(f\"check_conversation_limit: Max turns ({max_t}) reached. Ending conversation.\")\n",
    "        # 最後の応答を生成するために、一旦 final_response_node には行かせる\n",
    "        # もし、ここで完全に終了させたいなら、ENDに直接つなぐノードを用意する\n",
    "        return \"__end__\" # 直接ENDへ\n",
    "    print(f\"check_conversation_limit: Turn {current_turns}/{max_t}. Continuing.\")\n",
    "    return \"continue_conversation\" # 通常はカテゴリ分類へ\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = ____(AdvancedQABotState) # StateGraph\n",
    "\n",
    "workflow.add_node(\"input_handler\", capture_input_and_increment_turn)\n",
    "workflow.add_node(\"categorizer\", categorize_question_node)\n",
    "workflow.add_node(\"greeting_responder\", greeting_responder_node)\n",
    "workflow.add_node(\"knowledge_llm\", knowledge_llm_node)\n",
    "workflow.add_node(\"opinion_llm\", opinion_llm_node)\n",
    "workflow.add_node(\"unknown_responder\", unknown_question_node)\n",
    "workflow.add_node(\"final_responder\", final_response_node)\n",
    "\n",
    "workflow.set_entry_point(\"input_handler\")\n",
    "\n",
    "# 入力後、会話ターン数チェック\n",
    "workflow.add_conditional_edges(\n",
    "    \"input_handler\",\n",
    "    check_conversation_limit,\n",
    "    {\n",
    "        \"continue_conversation\": \"categorizer\",\n",
    "        \"__end__\": END # 最大ターン数に達したら終了\n",
    "    }\n",
    ")\n",
    "\n",
    "# カテゴリ分類後、各処理ノードへ\n",
    "workflow.add_conditional_edges(\n",
    "    \"categorizer\",\n",
    "    route_by_category,\n",
    "    {\n",
    "        \"greeting_responder\": \"greeting_responder\",\n",
    "        \"knowledge_llm\": \"knowledge_llm\",\n",
    "        \"opinion_llm\": \"opinion_llm\",\n",
    "        \"unknown_responder\": \"unknown_responder\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 各応答生成ノードから最終応答整形ノードへ\n",
    "workflow.add_edge(\"greeting_responder\", \"final_responder\")\n",
    "workflow.add_edge(\"knowledge_llm\", \"final_responder\")\n",
    "workflow.add_edge(\"opinion_llm\", \"final_responder\")\n",
    "workflow.add_edge(\"unknown_responder\", \"final_responder\")\n",
    "\n",
    "# 最終応答整形後、終了\n",
    "workflow.add_edge(\"final_responder\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 ---\n",
    "chat_history = []\n",
    "max_total_turns = 3 # このデモでの最大会話ターン数\n",
    "\n",
    "print(f\"--- Q&Aボット改善テスト (最大{max_total_turns}ターン) ---\")\n",
    "for turn in range(max_total_turns * 2): # ユーザー入力の機会を多めに用意\n",
    "    user_q = input(f\"You (Turn {turn//2 + 1}): \")\n",
    "    if user_q.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting chat.\")\n",
    "        break\n",
    "    \n",
    "    chat_history.append(HumanMessage(content=user_q))\n",
    "    inputs = {\n",
    "        \"messages\": chat_history,\n",
    "        \"max_turns\": max_total_turns \n",
    "        # conversation_turns は capture_input_and_increment_turn で初期化/更新\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        final_bot_state = app.invoke(inputs, {\"recursion_limit\": 15})\n",
    "        if final_bot_state and final_bot_state.get(\"messages\"):\n",
    "            bot_response_message = final_bot_state[\"messages\"][-1]\n",
    "            if isinstance(bot_response_message, AIMessage):\n",
    "                print(f\"Bot: {bot_response_message.content}\")\n",
    "                chat_history.append(bot_response_message)\n",
    "            else: # ENDに直接到達した場合など、AIMessageがない場合\n",
    "                print(\"(Bot ended conversation due to max turns or other condition)\")\n",
    "                if final_bot_state.get(\"llm_response\"):\n",
    "                     print(f\"Bot (last intended response): {final_bot_state['llm_response']}\") # END直前の応答を表示試行\n",
    "                break \n",
    "        else:\n",
    "            print(\"(No response from bot or bot ended)\")\n",
    "            break\n",
    "\n",
    "        # conversation_turns が max_turns に達したか、それ以上ならループを抜ける\n",
    "        if final_bot_state.get(\"conversation_turns\", 0) >= final_bot_state.get(\"max_turns\", max_total_turns):\n",
    "            print(\"Max conversation turns reached from bot's perspective.\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during bot invocation: {e}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n--- Final Chat History ---\")\n",
    "for msg in chat_history:\n",
    "    print(f\"  {msg.type.upper()}: {msg.content}\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答015</summary>\n",
    "\n",
    "``````python\n",
    "# 解答015\n",
    "from typing import TypedDict, Annotated, Literal, Optional\n",
    "import re\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ノートブック冒頭で`llm`変数が初期化されている前提\n",
    "\n",
    "# --- 状態定義 (State) ---\n",
    "QuestionCategory = Literal[\"greeting\", \"knowledge_q\", \"opinion_q\", \"unknown_q\"]\n",
    "\n",
    "class AdvancedQABotState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_input: str # 最新のユーザー入力\n",
    "    question_category: Optional[QuestionCategory] # 質問のカテゴリ\n",
    "    llm_response: Optional[str] # LLMからの最終的な応答\n",
    "    conversation_turns: int # 会話のターン数\n",
    "    max_turns: int # 最大許容ターン数\n",
    "    error_message: Optional[str]\n",
    "\n",
    "# --- ノード定義 (Nodes) ---\n",
    "def capture_input_and_increment_turn(state: AdvancedQABotState):\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    # messages以外のキーは、invokeの入力で与えられなければ、前のターンの値が残る。\n",
    "    # conversation_turns は入力で初期値0を渡すか、ここで .get(key, default) を使う。\n",
    "    current_turns = state.get(\"conversation_turns\", 0) + 1\n",
    "    print(f\"capture_input_and_increment_turn: User: '{user_message}', Turn: {current_turns}\")\n",
    "    return {\n",
    "        \"user_input\": user_message,\n",
    "        \"conversation_turns\": current_turns,\n",
    "        \"question_category\": None, \n",
    "        \"llm_response\": None, \n",
    "        \"error_message\": None \n",
    "    }\n",
    "\n",
    "def categorize_question_node(state: AdvancedQABotState):\n",
    "    text = state[\"user_input\"].lower()\n",
    "    category: QuestionCategory = \"unknown_q\"\n",
    "    \n",
    "    if any(greet in text for greet in [\"こんにちは\", \"やあ\", \"どうも\", \"hello\", \"hi\"]):\n",
    "        category = \"greeting\"\n",
    "    elif any(q_word in text for q_word in [\"とは\", \"何ですか\", \"教えて\", \"なぜ\", \"what is\", \"tell me about\", \"why\"]) or \"?\" in text:\n",
    "        if any(opinion_word in text for opinion_word in [\"どう思う\", \"あなたの意見は\", \"what do you think about\"]):\n",
    "             category = \"opinion_q\"\n",
    "        else:\n",
    "            category = \"knowledge_q\"\n",
    "    \n",
    "    print(f\"categorize_question_node: Input '{text}' categorized as '{category}'\")\n",
    "    return {\"question_category\": category, \"messages\": [AIMessage(content=f\"Category determination: {category}\")]}\n",
    "\n",
    "def greeting_responder_node(state: AdvancedQABotState):\n",
    "    response = \"こんにちは！何かお手伝いできることはありますか？\"\n",
    "    print(f\"greeting_responder_node: Responding with '{response}'\")\n",
    "    return {\"llm_response\": response}\n",
    "\n",
    "def knowledge_llm_node(state: AdvancedQABotState):\n",
    "    question = state[\"user_input\"]\n",
    "    print(f\"knowledge_llm_node: Asking LLM (knowledge): '{question}'\")\n",
    "    response_obj = llm.invoke([HumanMessage(content=question)])\n",
    "    response_text = response_obj.content.strip()\n",
    "    print(f\"  LLM response: {response_text}\")\n",
    "    return {\"llm_response\": response_text}\n",
    "\n",
    "def opinion_llm_node(state: AdvancedQABotState):\n",
    "    question = state[\"user_input\"]\n",
    "    print(f\"opinion_llm_node: Asking LLM (opinion): '{question}'\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"あなたは様々なトピックについて個人的な意見を述べることができるAIです。客観的な事実とあなたの意見を区別して話してください。\"),\n",
    "        (\"human\", \"{user_question}\")\n",
    "    ])\n",
    "    chain = prompt | llm\n",
    "    response_obj = chain.invoke({\"user_question\": question})\n",
    "    response_text = response_obj.content.strip()\n",
    "    print(f\"  LLM response: {response_text}\")\n",
    "    return {\"llm_response\": response_text}\n",
    "\n",
    "def unknown_question_node(state: AdvancedQABotState):\n",
    "    response = \"申し訳ありませんが、ご質問の意図を正確に理解できませんでした。もう少し具体的に、または別の言葉で質問していただけますでしょうか？\"\n",
    "    print(f\"unknown_question_node: Responding with '{response}'\")\n",
    "    return {\"llm_response\": response}\n",
    "\n",
    "def final_response_node(state: AdvancedQABotState):\n",
    "    final_resp = state.get(\"llm_response\", \"(AI did not generate a response for some reason)\")\n",
    "    print(f\"final_response_node: Final AI response to be added to messages: '{final_resp}'\")\n",
    "    return {\"messages\": [AIMessage(content=final_resp)]}\n",
    "\n",
    "# --- ルーター関数 ---\n",
    "def route_by_category(state: AdvancedQABotState):\n",
    "    category = state.get(\"question_category\")\n",
    "    print(f\"route_by_category: Routing based on category '{category}'\")\n",
    "    if category == \"greeting\": return \"greeting_responder\"\n",
    "    if category == \"knowledge_q\": return \"knowledge_llm\"\n",
    "    if category == \"opinion_q\": return \"opinion_llm\"\n",
    "    return \"unknown_responder\"\n",
    "\n",
    "def check_conversation_limit(state: AdvancedQABotState):\n",
    "    current_turns = state.get(\"conversation_turns\", 0)\n",
    "    max_t = state.get(\"max_turns\", 3) # デフォルトの最大ターン数を設定\n",
    "    if current_turns >= max_t:\n",
    "        print(f\"check_conversation_limit: Max turns ({max_t}) reached for this interaction. Ending conversation.\")\n",
    "        # 最後の応答をユーザーに返すために、ENDの前に final_response_node を挟むこともできるが、\n",
    "        # ここでは、最大ターンに達したら即座に終了するロジックとする。\n",
    "        # その場合、最後のユーザー入力に対する応答は生成されない。\n",
    "        # もし「最大ターンに達したので終了します」というメッセージを返したい場合は、\n",
    "        # このルーターから専用の終了メッセージ生成ノードに繋ぎ、そこからENDへ。\n",
    "        return \"__end__\" \n",
    "    print(f\"check_conversation_limit: Turn {current_turns}/{max_t}. Continuing to categorize.\")\n",
    "    return \"continue_to_categorizer\"\n",
    "\n",
    "# --- グラフ構築 (Graph) ---\n",
    "workflow = StateGraph(AdvancedQABotState)\n",
    "\n",
    "workflow.add_node(\"input_handler\", capture_input_and_increment_turn)\n",
    "workflow.add_node(\"categorizer\", categorize_question_node)\n",
    "workflow.add_node(\"greeting_responder\", greeting_responder_node)\n",
    "workflow.add_node(\"knowledge_llm\", knowledge_llm_node)\n",
    "workflow.add_node(\"opinion_llm\", opinion_llm_node)\n",
    "workflow.add_node(\"unknown_responder\", unknown_question_node)\n",
    "workflow.add_node(\"final_responder\", final_response_node)\n",
    "\n",
    "workflow.set_entry_point(\"input_handler\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"input_handler\",\n",
    "    check_conversation_limit,\n",
    "    {\n",
    "        \"continue_to_categorizer\": \"categorizer\",\n",
    "        \"__end__\": END \n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"categorizer\",\n",
    "    route_by_category,\n",
    "    {\n",
    "        \"greeting_responder\": \"greeting_responder\",\n",
    "        \"knowledge_llm\": \"knowledge_llm\",\n",
    "        \"opinion_llm\": \"opinion_llm\",\n",
    "        \"unknown_responder\": \"unknown_responder\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"greeting_responder\", \"final_responder\")\n",
    "workflow.add_edge(\"knowledge_llm\", \"final_responder\")\n",
    "workflow.add_edge(\"opinion_llm\", \"final_responder\")\n",
    "workflow.add_edge(\"unknown_responder\", \"final_responder\")\n",
    "workflow.add_edge(\"final_responder\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- グラフの実行と結果表示 (インタラクティブテスト) ---\n",
    "chat_history_for_bot = []\n",
    "max_dialogue_turns = 3 # このインタラクティブセッションでの最大会話往復数\n",
    "\n",
    "print(f\"--- 第1章まとめ Q&Aボット (最大{max_dialogue_turns}往復) ---\")\n",
    "print(\"チャットを開始します。'exit' または 'quit' で終了します。\")\n",
    "\n",
    "for i in range(max_dialogue_turns):\n",
    "    user_text = input(f\"あなた (Turn {i + 1}): \")\n",
    "    if user_text.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"チャットを終了します。\")\n",
    "        break\n",
    "    \n",
    "    chat_history_for_bot.append(HumanMessage(content=user_text))\n",
    "    \n",
    "    # conversation_turnsはグラフ内で管理されるが、max_turnsは外部から指定\n",
    "    # messages以外のキーは、前回のinvokeの出力状態が引き継がれるが、\n",
    "    # 毎回クリーンな状態で始めたい場合は、ここで初期値を設定する。\n",
    "    # この例では、conversation_turnsはグラフ内でインクリメントされる想定。\n",
    "    current_invoke_inputs = {\n",
    "        \"messages\": chat_history_for_bot,\n",
    "        \"max_turns\": max_dialogue_turns, # グラフ内の会話ターン数上限\n",
    "        \"conversation_turns\": i # 現在のinvoke呼び出しが何ターン目か（グラフ内部のカウンタとは別管理）\n",
    "                                 # もしくは、グラフに初期ターン数を渡すなら conversation_turns: 0 のようにする\n",
    "                                 # ここでは capture_input_and_increment_turn で conversation_turns を\n",
    "                                 # state.get(\"conversation_turns\", 0) + 1 で更新するので、\n",
    "                                 # invokeのたびに初期化されるか、前回の値が引き継がれる。\n",
    "                                 # 簡単のため、毎回 fresh な conversation_turns で始める場合は\n",
    "                                 # invoke の input に conversation_turns: 0 を含めるか、\n",
    "                                 # capture_input_and_increment_turn の中で messages の長さなどから判断する。\n",
    "                                 # ここでは、デモとして conversation_turns はグラフ内でインクリメントされる想定\n",
    "                                 # ただし、max_turns は invoke 時に渡す必要がある。\n",
    "    }\n",
    "    if i == 0: # 最初のターンのみ conversation_turns を明示的に0に設定\n",
    "        current_invoke_inputs[\"conversation_turns\"] = 0\n",
    "\n",
    "    try:\n",
    "        # streamで途中経過を見る場合\n",
    "        # print(\"\\nBot thinking...\")\n",
    "        # for event in app.stream(current_invoke_inputs, {\"recursion_limit\": 25}):\n",
    "        #    print(f\"  Stream event: {event}\")\n",
    "        # final_bot_state = event[list(event.keys())[-1]] # streamの最後のイベントのデータ部分\n",
    "\n",
    "        final_bot_state = app.invoke(current_invoke_inputs, {\"recursion_limit\": 25})\n",
    "\n",
    "        if final_bot_state and final_bot_state.get(\"messages\"):\n",
    "            # messagesリストの最後のAIメッセージを取得\n",
    "            ai_messages = [m for m in final_bot_state[\"messages\"] if isinstance(m, AIMessage)]\n",
    "            if ai_messages:\n",
    "                # 最後のAIメッセージ（final_responderによって追加されたもの）を表示\n",
    "                bot_actual_response = ai_messages[-1].content \n",
    "                print(f\"AIボット: {bot_actual_response}\")\n",
    "                chat_history_for_bot.append(AIMessage(content=bot_actual_response))\n",
    "            else:\n",
    "                print(\"AIボット: (応答がありませんでした。最大ターン数に達した可能性があります)\")\n",
    "                break # AIからの応答がない場合はループ終了\n",
    "        else:\n",
    "            print(\"AIボット: (状態取得エラーまたは会話終了)\")\n",
    "            break\n",
    "\n",
    "        # グラフ内部の会話ターン数が上限に達したら終了\n",
    "        if final_bot_state.get(\"conversation_turns\", 0) >= final_bot_state.get(\"max_turns\", max_dialogue_turns):\n",
    "             print(\"(最大会話ターン数に達しました)\")\n",
    "             break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        break\n",
    "else: # forループが正常に終了した場合 (breakされなかった場合)\n",
    "    print(\"\\n最大会話往復数に達したのでチャットを終了します。\")\n",
    "\n",
    "print(\"\\n--- 最終的なチャット履歴 (ボットとの対話) ---\")\n",
    "for msg in chat_history_for_bot:\n",
    "    print(f\"  {msg.type.upper()}: {msg.content}\")\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗: {e}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説015</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "*   **学習内容:** 第1章で学んだ複数の概念（`StateGraph`の定義、`TypedDict`による状態管理、ノードとエッジの追加、LLM呼び出し、条件付きエッジによる分岐、ループ（会話ターン数制限による間接的なループ制御）、状態キーの更新）を統合し、少し複雑な対話型のQ&Aボットを構築します。これにより、LangGraphの基本的な要素を組み合わせて実用的なアプリケーションを作成する流れを体験します。\n",
    "*   **コード解説:**\n",
    "    *   **`AdvancedQABotState`**: ユーザー入力、質問カテゴリ、LLM応答、会話ターン数、最大ターン数、エラーメッセージなど、多様な情報を保持する状態を定義します。\n",
    "    *   **ノード群:**\n",
    "        *   `capture_input_and_increment_turn`: ユーザー入力を取得し、会話ターン数をインクリメント。他の関連状態も初期化。\n",
    "        *   `categorize_question_node`: ユーザー入力を簡単なルールでカテゴリ分け（挨拶、知識質問、意見質問、不明）。\n",
    "        *   `greeting_responder_node`, `knowledge_llm_node`, `opinion_llm_node`, `unknown_question_node`: 各カテゴリに応じた応答を生成するノード。知識・意見質問ではLLMを呼び出します（`opinion_llm_node`では専用プロンプトを使用）。\n",
    "        *   `final_response_node`: 生成された応答を最終的に`messages`状態に追加し、ユーザーに見える形にします。\n",
    "    *   **ルーター関数:**\n",
    "        *   `check_conversation_limit`: 会話ターン数が上限（`max_turns`）に達していたらグラフを終了させます。達していなければカテゴリ分類に進みます。\n",
    "        *   `route_by_category`: `question_category`状態に基づいて、適切な応答生成ノードへ処理を分岐させます。\n",
    "    *   **グラフ構造:**\n",
    "        1.  エントリーポイントは `input_handler`。\n",
    "        2.  `input_handler`の後、`check_conversation_limit`ルーターでターン数上限をチェック。上限なら`END`、そうでなければ`categorizer`へ。\n",
    "        3.  `categorizer`の後、`route_by_category`ルーターで質問カテゴリに応じて各応答生成ノード（`greeting_responder`など）へ分岐。\n",
    "        4.  各応答生成ノードは、処理後に`final_responder`へ遷移。\n",
    "        5.  `final_responder`が最終応答を`messages`に追加し、その後`END`へ遷移してグラフの1回の実行が終了。\n",
    "    *   **実行部分:** `input()`関数を使ってユーザーと対話形式でテストします。ユーザーが\"exit\"か\"quit\"を入力するか、最大会話往復数に達するまでループします。`invoke`のたびに、`messages`（会話履歴）と`max_turns`を渡しています。`conversation_turns`はグラフ内部でインクリメント・チェックされます。\n",
    "*   **第1章の総括:**\n",
    "    *   この問題を通じて、LangGraphの基本的ながらも強力な機能を一通り組み合わせる体験ができました。\n",
    "    *   状態(`State`)がグラフ全体の情報伝達と制御のハブとして機能すること、ノードが個々の処理単位であること、エッジ（特に条件付きエッジ）が処理の流れを柔軟に定義することを実感できたでしょう。\n",
    "    *   LLMの組み込みも、プロンプトを工夫することで様々な役割を担わせることが可能です。\n",
    "    *   第2章以降では、ここでの基礎を元に、より高度な制御フロー（自己修正ループ、エラーハンドリング、人間による介入など）を学んでいきます。\n",
    "---\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
