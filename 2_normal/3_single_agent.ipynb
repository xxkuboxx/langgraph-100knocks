{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3章: シングルエージェントとツール活用\n",
    "\n",
    "## 準備\n",
    "\n",
    "以下のセルを順番に実行して、演習に必要な環境をセットアップします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMプロバイダーの選択\n",
    "\n",
    "このセルでは、使用するLLMプロバイダーを選択します。\n",
    "`LLM_PROVIDER` 変数に、利用したいプロバイダー名を設定してください。\n",
    "選択可能なプロバイダー: `\"openai\"`, `\"azure\"`, `\"google\"` (Vertex AI), `\"google_genai\"` (Gemini API), `\"anthropic\"`, `\"bedrock\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMプロバイダーの選択 ===\n",
    "# 利用したいLLMプロバイダーを以下の変数で指定してください。\n",
    "# \"openai\", \"azure\", \"google\" (Vertex AI), \"google_genai\" (Gemini API), \"anthropic\", \"bedrock\" のいずれかを選択できます。\n",
    "LLM_PROVIDER = \"openai\"  # 例: OpenAI を利用する場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIキー/環境変数の設定\n",
    "\n",
    "以下のセルを実行する前に、選択したLLMプロバイダーに応じたAPIキーまたは環境変数を設定する必要があります。\n",
    "\n",
    "**手順:**\n",
    "1.  `.env.sample` ファイルをコピーして `.env` ファイルを作成します。\n",
    "2.  `.env` ファイルを開き、選択したLLMプロバイダーに対応するAPIキーや必要な情報を記述します。\n",
    "    *   **OpenAI:** `OPENAI_API_KEY`\n",
    "    *   **Azure OpenAI:** `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `OPENAI_API_VERSION`, `AZURE_OPENAI_DEPLOYMENT_NAME`\n",
    "    *   **Google (Vertex AI):** `GOOGLE_CLOUD_PROJECT_ID`, `GOOGLE_CLOUD_LOCATION` (Colab環境外で実行する場合、`GOOGLE_APPLICATION_CREDENTIALS` 環境変数の設定も必要になることがあります)\n",
    "    *   **Google (Gemini API):** `GOOGLE_API_KEY`\n",
    "    *   **Anthropic:** `ANTHROPIC_API_KEY`\n",
    "    *   **AWS Bedrock:** `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION_NAME` (IAMロールを使用する場合は、これらのキー設定は不要な場合がありますが、リージョン名は必須です)\n",
    "3.  ファイルを保存します。\n",
    "\n",
    "**Google Colab を使用している場合:**\n",
    "上記の `.env` ファイルを使用する代わりに、Colabのシークレットマネージャーに必要なキーを登録してください。\n",
    "例えば、OpenAIを使用する場合は `OPENAI_API_KEY` という名前でシークレットを登録します。\n",
    "Vertex AI を利用する場合は、Colab上での認証 (`google.colab.auth.authenticate_user()`) が実行されます。\n",
    "\n",
    "このセルは、設定された情報に基づいて環境変数をロードし、LLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === APIキー/環境変数の設定 ===\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envファイルから環境変数を読み込む (存在する場合)\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# --- OpenAI ---\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY and IS_COLAB:\n",
    "        OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"OpenAI APIキーが設定されていません。環境変数 OPENAI_API_KEY を設定するか、Colab環境の場合はシークレットに OPENAI_API_KEY を設定してください。\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# --- Azure OpenAI ---\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not AZURE_OPENAI_API_KEY: AZURE_OPENAI_API_KEY = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not AZURE_OPENAI_ENDPOINT: AZURE_OPENAI_ENDPOINT = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        if not OPENAI_API_VERSION: OPENAI_API_VERSION = userdata.get(\"OPENAI_API_VERSION\") # 例: \"2023-07-01-preview\"\n",
    "        if not AZURE_OPENAI_DEPLOYMENT_NAME: AZURE_OPENAI_DEPLOYMENT_NAME = userdata.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if not AZURE_OPENAI_API_KEY: raise ValueError(\"Azure OpenAI APIキー (AZURE_OPENAI_API_KEY) が設定されていません。\")\n",
    "    if not AZURE_OPENAI_ENDPOINT: raise ValueError(\"Azure OpenAI エンドポイント (AZURE_OPENAI_ENDPOINT) が設定されていません。\")\n",
    "    if not OPENAI_API_VERSION: OPENAI_API_VERSION = \"2023-07-01-preview\" # デフォルトを設定することも可能\n",
    "    if not AZURE_OPENAI_DEPLOYMENT_NAME: raise ValueError(\"Azure OpenAI デプロイメント名 (AZURE_OPENAI_DEPLOYMENT_NAME) が設定されていません。\")\n",
    "\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "    os.environ[\"OPENAI_API_VERSION\"] = OPENAI_API_VERSION\n",
    "\n",
    "# --- Google Cloud Vertex AI (Gemini) ---\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT_ID\") # .env 用に修正\n",
    "    LOCATION = os.environ.get(\"GOOGLE_CLOUD_LOCATION\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not PROJECT_ID: PROJECT_ID = userdata.get(\"GOOGLE_CLOUD_PROJECT_ID\")\n",
    "        if not LOCATION: LOCATION = userdata.get(\"GOOGLE_CLOUD_LOCATION\") # 例: \"us-central1\"\n",
    "        from google.colab import auth as google_auth\n",
    "        google_auth.authenticate_user() # Vertex AI を使う場合は Colab での認証を推奨\n",
    "    else: # Colab外の場合、.envから読み込んだ値で環境変数を設定\n",
    "        if PROJECT_ID: os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID # Vertex AI SDKが参照する標準的な環境変数名\n",
    "        if LOCATION: os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "\n",
    "    if not PROJECT_ID: raise ValueError(\"Google Cloud Project ID が設定されていません。環境変数 GOOGLE_CLOUD_PROJECT_ID を設定するか、Colab環境の場合はシークレットに GOOGLE_CLOUD_PROJECT_ID を設定してください。\")\n",
    "    if not LOCATION: LOCATION = \"us-central1\" # デフォルトロケーション\n",
    "\n",
    "# --- Google Gemini API (langchain-google-genai) ---\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY and IS_COLAB:\n",
    "        GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"Google APIキーが設定されていません。環境変数 GOOGLE_API_KEY を設定するか、Colab環境の場合はシークレットに GOOGLE_API_KEY を設定してください。\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# --- Anthropic (Claude) ---\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY and IS_COLAB:\n",
    "        ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        raise ValueError(\"Anthropic APIキーが設定されていません。環境変数 ANTHROPIC_API_KEY を設定するか、Colab環境の場合はシークレットに ANTHROPIC_API_KEY を設定してください。\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "# --- Amazon Bedrock (Claude) ---\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    AWS_REGION_NAME = os.environ.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if IS_COLAB: \n",
    "        if not AWS_ACCESS_KEY_ID: AWS_ACCESS_KEY_ID = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
    "        if not AWS_SECRET_ACCESS_KEY: AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        if not AWS_REGION_NAME: AWS_REGION_NAME = userdata.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if not AWS_REGION_NAME:\n",
    "         raise ValueError(\"AWSリージョン名 (AWS_REGION_NAME) が設定されていません。Bedrock利用にはリージョン指定が必要です。\")\n",
    "\n",
    "    # 環境変数に設定 (boto3がこれらを自動で読み込む)\n",
    "    if AWS_ACCESS_KEY_ID: os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "    if AWS_SECRET_ACCESS_KEY: os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION_NAME # boto3が参照する標準的なリージョン環境変数名\n",
    "    os.environ[\"AWS_REGION\"] = AWS_REGION_NAME # いくつかのライブラリはこちらを参照することもある\n",
    "\n",
    "print(f\"APIキー/環境変数の設定完了 (プロバイダー: {LLM_PROVIDER})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMクライアントの初期化\n",
    "\n",
    "このセルは、上で選択・設定したLLMプロバイダーに基づいて、対応するLLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMクライアントの動的初期化 ===\n",
    "llm = None\n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), # 環境変数から取得\n",
    "        openai_api_version=os.environ.get(\"OPENAI_API_VERSION\"), # 環境変数から取得\n",
    "        temperature=0,\n",
    "    )\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    from langchain_google_vertexai import ChatVertexAI\n",
    "    # PROJECT_ID, LOCATION は前のセルで環境変数に設定済みか、Colabの場合は直接利用\n",
    "    llm = ChatVertexAI(model_name=\"gemini-2.0-flash\", temperature=0, project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"), location=os.environ.get(\"GOOGLE_CLOUD_LOCATION\"))\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    from langchain_anthropic import ChatAnthropic\n",
    "    llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    from langchain_aws import ChatBedrock # langchain_community.chat_models から langchain_aws に変更の可能性あり\n",
    "    # AWS_REGION_NAME は前のセルで環境変数 AWS_DEFAULT_REGION に設定済み\n",
    "    llm = ChatBedrock( # BedrockChat ではなく ChatBedrock が一般的\n",
    "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        # region_name=os.environ.get(\"AWS_DEFAULT_REGION\"), # 通常、boto3が環境変数から自動で読み込む\n",
    "        model_kwargs={\"temperature\": 0},\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Unsupported LLM_PROVIDER: {LLM_PROVIDER}. \"\n",
    "        \"Please choose from 'openai', 'azure', 'google', 'google_genai', 'anthropic', or 'bedrock'.\"\n",
    "    )\n",
    "\n",
    "print(f\"LLM Provider: {LLM_PROVIDER}\")\n",
    "if llm:\n",
    "    print(f\"LLM Client Type: {type(llm)}\")\n",
    "    # モデル名取得の試行を汎用的に\n",
    "    model_attr = (\n",
    "                 getattr(llm, 'model', None) or\n",
    "                 getattr(llm, 'model_name', None) or\n",
    "                 getattr(llm, 'model_id', None) or\n",
    "                 (hasattr(llm, 'llm') and getattr(llm.llm, 'model', None)) # 一部のLLMクライアントのネスト構造に対応\n",
    "    )\n",
    "    if hasattr(llm, 'azure_deployment') and not model_attr: # Azure特有の属性\n",
    "        model_attr = llm.azure_deployment\n",
    "        \n",
    "    if model_attr:\n",
    "        print(f\"LLM Model: {model_attr}\")\n",
    "    else:\n",
    "        print(\"LLM Model: (Could not determine model name from client attributes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題001: 基本的なツールの実装と ToolNode の使用\n",
    "\n",
    "LangGraphで外部ツールを利用する最初のステップとして、シンプルなツールを定義し、それをグラフ内の `ToolNode` から呼び出す方法を学びます。ここでは、与えられた数値を二乗する簡単なPython関数をツールとして実装し、LLM（またはエージェント）からのツール呼び出し要求に応じてそのツールを実行するグラフを構築します。\n",
    "\n",
    "*   **学習内容:**\n",
    "    *   `@tool` デコレータを使ったLangChainツールの基本的な作成方法。\n",
    "    *   ツール呼び出し(`ToolCall`)とツール結果(`ToolMessage`)のメッセージ形式の理解。\n",
    "    *   `ToolNode` を使って、グラフ内でツールを実行する方法。\n",
    "    *   LLMにツールを使わせるためのプロンプト（または `bind_tools` の利用）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001 - グラフ構築\n",
    "from typing import ____, ____, List, Union\n",
    "from langchain_core.tools import tool\n",
    "from ____.____ import ____, ____\n",
    "from ____.____.message import ____\n",
    "from langchain_core.messages import ____, HumanMessage, AIMessage, ToolMessage, ToolCall # ToolCallを追加 (解答例より)\n",
    "from uuid import uuid4 # uuid4を追加 (解答例より)\n",
    "\n",
    "# --- 1. ツールの定義 ---\n",
    "@tool\n",
    "def square_number(number: float) -> float:\n",
    "    \"\"\"与えられた数値を二乗します。\"\"\"\n",
    "    print(f\"  ツール[square_number]呼び出し: number={number}\")\n",
    "    return number ** 2\n",
    "\n",
    "tools = [square_number] # 作成したツールをリストに格納 (解答例より)\n",
    "\n",
    "# --- 2. 状態の定義 ---\n",
    "class BasicToolAgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- 3. ノードの定義 ---\n",
    "\n",
    "# LLMノード (エージェントノード)\n",
    "def agent_node(state: BasicToolAgentState):\n",
    "    print(\"\n",
    "[エージェントノード]\")\n",
    "    current_messages = state[\"messages\"] # 解答例より\n",
    "    response = None # 解答例より\n",
    "    if hasattr(llm, 'bind_tools') and LLM_PROVIDER != \"fake\": # FakeLLMはbind_toolsを持たない場合がある (解答例より)\n",
    "        llm_with_tools = llm.bind_tools(tools)\n",
    "        response = llm_with_tools.invoke(current_messages)\n",
    "    else:\n",
    "        print(\"  WARN: LLMがbind_toolsをサポートしていないか、FakeLLMです。手動でツールコールを模倣します。\") # 解答例より\n",
    "        last_message_text = current_messages[-1].content.lower()\n",
    "        if isinstance(current_messages[-1], HumanMessage) and (\"二乗\" in last_message_text or \"square\" in last_message_text): # 解答例より\n",
    "            import re\n",
    "            match = re.search(r'(\\d+\\.?\\d*|\\.\\d+)', last_message_text) # 数値を抽出 (解答例より)\n",
    "            if match:\n",
    "                num_to_square = float(match.group(1))\n",
    "                print(f\"  FakeLLM/Fallback: キーワード検知。square_number({num_to_square}) を呼び出します。\") # 解答例より\n",
    "                response = AIMessage(\n",
    "                    content=\"\", \n",
    "                    tool_calls=[ToolCall(name=\"square_number\", args={\"number\": num_to_square}, id=f\"tool_call_{str(uuid4())[:4]}\")] # 解答例より\n",
    "                )\n",
    "            else:\n",
    "                response = AIMessage(content=\"FakeLLM/Fallback: 二乗する数値が見つかりませんでした。\") # 解答例より\n",
    "        elif isinstance(current_messages[-1], ToolMessage): # 解答例より\n",
    "             response = AIMessage(content=f\"ツール実行結果を受け取りました: {current_messages[-1].content}\") # 解答例より\n",
    "        else:\n",
    "            response = llm.invoke(current_messages) # 通常のLLM呼び出し (解答例より)\n",
    "            \n",
    "    print(f\"  エージェント応答: {response}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# ツール実行ノード\n",
    "from langgraph.prebuilt import ToolNode\n",
    "tool_node = ToolNode(tools) # ToolNodeを初期化 (解答例より)\n",
    "\n",
    "# --- 4. ルーター関数の定義 ---\n",
    "def router_function(state: BasicToolAgentState) -> str:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        print(\"  -> ルーター: ツール呼び出しあり。ツールノードへ。\")\n",
    "        return \"call_tools\"\n",
    "    else:\n",
    "        print(\"  -> ルーター: ツール呼び出しなし。終了。\")\n",
    "        return \"__end__\"\n",
    "\n",
    "# --- 5. グラフの構築 ---\n",
    "workflow = StateGraph(BasicToolAgentState)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node) # tool_node を \"tools\" という名前で追加 (解答例より)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    router_function,\n",
    "    {\n",
    "        \"call_tools\": \"tools\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\") # ツール実行後、再度エージェントノードに戻して結果を処理させる\n",
    "\n",
    "graph_q1 = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001 - グラフ可視化\n",
    "____ IPython.display ____ Image, display # 解答例より display をインポート\n",
    "\n",
    "____:\n",
    "    display(Image(graph_q1.get_graph().draw_png()))\n",
    "____ Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001 - グラフ実行\n",
    "____:\n",
    "    print(\"--- 基本ツールのテスト実行 (5の二乗) ---\")\n",
    "    initial_state_q1 = {\"messages\": [____(content=\"5を二乗して結果を教えてください。\")]} # 解答例よりメッセージ変更\n",
    "    thread_q1 = {\"configurable\": {\"thread_id\": \"test-thread-q1\"}} # 解答例より config 追加\n",
    "\n",
    "    final_response_content = \"\" # 解答例より\n",
    "    ____ event ____ graph_q1.____(initial_state_q1, config=thread_q1, recursion_limit=5): # 解答例より config 追加, recursion_limit は元の解答から\n",
    "        print(f\"Event: {event}\")\n",
    "        # 最後のAIMessage (ツールコールではないもの) の内容を取得 (解答例より)\n",
    "        ____ 'agent' ____ event:\n",
    "            agent_messages = event['agent'].get('messages', [])\n",
    "            ____ agent_messages and isinstance(agent_messages[0], ____) and not agent_messages[0].____:\n",
    "                final_response_content = agent_messages[0].content\n",
    "        print(\"----\")\n",
    "\n",
    "    print(f\"\n",
    "最終的なAIの応答内容: {final_response_content}\") # 解答例より\n",
    "    if \"25\" in final_response_content or \"25.0\" in final_response_content: # 解答例より\n",
    "        print(\"テスト成功: 応答に「25」が含まれています。\") # 解答例より\n",
    "    else:\n",
    "        print(f\"テスト失敗: 応答に「25」が含まれていません。実際の応答: {final_response_content}\") # 解答例より\n",
    "except Exception as e:\n",
    "    print(f\"エラーが発生しました: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答001</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, ToolCall\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- 1. ツールの定義 ---\n",
    "@tool\n",
    "def square_number(number: float) -> float:\n",
    "    \"\"\"与えられた数値を二乗します。\"\"\"\n",
    "    print(f\"  ツール[square_number]呼び出し: number={number}\")\n",
    "    return number ** 2\n",
    "\n",
    "tools = [square_number]\n",
    "\n",
    "# --- 2. 状態の定義 ---\n",
    "class BasicToolAgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- 3. ノードの定義 ---\n",
    "def agent_node(state: BasicToolAgentState):\n",
    "    print(\"\n",
    "[エージェントノード]\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    response = None\n",
    "    if hasattr(llm, 'bind_tools') and LLM_PROVIDER != \"fake\": # FakeLLMはbind_toolsを持たない場合がある\n",
    "        llm_with_tools = llm.bind_tools(tools)\n",
    "        response = llm_with_tools.invoke(current_messages)\n",
    "    else:\n",
    "        print(\"  WARN: LLMがbind_toolsをサポートしていないか、FakeLLMです。手動でツールコールを模倣します。\")\n",
    "        last_message_text = current_messages[-1].content.lower()\n",
    "        if isinstance(current_messages[-1], HumanMessage) and (\"二乗\" in last_message_text or \"square\" in last_message_text):\n",
    "            import re\n",
    "            match = re.search(r'(\\d+\\.?\\d*|\\.\\d+)', last_message_text) # 数値を抽出\n",
    "            if match:\n",
    "                num_to_square = float(match.group(1))\n",
    "                print(f\"  FakeLLM/Fallback: キーワード検知。square_number({num_to_square}) を呼び出します。\")\n",
    "                response = AIMessage(\n",
    "                    content=\"\", \n",
    "                    tool_calls=[ToolCall(name=\"square_number\", args={\"number\": num_to_square}, id=f\"tool_call_{str(uuid4())[:4]}\")]\n",
    "                )\n",
    "            else:\n",
    "                response = AIMessage(content=\"FakeLLM/Fallback: 二乗する数値が見つかりませんでした。\")\n",
    "        elif isinstance(current_messages[-1], ToolMessage):\n",
    "             response = AIMessage(content=f\"ツール実行結果を受け取りました: {current_messages[-1].content}\")\n",
    "        else:\n",
    "            response = llm.invoke(current_messages) # 通常のLLM呼び出し\n",
    "            \n",
    "    print(f\"  エージェント応答: {response}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# --- 4. ルーター関数の定義 ---\n",
    "def router_function(state: BasicToolAgentState) -> str:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        print(\"  -> ルーター: ツール呼び出しあり。ツールノードへ。\")\n",
    "        return \"call_tools\"\n",
    "    else:\n",
    "        print(\"  -> ルーター: ツール呼び出しなし。終了。\")\n",
    "        return \"__end__\"\n",
    "\n",
    "# --- 5. グラフの構築 ---\n",
    "workflow = StateGraph(BasicToolAgentState)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    router_function,\n",
    "    {\n",
    "        \"call_tools\": \"tools\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph_q1 = workflow.compile()\n",
    "try:\n",
    "    display(Image(graph_q1.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")\n",
    "\n",
    "# --- 6. グラフの実行 --- \n",
    "print(\"--- 基本ツールのテスト実行 (5の二乗) ---\")\n",
    "initial_state_q1 = {\"messages\": [HumanMessage(content=\"5を二乗して結果を教えてください。\")]}\n",
    "thread_q1 = {\"configurable\": {\"thread_id\": \"test-thread-q1\"}}}\n",
    "\n",
    "final_response_content = \"\"\n",
    "for event in graph_q1.stream(initial_state_q1, config=thread_q1, recursion_limit=5):\n",
    "    print(f\"Event: {event}\")\n",
    "    # 最後のAIMessage (ツールコールではないもの) の内容を取得\n",
    "    if 'agent' in event:\n",
    "        agent_messages = event['agent'].get('messages', [])\n",
    "        if agent_messages and isinstance(agent_messages[0], AIMessage) and not agent_messages[0].tool_calls:\n",
    "            final_response_content = agent_messages[0].content\n",
    "    print(\"----\")\n",
    "\n",
    "print(f\"\n",
    "最終的なAIの応答内容: {final_response_content}\")\n",
    "if \"25\" in final_response_content or \"25.0\" in final_response_content:\n",
    "    print(\"テスト成功: 応答に「25」が含まれています。\")\n",
    "else:\n",
    "    print(f\"テスト失敗: 応答に「25」が含まれていません。実際の応答: {final_response_content}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説001</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **`@tool`デコレータ:** Python関数に`@tool`デコレータを付けるだけで、LangChainが認識できるツールとして簡単に定義できます。関数のdocstringはツールの説明としてLLMに提供され、型ヒントはLLMがツールを正しく呼び出すための引数の型情報として利用されます。\n",
    "*   **`ToolNode`:** `langgraph.prebuilt.ToolNode` は、渡されたツールのリスト（この場合は `[square_number]`）を実行するための専用ノードです。LLM（エージェントノード）がツール呼び出しを含む`AIMessage`を返すと、その`tool_calls`属性に基づいて`ToolNode`が対応するツールを実行し、結果を`ToolMessage`として返します。\n",
    "*   **LLMへのツールのバインド:** `llm.bind_tools(tools)` を使うことで、LLM（特にOpenAIのFunction Calling対応モデルなど）に対して利用可能なツールを明示的に伝えることができます。これにより、LLMはユーザーの指示に応じて適切なツールを選択し、必要な引数と共に呼び出す形式の `AIMessage` (具体的には `tool_calls` 属性を持つ) を生成するようになります。\n",
    "    *   `bind_tools` をサポートしていないLLMや、FakeLLMを使用している場合は、この機能が期待通りに動作しないことがあります。その場合のフォールバックとして、この解答例では入力テキストのキーワードに基づいて手動でツールコールを模倣するロジックを入れていますが、これはあくまでデモ用です。\n",
    "*   **グラフのフロー:**\n",
    "    1.  `agent_node`がユーザー入力に基づいて応答を生成します。ツールが必要だと判断すれば、`tool_calls`を含む`AIMessage`を返します。\n",
    "    2.  `router_function`が`AIMessage`に`tool_calls`が含まれているかを確認し、含まれていれば処理を`tools` (ToolNode) に分岐します。\n",
    "    3.  `ToolNode`がツール（`square_number`）を実行し、結果を`ToolMessage`として返します。\n",
    "    4.  `ToolNode`の処理後、エッジは再び`agent_node`に戻ります。`agent_node`は`ToolMessage`（ツールの実行結果）を含む更新された状態を受け取り、それに基づいて最終的な応答をユーザーに返します。\n",
    "    5.  最終的な応答にツール呼び出しが含まれていなければ、ルーターは処理を`END`に分岐させ、グラフが終了します。\n",
    "*   **状態 (`messages`):** 会話の履歴（`HumanMessage`, `AIMessage`, `ToolMessage`）が`messages`キーに蓄積されていき、各ノードはこの履歴を参照して次のアクションを決定します。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題002: ReAct 風エージェント - LLM によるツール使用判断と応答選択\n",
    "\n",
    "より自律的なエージェントの振る舞いを実現するために、LLM自身に「次にどのツールを使うべきか」または「ユーザーに直接応答すべきか」を判断させるReAct (Reasoning and Acting) 風のロジックを組み込みます。ここでは、LLMが状況に応じてツール（例: 前問の`square_number`や、新たに定義する簡単な文字列操作ツール）の使用を決定し、その結果を元に応答を生成する、というサイクルを実装します。\n",
    "\n",
    "*   **学習内容:**\n",
    "    *   LLMのプロンプトを工夫し、ツールを使うべきか、どのツールを使うべきか、あるいは直接応答すべきかを考えさせる方法。\n",
    "    *   ツール呼び出し(`tool_calls`)の有無によって、グラフの次の遷移先（ツール実行ノード or 終了ノード）を動的に決定するルーターの実装。\n",
    "    *   ツール実行後、その結果 (`ToolMessage`) をLLMに渡し、最終的な応答を生成させる流れ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002 - グラフ構築\n",
    "from langchain_core.tools import tool\n",
    "from ____.____ import ____, ____ # ENDを追加 (解答例より)\n",
    "from ____.____.message import ____\n",
    "from langchain_core.messages import ____, ____, ____, ToolMessage, ToolCall # BaseMessage, ToolCallを追加 (解答例より)\n",
    "from typing import TypedDict, Annotated, List # TypedDict, Annotated, List を追加 (解答例より)\n",
    "from uuid import uuid4\n",
    "from langgraph.prebuilt import ToolNode # ToolNodeをインポート (解答例より)\n",
    "\n",
    "# --- 1. ツールの追加定義 ---\n",
    "@tool\n",
    "def reverse_string(text: str) -> str:\n",
    "    \"\"\"与えられた文字列を逆順にします。\"\"\"\n",
    "    print(f\"  ツール[reverse_string]呼び出し: text='{text}'\")\n",
    "    return text[::-1]\n",
    "\n",
    "tools_q2 = [square_number, reverse_string] # square_numberは問題001から再利用 (解答例より)\n",
    "\n",
    "# --- 2. 状態の定義 (問題001と同じでOK) ---\n",
    "# class BasicToolAgentState(TypedDict): messages: Annotated[list, add_messages] (解答例よりクラス定義はコメントアウト)\n",
    "\n",
    "# --- 3. ノードの定義 ---\n",
    "def react_agent_node(state: BasicToolAgentState):\n",
    "    print(\"\n",
    "[ReActエージェントノード]\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    response = None\n",
    "\n",
    "    # OpenAI Function Calling対応モデルや、ツール利用をサポートするLLMを想定\n",
    "    if hasattr(llm, 'bind_tools') and LLM_PROVIDER != \"fake\":\n",
    "        llm_with_tools_q2 = llm.bind_tools(tools_q2)\n",
    "        response = llm_with_tools_q2.invoke(current_messages)\n",
    "    else:\n",
    "        print(\"  WARN: LLMがbind_toolsをサポートしていないかFakeLLMです。手動でツールコールを模倣します。\")\n",
    "        last_message = current_messages[-1] # 解答例より\n",
    "        tool_called_in_fallback = False # 解答例より\n",
    "        if isinstance(last_message, HumanMessage): # 解答例より\n",
    "            text_content = last_message.content.lower() # 解答例より\n",
    "            import re # 解答例より\n",
    "            if \"二乗\" in text_content or \"square\" in text_content: # 解答例より\n",
    "                match = re.search(r'(\\d+\\.?\\d*|\\.\\d+)', text_content) # 解答例より\n",
    "                if match: # 解答例より\n",
    "                    num = float(match.group(1)) # 解答例より\n",
    "                    response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"square_number\", args={\"number\": num}, id=f\"ftc_sq_{uuid4()[:4]}\")]) # 解答例より\n",
    "                    tool_called_in_fallback = True # 解答例より\n",
    "            elif \"逆順\" in text_content or \"reverse\" in text_content: # 解答例より\n",
    "                match = re.search(r\"['\"]([^'\"]*)['\"]\", text_content) # クォートされた文字列を優先 (解答例より)\n",
    "                str_to_rev = match.group(1) if match else text_content.replace(\"逆順\",\"\").replace(\"reverse\",\"\").strip() # 解答例より\n",
    "                response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"reverse_string\", args={\"text\": str_to_rev}, id=f\"ftc_rev_{uuid4()[:4]}\")]) # 解答例より\n",
    "                tool_called_in_fallback = True # 解答例より\n",
    "        \n",
    "        if not tool_called_in_fallback: # 解答例より\n",
    "            if isinstance(last_message, ToolMessage): # 解答例より\n",
    "                response = AIMessage(content=f\"FakeLLM: ツール「{last_message.name}」の結果「{last_message.content}」を受け取りました。これを元に応答します。\") # 解答例より\n",
    "            else: # 解答例より\n",
    "                response = AIMessage(content=\"FakeLLM: こんにちは！ツールは不要と判断しました。\") # 解答例より\n",
    "\n",
    "    print(f\"  エージェント応答: {response}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_node_q2 = ToolNode(tools_q2) # tools_q2 を使用 (解答例より)\n",
    "\n",
    "# ルーター関数 (問題001と同じでOK)\n",
    "# def router_function(state: BasicToolAgentState) -> str: ... (解答例よりコメントアウト)\n",
    "\n",
    "# --- 4. グラフの構築 ---\n",
    "workflow_q2 = StateGraph(BasicToolAgentState)\n",
    "workflow_q2.add_node(\"agent\", react_agent_node)\n",
    "workflow_q2.add_node(\"tools\", tool_node_q2)\n",
    "\n",
    "workflow_q2.set_entry_point(\"agent\")\n",
    "workflow_q2.add_conditional_edges(\"agent\", router_function, {\"call_tools\": \"tools\", \"__end__\": END})\n",
    "workflow_q2.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph_q2 = workflow_q2.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002 - グラフ可視化\n",
    "____ IPython.display ____ Image, display # 解答例より display をインポート\n",
    "\n",
    "____:\n",
    "    display(Image(graph_q2.get_graph().draw_png()))\n",
    "____ Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002 - グラフ実行\n",
    "thread_q2_1 = {\"configurable\": {\"thread_id\": \"test-thread-q2-sq\"}} # 解答例より\n",
    "print(\"\n",
    "--- ReAct風エージェントテスト (square_number: 12の二乗) ---\") # 解答例よりメッセージ変更\n",
    "initial_state_q2_sq = {\"messages\": [____(content=\"数値12を二乗して、その結果を教えてください。\")]} # 解答例よりメッセージ変更\n",
    "____ event ____ graph_q2.____(initial_state_q2_sq, config=thread_q2_1, recursion_limit=5): print(f\"Event: {event}\n",
    "----\"); # 解答例より config 追加, recursion_limit は元の解答から\n",
    "\n",
    "thread_q2_2 = {\"configurable\": {\"thread_id\": \"test-thread-q2-rev\"}} # 解答例より\n",
    "print(\"\n",
    "--- ReAct風エージェントテスト (reverse_string: 'LangGraph'を逆順に) ---\") # 解答例よりメッセージ変更\n",
    "initial_state_q2_rev = {\"messages\": [____(content=\"'LangGraph'という文字列を逆順にして、その結果を教えてください。\")]} # 解答例よりメッセージ変更\n",
    "____ event ____ graph_q2.____(initial_state_q2_rev, config=thread_q2_2, recursion_limit=5): print(f\"Event: {event}\n",
    "----\"); # 解答例より config 追加, recursion_limit は元の解答から\n",
    "\n",
    "thread_q2_3 = {\"configurable\": {\"thread_id\": \"test-thread-q2-no-tool\"}} # 解答例より\n",
    "print(\"\n",
    "--- ReAct風エージェントテスト (ツール不要: こんにちは) ---\") # 解答例よりメッセージ変更\n",
    "initial_state_q2_no_tool = {\"messages\": [____(content=\"こんにちは、調子はどうですか？\")]}\n",
    "for event in graph_q2.____(initial_state_q2_no_tool, config=thread_q2_3, recursion_limit=5): print(f\"Event: {event}\n",
    "----\"); # 解答例より config 追加, recursion_limit は元の解答から"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答002</summary>\n",
    "\n",
    "``````python\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, ToolCall\n",
    "from typing import TypedDict, Annotated, List\n",
    "from uuid import uuid4\n",
    "from IPython.display import Image, display\n",
    "from langgraph.prebuilt import ToolNode # ToolNodeをインポート\n",
    "\n",
    "# --- 1. ツールの追加定義 ---\n",
    "# square_number は問題001で定義済みなので再利用\n",
    "\n",
    "@tool\n",
    "def reverse_string(text: str) -> str:\n",
    "    \"\"\"与えられた文字列を逆順にします。\"\"\"\n",
    "    print(f\"  ツール[reverse_string]呼び出し: text='{text}'\")\n",
    "    return text[::-1]\n",
    "\n",
    "tools_q2 = [square_number, reverse_string] \n",
    "\n",
    "# --- 2. 状態の定義 (問題001と同じ) ---\n",
    "class BasicToolAgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- 3. ノードの定義 ---\n",
    "def react_agent_node(state: BasicToolAgentState):\n",
    "    print(\"\n",
    "[ReActエージェントノード]\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    response = None\n",
    "    \n",
    "    if hasattr(llm, 'bind_tools') and LLM_PROVIDER != \"fake\":\n",
    "        llm_with_tools_q2 = llm.bind_tools(tools_q2)\n",
    "        # 実際のLLMなら、ここでシステムプロンプトにReAct的な指示を入れるとより効果的\n",
    "        # 例: \"You are a helpful assistant that can use tools. First think step-by-step if a tool is needed. Then call the tool or respond directly.\"\n",
    "        response = llm_with_tools_q2.invoke(current_messages)\n",
    "    else: # FakeLLMやbind_tools非対応LLM用のフォールバック\n",
    "        print(\"  WARN: LLMがbind_toolsをサポートしていないかFakeLLMです。手動でツールコールを模倣します。\")\n",
    "        last_message = current_messages[-1]\n",
    "        tool_called_in_fallback = False\n",
    "        if isinstance(last_message, HumanMessage):\n",
    "            text_content = last_message.content.lower()\n",
    "            import re\n",
    "            if \"二乗\" in text_content or \"square\" in text_content:\n",
    "                match = re.search(r'(\\d+\\.?\\d*|\\.\\d+)', text_content)\n",
    "                if match:\n",
    "                    num = float(match.group(1))\n",
    "                    response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"square_number\", args={\"number\": num}, id=f\"ftc_sq_{uuid4()[:4]}\")])\n",
    "                    tool_called_in_fallback = True\n",
    "            elif \"逆順\" in text_content or \"reverse\" in text_content:\n",
    "                match = re.search(r\"['\"]([^'\"]*)['\"]\", text_content) # クォートされた文字列を優先\n",
    "                str_to_rev = match.group(1) if match else text_content.replace(\"逆順\",\"\").replace(\"reverse\",\"\").strip()\n",
    "                response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"reverse_string\", args={\"text\": str_to_rev}, id=f\"ftc_rev_{uuid4()[:4]}\")])\n",
    "                tool_called_in_fallback = True\n",
    "        \n",
    "        if not tool_called_in_fallback:\n",
    "            if isinstance(last_message, ToolMessage):\n",
    "                response = AIMessage(content=f\"FakeLLM: ツール「{last_message.name}」の結果「{last_message.content}」を受け取りました。これを元に応答します。\")\n",
    "            else:\n",
    "                response = AIMessage(content=\"FakeLLM: こんにちは！ツールは不要と判断しました。\")\n",
    "    \n",
    "    print(f\"  エージェント応答: {response}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_node_q2 = ToolNode(tools_q2) \n",
    "\n",
    "# ルーター関数 (問題001と同じものを使用)\n",
    "def router_function(state: BasicToolAgentState) -> str:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        print(\"  -> ルーター: ツール呼び出しあり。ツールノードへ。\")\n",
    "        return \"call_tools\"\n",
    "    else:\n",
    "        print(\"  -> ルーター: ツール呼び出しなし。終了。\")\n",
    "        return \"__end__\"\n",
    "\n",
    "# --- 4. グラフの構築 ---\n",
    "workflow_q2 = StateGraph(BasicToolAgentState)\n",
    "workflow_q2.add_node(\"agent\", react_agent_node)\n",
    "workflow_q2.add_node(\"tools\", tool_node_q2)\n",
    "\n",
    "workflow_q2.set_entry_point(\"agent\")\n",
    "workflow_q2.add_conditional_edges(\"agent\", router_function, {\"call_tools\": \"tools\", \"__end__\": END})\n",
    "workflow_q2.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph_q2 = workflow_q2.compile()\n",
    "try:\n",
    "    display(Image(graph_q2.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")\n",
    "\n",
    "# --- 5. グラフの実行 --- \n",
    "thread_q2_1 = {\"configurable\": {\"thread_id\": \"test-thread-q2-sq\"}}}\n",
    "print(\"\n",
    "--- ReAct風エージェントテスト (square_number: 12の二乗) ---\")\n",
    "initial_state_q2_sq = {\"messages\": [HumanMessage(content=\"数値12を二乗して、その結果を教えてください。\")]}\n",
    "for event in graph_q2.stream(initial_state_q2_sq, config=thread_q2_1, recursion_limit=5): print(f\"Event: {event}\n",
    "----\");\n",
    "\n",
    "thread_q2_2 = {\"configurable\": {\"thread_id\": \"test-thread-q2-rev\"}}}\n",
    "print(\"\n",
    "--- ReAct風エージェントテスト (reverse_string: 'LangGraph'を逆順に) ---\")\n",
    "initial_state_q2_rev = {\"messages\": [HumanMessage(content=\"'LangGraph'という文字列を逆順にして、その結果を教えてください。\")]}\n",
    "for event in graph_q2.stream(initial_state_q2_rev, config=thread_q2_2, recursion_limit=5): print(f\"Event: {event}\n",
    "----\");\n",
    "\n",
    "thread_q2_3 = {\"configurable\": {\"thread_id\": \"test-thread-q2-no-tool\"}}}\n",
    "print(\"\n",
    "--- ReAct風エージェントテスト (ツール不要: こんにちは) ---\")\n",
    "initial_state_q2_no_tool = {\"messages\": [HumanMessage(content=\"こんにちは、調子はどうですか？\")]}\n",
    "for event in graph_q2.stream(initial_state_q2_no_tool, config=thread_q2_3, recursion_limit=5): print(f\"Event: {event}\n",
    "----\");\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説002</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **LLMによる判断の重要性:** ReAct（Reasoning and Acting）パラダイムの核心は、LLM自身が次のアクション（ツールを使うか、どのツールを使うか、ユーザーに応答するかなど）を思考し、決定することです。これを実現するためには、LLMにその能力を発揮させるようなプロンプト設計が重要になります。例えば、システムプロンプトに「あなたはタスクを達成するために利用可能なツールを計画的に使うことができます。まず現在の状況と目標を考慮し、次に取るべきアクションを決定してください。アクションは、ツールの利用か、ユーザーへの最終応答です。」といった指示を含めることが考えられます。\n",
    "*   **ツールリストの提供:** `llm.bind_tools(tools_q2)` のように、利用可能なツールのリストをLLMに提供することで、LLMはそれらのツールの中から適切なものを選択しようとします。各ツールの説明（docstring）が、LLMがツールを正しく理解し選択する上で非常に重要です。\n",
    "*   **ルーティングロジック:** `router_function` は、LLM（エージェントノード）の応答に `tool_calls` が含まれているかどうかを判断します。\n",
    "    *   `tool_calls` があれば、次にツール実行ノード (`tool_node_q2`) に処理を移します。\n",
    "    *   `tool_calls` がなければ、LLMは直接ユーザーに応答しようとしていると判断し、処理を終了 (`END`) させます。\n",
    "*   **処理サイクル:**\n",
    "    1.  ユーザー入力がエージェントノードに渡されます。\n",
    "    2.  エージェントノード内のLLMが、入力と会話履歴、利用可能なツール情報に基づいて、ツールを使うか最終応答を返すかを決定します。\n",
    "    3.  ツールを使うと判断した場合、`tool_calls` を含む `AIMessage` を返します。\n",
    "    4.  ルーターがこれを検知し、ツールノードに処理を渡します。\n",
    "    5.  ツールノードが指定されたツールを実行し、結果を `ToolMessage` として返します。\n",
    "    6.  この `ToolMessage` を含む更新された会話履歴が、再びエージェントノードに渡されます。\n",
    "    7.  LLMはツールの実行結果を踏まえて、最終的な応答を生成するか、さらに別のツールを使うかなどを判断します。\n",
    "    8.  最終的にLLMがツールを使わずに応答を返すと判断した場合、ルーターは処理を終了させます。\n",
    "*   **FakeLLMの限界:** 解答例のフォールバック処理で示されているように、`bind_tools` を完全には模倣できないFakeLLMや、ツール利用を前提としない単純なLLMでは、ReAct的な挙動をさせるのは難しいです。実際のReActエージェントでは、OpenAIのFunction Calling対応モデルや、LangChainのAgent Executorと連携可能なモデルなど、ツール利用を高度にサポートするLLMの利用が一般的です。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題003: 複数ツールからの選択\n",
    "\n",
    "エージェントが複数のツールを利用可能な状況で、現在のタスクや会話の文脈に応じて、LLMがそれらの中から最も適切なツールを一つ（または複数）選択して使用する能力は非常に重要です。この問題では、複数の異なるツール（例: `square_number`、`reverse_string`、そして新たにウェブ検索を行う`search_tool`）をエージェントに提供し、LLMがユーザーの質問に応じて適切なツールを選択・実行するグラフを構築します。\n",
    "\n",
    "*   **学習内容:**\n",
    "    *   複数のツールをリストとしてLLMに提供する方法。\n",
    "    *   LLMがユーザーの意図を解釈し、提供されたツールの中から最適なものを選択する（またはツールを使わないと判断する）思考プロセス（のシミュレーションや期待）。\n",
    "    *   選択されたツールに応じた引数をLLMが正しく生成する能力（の期待）。\n",
    "    *   Tavily Search API ( `langchain_community.tools.tavily_search.TavilySearchResults` ) のような外部連携ツールをグラフに組み込む基本的な方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003 - グラフ構築\n",
    "from langchain_core.tools import tool # 解答例より tool をインポート (既にインポート済みだが明示)\n",
    "from ____.____ import ____, ____ # 解答例より ____ をインポート (既にインポート済みだが明示)\n",
    "from ____.____.message import ____ # 解答例より (既にインポート済みだが明示)\n",
    "from langchain_core.messages import ____, ____, AIMessage, ToolMessage, ToolCall # 解答例より (既にインポート済みだが明示)\n",
    "from typing import TypedDict, Annotated, List # 解答例より (既にインポート済みだが明示)\n",
    "from uuid import uuid4 # 解答例より (既にインポート済みだが明示)\n",
    "from langgraph.prebuilt import ToolNode # 解答例より (既にインポート済みだが明示)\n",
    "\n",
    "# search_tool は準備セルで TavilySearchResults またはダミーとして初期化済み\n",
    "# square_number, reverse_string も前の問題で定義済み\n",
    "\n",
    "tools_q3 = [square_number, reverse_string, search_tool] # search_tool を追加 (解答例より)\n",
    "\n",
    "# 状態定義 (BasicToolAgentStateを再利用)\n",
    "# class BasicToolAgentState(TypedDict): messages: Annotated[list, add_messages] (解答例よりコメントアウト)\n",
    "\n",
    "# ノード定義\n",
    "def multi_tool_agent_node(state: BasicToolAgentState):\n",
    "    print(\"\n",
    "[マルチツールエージェントノード]\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    response = None\n",
    "\n",
    "    if hasattr(llm, 'bind_tools') and LLM_PROVIDER != \"fake\":\n",
    "        llm_with_tools_q3 = llm.bind_tools(tools_q3)\n",
    "        response = llm_with_tools_q3.invoke(current_messages)\n",
    "    else:\n",
    "        print(\"  WARN: LLMがbind_toolsをサポートしていないかFakeLLMです。手動でツールコールを模倣します。\")\n",
    "        last_message = current_messages[-1]\n",
    "        tool_called_in_fallback = False\n",
    "        if isinstance(last_message, HumanMessage):\n",
    "            text_content = last_message.content.lower()\n",
    "            import re\n",
    "            # 検索ツールの呼び出し模倣 (Tavilyが利用可能か、またはダミーかで挙動が変わる) (解答例より)\n",
    "            if (\"天気\" in text_content or \"検索\" in text_content or \"調べて\" in text_content) and search_tool is not None: # 解答例より\n",
    "                query = text_content\n",
    "                if \"天気\" in text_content: query = text_content.replace(\"教えて\",\"\").strip(\"?？ \") # 解答例より\n",
    "                response = AIMessage(content=\"\", tool_calls=[ToolCall(name=search_tool.name, args={\"query\": query}, id=f\"ftc_search_{uuid4()[:4]}\")]) # 解答例より\n",
    "                tool_called_in_fallback = True\n",
    "            elif \"二乗\" in text_content:\n",
    "                match = re.search(r'(\\d+\\.?\\d*|\\.\\d+)', text_content)\n",
    "                if match: \n",
    "                    response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"square_number\", args={\"number\": float(match.group(1))}, id=f\"ftc_sq_{uuid4()[:4]}\")])\n",
    "                    tool_called_in_fallback = True\n",
    "            elif \"逆順\" in text_content: # 解答例より\n",
    "                match = re.search(r\"['\"]([^'\"]*)['\"]\", text_content) # 解答例より\n",
    "                str_to_rev = match.group(1) if match else text_content.replace(\"逆順\",\"\").strip() # 解答例より\n",
    "                response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"reverse_string\", args={\"text\": str_to_rev}, id=f\"ftc_rev_{uuid4()[:4]}\")]) # 解答例より\n",
    "                tool_called_in_fallback = True # 解答例より\n",
    "        \n",
    "        if not tool_called_in_fallback:\n",
    "            if isinstance(last_message, ToolMessage):\n",
    "                response = AIMessage(content=f\"FakeLLM: ツール「{last_message.name}」の結果「{last_message.content[:100]}...」を元に最終応答を生成します。\") # 解答例より\n",
    "            else:\n",
    "                response = AIMessage(content=\"FakeLLM: どのツールも適切でないと判断しました。通常の応答を返します。\") # 解答例より\n",
    "    \n",
    "    print(f\"  エージェント応答: {response}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_node_q3 = ToolNode(tools_q3) # tools_q3 を使用 (解答例より)\n",
    "\n",
    "# ルーター関数 (router_functionを再利用)\n",
    "# def router_function(state: BasicToolAgentState) -> str: ... (解答例よりコメントアウト)\n",
    "\n",
    "# グラフ構築\n",
    "workflow_q3 = StateGraph(BasicToolAgentState)\n",
    "workflow_q3.add_node(\"agent\", multi_tool_agent_node)\n",
    "workflow_q3.add_node(\"tools\", tool_node_q3)\n",
    "\n",
    "workflow_q3.set_entry_point(\"agent\")\n",
    "workflow_q3.add_conditional_edges(\"agent\", router_function, {\"call_tools\": \"tools\", \"__end__\": END})\n",
    "workflow_q3.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph_q3 = workflow_q3.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003 - グラフ可視化\n",
    "____ IPython.display ____ Image, display # 解答例より display をインポート\n",
    "\n",
    "____:\n",
    "    display(Image(graph_q3.get_graph().draw_png()))\n",
    "____ Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003 - グラフ実行\n",
    "queries_q3 = [\n",
    "    \"日本の首都、東京の今日の天気を教えてください。\",\n",
    "    \"数値 7 を二乗するといくつになりますか？\",\n",
    "    \"'supercalifragilisticexpialidocious' を逆順にするとどうなりますか？\",\n",
    "    \"今日の気分はどうですか？\" # ツール不要な質問\n",
    "]\n",
    "\n",
    "____ i, q ____ enumerate(queries_q3):\n",
    "    print(f\"\n",
    "--- マルチツール選択テスト {i+1} (入力: {q}) ---\")\n",
    "    ____ \"天気\" ____ q.lower() ____ (search_tool ____ None or (hasattr(search_tool, 'name') and search_tool.name == \"dummy_search_tool\")) and TAVILY_API_KEY is None: # 解答例より\n",
    "        print(\"  TAVILY_API_KEY が未設定のため、天気検索はダミー応答になります。\") # 解答例より\n",
    "\n",
    "    thread_q3 = {\"configurable\": {\"thread_id\": f\"test-thread-q3-{i}-{uuid4()[:4]}\"}} # よりユニークなID (解答例より)\n",
    "    initial_state_q3 = {\"messages\": [____(content=q)]}\n",
    "    final_content_q3 = \"(応答なし)\" # 解答例より\n",
    "    for event in graph_q3.____(initial_state_q3, config=thread_q3, recursion_limit=5): # 解答例より config 追加, recursion_limit は元の解答から\n",
    "        print(f\"Event: {event}\")\n",
    "        if 'agent' in event: # 解答例より\n",
    "            agent_messages = event['agent'].get('messages', [])\n",
    "            if agent_messages and isinstance(agent_messages[0], ____) and not agent_messages[0].____:\n",
    "                final_content_q3 = agent_messages[0].content\n",
    "        print(\"----\");\n",
    "    print(f\"最終応答内容: {final_content_q3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答003</summary>\n",
    "\n",
    "``````python\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, ToolCall\n",
    "from typing import TypedDict, Annotated, List\n",
    "from uuid import uuid4\n",
    "from IPython.display import Image, display\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# search_tool は準備セルで TavilySearchResults またはダミーとして初期化済み\n",
    "# square_number, reverse_string も前の問題で定義済み\n",
    "\n",
    "tools_q3 = [square_number, reverse_string, search_tool] \n",
    "\n",
    "# 状態定義 (BasicToolAgentStateを再利用)\n",
    "# class BasicToolAgentState(TypedDict): messages: Annotated[list, add_messages]\n",
    "\n",
    "# ノード定義\n",
    "def multi_tool_agent_node(state: BasicToolAgentState):\n",
    "    print(\"\n",
    "[マルチツールエージェントノード]\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    response = None\n",
    "\n",
    "    if hasattr(llm, 'bind_tools') and LLM_PROVIDER != \"fake\":\n",
    "        llm_with_tools_q3 = llm.bind_tools(tools_q3)\n",
    "        response = llm_with_tools_q3.invoke(current_messages)\n",
    "    else:\n",
    "        print(\"  WARN: LLMがbind_toolsをサポートしていないかFakeLLMです。手動でツールコールを模倣します。\")\n",
    "        last_message = current_messages[-1]\n",
    "        tool_called_in_fallback = False\n",
    "        if isinstance(last_message, HumanMessage):\n",
    "            text_content = last_message.content.lower()\n",
    "            import re\n",
    "            # 検索ツールの呼び出し模倣 (Tavilyが利用可能か、またはダミーかで挙動が変わる)\n",
    "            if (\"天気\" in text_content or \"検索\" in text_content or \"調べて\" in text_content) and search_tool is not None:\n",
    "                query = text_content\n",
    "                if \"天気\" in text_content: query = text_content.replace(\"教えて\",\"\").strip(\"?？ \")\n",
    "                # search_tool.name は TavilySearchResults インスタンスなら 'tavily_search_results_json'\n",
    "                # ダミーなら 'dummy_search_tool'\n",
    "                response = AIMessage(content=\"\", tool_calls=[ToolCall(name=search_tool.name, args={\"query\": query}, id=f\"ftc_search_{uuid4()[:4]}\")])\n",
    "                tool_called_in_fallback = True\n",
    "            elif \"二乗\" in text_content:\n",
    "                match = re.search(r'(\\d+\\.?\\d*|\\.\\d+)', text_content)\n",
    "                if match: \n",
    "                    response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"square_number\", args={\"number\": float(match.group(1))}, id=f\"ftc_sq_{uuid4()[:4]}\")])\n",
    "                    tool_called_in_fallback = True\n",
    "            elif \"逆順\" in text_content:\n",
    "                match = re.search(r\"['\"]([^'\"]*)['\"]\", text_content)\n",
    "                str_to_rev = match.group(1) if match else text_content.replace(\"逆順\",\"\").strip()\n",
    "                response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"reverse_string\", args={\"text\": str_to_rev}, id=f\"ftc_rev_{uuid4()[:4]}\")])\n",
    "                tool_called_in_fallback = True\n",
    "        \n",
    "        if not tool_called_in_fallback:\n",
    "            if isinstance(last_message, ToolMessage):\n",
    "                response = AIMessage(content=f\"FakeLLM: ツール「{last_message.name}」の結果「{last_message.content[:100]}...」を元に最終応答を生成します。\")\n",
    "            else:\n",
    "                response = AIMessage(content=\"FakeLLM: どのツールも適切でないと判断しました。通常の応答を返します。\")\n",
    "    \n",
    "    print(f\"  エージェント応答: {response}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_node_q3 = ToolNode(tools_q3)\n",
    "\n",
    "# ルーター関数 (router_functionを再利用)\n",
    "# def router_function(state: BasicToolAgentState) -> str: ... (問題001, 002と同じ)\n",
    "\n",
    "# グラフ構築\n",
    "workflow_q3 = StateGraph(BasicToolAgentState)\n",
    "workflow_q3.add_node(\"agent\", multi_tool_agent_node)\n",
    "workflow_q3.add_node(\"tools\", tool_node_q3)\n",
    "\n",
    "workflow_q3.set_entry_point(\"agent\")\n",
    "workflow_q3.add_conditional_edges(\"agent\", router_function, {\"call_tools\": \"tools\", \"__end__\": END})\n",
    "workflow_q3.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph_q3 = workflow_q3.compile()\n",
    "try:\n",
    "    display(Image(graph_q3.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")\n",
    "\n",
    "# 実行テスト\n",
    "queries_q3 = [\n",
    "    \"日本の首都、東京の今日の天気を教えてください。\",\n",
    "    \"数値 7 を二乗するといくつになりますか？\",\n",
    "    \"'supercalifragilisticexpialidocious' を逆順にするとどうなりますか？\",\n",
    "    \"今日の気分はどうですか？\" \n",
    "]\n",
    "\n",
    "for i, q in enumerate(queries_q3):\n",
    "    print(f\"\n",
    "--- マルチツール選択テスト {i+1} (入力: {q}) ---\")\n",
    "    if \"天気\" in q.lower() and (search_tool is None or (hasattr(search_tool, 'name') and search_tool.name == \"dummy_search_tool\")) and TAVILY_API_KEY is None:\n",
    "        print(\"  TAVILY_API_KEY が未設定のため、天気検索はダミー応答になります。\")\n",
    "\n",
    "    thread_q3 = {\"configurable\": {\"thread_id\": f\"test-thread-q3-{i}-{uuid4()[:4]}\"}} # よりユニークなID\n",
    "    initial_state_q3 = {\"messages\": [HumanMessage(content=q)]}\n",
    "    final_content_q3 = \"(応答なし)\"\n",
    "    for event in graph_q3.stream(initial_state_q3, config=thread_q3, recursion_limit=5):\n",
    "        print(f\"Event: {event}\")\n",
    "        if 'agent' in event:\n",
    "            agent_messages = event['agent'].get('messages', [])\n",
    "            if agent_messages and isinstance(agent_messages[0], AIMessage) and not agent_messages[0].tool_calls:\n",
    "                final_content_q3 = agent_messages[0].content\n",
    "        print(\"----\");\n",
    "    print(f\"最終応答内容: {final_content_q3}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説003</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **ツールセットの拡張:** `tools_q3 = [square_number, reverse_string, search_tool]` のように、エージェントが利用できるツールをリストとして定義し、これをLLMと`ToolNode`に提供します。`search_tool` は準備セルで `TavilySearchResults` またはそのダミーとして初期化されています。\n",
    "*   **LLMによるツール選択:** 理想的には、`multi_tool_agent_node` 内のLLM (`llm.bind_tools(tools_q3).invoke(...)`) が、ユーザーの質問の意図を理解し、`tools_q3` の中から最も適切なツール（またはツール不要と判断）を選択し、そのツールに必要な引数を生成して `tool_calls` に含めます。\n",
    "    *   例えば、「東京の天気は？」という質問なら `search_tool` を、「10を二乗して」なら `square_number` を選択することが期待されます。\n",
    "    *   ツールのdocstringと型ヒントが、ここでもLLMの正しいツール選択と引数生成に役立ちます。\n",
    "*   **Tavily Search APIの利用:** `TavilySearchResults` は、ウェブ検索を行うためのツールです。APIキー (`TAVILY_API_KEY`) が正しく設定されていれば、実際にウェブ検索を行いその結果を返します。APIキーがない場合やダミーのツールが使われている場合は、その旨を示すメッセージが返るか、あるいはエラーになります（この解答例ではダミーが機能するようにしています）。\n",
    "*   **グラフ構造と処理フロー:** グラフの構造自体は問題001や002とほぼ同じですが、エージェントがより多くのツールを扱えるようになった点が異なります。LLMがどのツールを選択するかによって、`ToolNode`で実行される具体的な処理が変わります。\n",
    "*   **FakeLLMでの模倣の複雑化:** `LLM_PROVIDER = \"fake\"` の場合のフォールバックロジックは、複数のツールに対応するためにさらに複雑になります。どのツールを呼び出すべきか、そしてその引数をどう抽出するかをルールベースで模倣する必要があり、これは実際のLLMの能力を完全に再現するものではありません。あくまで、グラフの基本的な流れとツール連携の概念を理解するためものと捉えてください。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題004: ツール呼び出しの強制 (Forced Tool Calling)\n",
    "\n",
    "特定の状況やタスクにおいて、エージェントに特定のツールを必ず使用させたい場合があります。LangChainの一部のLLM連携機能では、ツール呼び出しを「強制」するオプションが提供されています。この問題では、LLMに対して特定のツール（例: `square_number`）の使用を強制し、ユーザー入力の内容に関わらずそのツールが呼び出されるようにグラフを構築します。\n",
    "\n",
    "*   **学習内容:**\n",
    "    *   LLMの `bind_tools` メソッド（または類似の機能）において、特定のツール呼び出しを強制する `tool_choice` パラメータ（または等価のオプション）の利用方法（主にOpenAIのモデルで顕著）。\n",
    "    *   強制ツール呼び出しが設定された場合のLLMの応答（`tool_calls` に指定ツールが含まれる）と、それに続くグラフの挙動の確認。\n",
    "    *   この機能が利用できるLLMとそうでないLLMがあることの理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004 - グラフ構築\n",
    "from langchain_core.tools import tool # 解答例より (既にインポート済みだが明示)\n",
    "from ____.____ import ____, ____ # 解答例より (既にインポート済みだが明示)\n",
    "from ____.____.message import ____ # 解答例より (既にインポート済みだが明示)\n",
    "from langchain_core.messages import ____, ____, ____, ToolMessage, ToolCall # 解答例より (既にインポート済みだが明示)\n",
    "from typing import TypedDict, Annotated, List # 解答例より (既にインポート済みだが明示)\n",
    "from uuid import uuid4 # 解答例より (既にインポート済みだが明示)\n",
    "from langgraph.prebuilt import ToolNode # 解答例より (既にインポート済みだが明示)\n",
    "\n",
    "# square_number ツールは問題001で定義済み\n",
    "\n",
    "# 状態定義 (BasicToolAgentStateを再利用)\n",
    "# class BasicToolAgentState(TypedDict): messages: Annotated[list, add_messages] (解答例よりコメントアウト)\n",
    "\n",
    "# ノード定義\n",
    "def forced_tool_agent_node(state: BasicToolAgentState):\n",
    "    print(\"\n",
    "[強制ツール呼び出しエージェントノード]\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    response = None\n",
    "    # tool_choice_arg = {\"type\": \"tool\", \"name\": \"square_number\"} # OpenAIの tool_choice 形式 (解答例より)\n",
    "\n",
    "    if hasattr(llm, 'bind_tools') and LLM_PROVIDER == \"openai\": # 解答例の条件分岐を参考\n",
    "        try:\n",
    "            llm_force_square = llm.bind_tools([square_number], tool_choice=\"square_number\") # 強制するツール名を指定 (解答例より)\n",
    "            print(f\"  強制ツール呼び出し[square_number]を設定してLLMを実行します。\")\n",
    "            response = llm_force_square.invoke(current_messages)\n",
    "        except Exception as e:\n",
    "            print(f\"  WARN: tool_choice='square_number' でエラー: {e}。詳細な辞書形式で再試行します。\") # 解答例より\n",
    "            try:\n",
    "                 llm_force_square = llm.bind_tools([square_number], tool_choice={\"name\": \"square_number\"}) # 解答例より\n",
    "                 response = llm_force_square.invoke(current_messages)\n",
    "            except Exception as e2:\n",
    "                print(f\"  ERROR: 詳細な辞書形式でもエラー: {e2}。フォールバックします。\") # 解答例より\n",
    "                print(\"  Fallback: square_number(99) を強制的に呼び出します。 (入力無視)\") # 解答例より\n",
    "                response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"square_number\", args={\"number\": 99.0}, id=f\"ftc_forced_sq_{uuid4()[:4]}\")]) # 解答例より\n",
    "    elif hasattr(llm, 'bind_tools'): \n",
    "        print(\"  WARN: このLLMはtool_choiceによる強制ツール呼び出しを直接サポートしていない可能性があります。フォールバックします。\") # 解答例より\n",
    "        print(\"  Fallback: square_number(99) を強制的に呼び出します。 (入力無視)\")\n",
    "        response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"square_number\", args={\"number\": 99.0}, id=f\"ftc_forced_sq_{uuid4()[:4]}\")])\n",
    "    else: \n",
    "        print(\"  ERROR: このLLMはツールをバインドできません。強制呼び出しは不可能です。フォールバックします。\") # 解答例より\n",
    "        print(\"  Fallback: square_number(99) を強制的に呼び出します。 (入力無視)\") # 解答例より\n",
    "        response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"square_number\", args={\"number\": 99.0}, id=f\"ftc_forced_sq_{uuid4()[:4]}\")]) # 解答例より\n",
    "\n",
    "    print(f\"  エージェント応答: {response}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_node_q4 = ToolNode([square_number]) # square_numberのみを使用\n",
    "\n",
    "# ルーター関数 (router_functionを再利用)\n",
    "# def router_function(state: BasicToolAgentState) -> str: ... (解答例よりコメントアウト)\n",
    "\n",
    "# グラフ構築\n",
    "workflow_q4 = StateGraph(BasicToolAgentState)\n",
    "workflow_q4.add_node(\"agent\", forced_tool_agent_node)\n",
    "workflow_q4.add_node(\"tools\", tool_node_q4)\n",
    "\n",
    "workflow_q4.set_entry_point(\"agent\")\n",
    "workflow_q4.add_conditional_edges(\"agent\", router_function, {\"call_tools\": \"tools\", \"__end__\": END})\n",
    "workflow_q4.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph_q4 = workflow_q4.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004 - グラフ可視化\n",
    "____ IPython.display ____ Image, display # 解答例より display をインポート\n",
    "\n",
    "____:\n",
    "    display(Image(graph_q4.get_graph().draw_png()))\n",
    "____ Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004 - グラフ実行\n",
    "print(\"\n",
    "--- 強制ツール呼び出しテスト (入力は無視され、square_numberが呼ばれるはず) ---\")\n",
    "initial_state_q4 = {\"messages\": [____(content=\"こんにちは！今日の天気はどうですか？\")]} # 入力内容は強制呼び出しにより無視される想定\n",
    "thread_q4 = {\"configurable\": {\"thread_id\": f\"test-thread-q4-{uuid4()[:4]}\"}}}\n",
    "final_content_q4 = \"(応答なし)\"\n",
    "____ event ____ graph_q4.____(initial_state_q4, config=thread_q4, recursion_limit=5): # 解答例より config 追加, recursion_limit は元の解答から\n",
    "    print(f\"Event: {event}\")\n",
    "    ____ 'agent' ____ event: # 解答例より\n",
    "        agent_messages = event['agent'].get('messages', [])\n",
    "        ____ agent_messages ____ isinstance(agent_messages[0], ____) and not agent_messages[0].____:\n",
    "            final_content_q4 = agent_messages[0].content\n",
    "    print(\"----\");\n",
    "print(f\"最終応答内容: {final_content_q4}\")\n",
    "\n",
    "if \"9801\" in final_content_q4 or (LLM_PROVIDER == \"openai\" and final_content_q4): # OpenAIは引数をLLMが生成するので99とは限らない (解答例より)\n",
    "    print(f\"テスト成功の可能性: 応答にsquare_numberの結果らしきものが含まれています。 (LLMプロバイダ: {LLM_PROVIDER})\") # 解答例より\n",
    "else:\n",
    "    print(f\"テスト失敗/確認要: 応答に期待した結果が含まれていません。実際の応答: {final_content_q4} (LLMプロバイダ: {LLM_PROVIDER})\") # 解答例より"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答004</summary>\n",
    "\n",
    "``````python\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, ToolCall\n",
    "from typing import TypedDict, Annotated, List\n",
    "from uuid import uuid4\n",
    "from IPython.display import Image, display\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# square_number ツールは問題001で定義済み\n",
    "\n",
    "# 状態定義 (BasicToolAgentStateを再利用)\n",
    "# class BasicToolAgentState(TypedDict): messages: Annotated[list, add_messages]\n",
    "\n",
    "# ノード定義\n",
    "def forced_tool_agent_node(state: BasicToolAgentState):\n",
    "    print(\"\n",
    "[強制ツール呼び出しエージェントノード]\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    response = None\n",
    "    tool_choice_arg = {\"type\": \"tool\", \"name\": \"square_number\"} # OpenAIの tool_choice 形式\n",
    "\n",
    "    # OpenAIモデルなど、tool_choiceをサポートするLLMの場合\n",
    "    # (注意: llm.bind_tools の tool_choice 引数は LangChain のバージョンやモデルによって挙動が異なる場合がある)\n",
    "    #   ChatOpenAI の場合は invoke の引数 tool_choice で渡すのがより確実な場合もある。\n",
    "    #   llm.invoke(current_messages, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"square_number\"}})\n",
    "    #   ここでは bind_tools の tool_choice を試みる。\n",
    "    if hasattr(llm, 'bind_tools') and LLM_PROVIDER == \"openai\": \n",
    "        try:\n",
    "            # tool_choiceにツール名を直接渡すか、より詳細な辞書形式で渡す (モデル/バージョン依存)\n",
    "            # LangChainの ChatOpenAI では tool_choice=\"square_number\" のような文字列指定も可能だったり、\n",
    "            # あるいは {\"name\": \"square_number\"} のような辞書を期待する場合もある。\n",
    "            # 最新のOpenAI API仕様に準拠した形は {\"type\": \"function\", \"function\": {\"name\": \"tool_name\"}} だが、\n",
    "            # LangChainの抽象化レイヤーで扱いやすいように調整されていることが多い。\n",
    "            # ここでは最も一般的と思われるツール名直接指定を試みる。\n",
    "            llm_force_square = llm.bind_tools([square_number], tool_choice=\"square_number\")\n",
    "            print(f\"  強制ツール呼び出し[square_number]を設定してLLMを実行します。\")\n",
    "            response = llm_force_square.invoke(current_messages)\n",
    "        except Exception as e:\n",
    "            print(f\"  WARN: tool_choice='square_number' でエラー: {e}。詳細な辞書形式で再試行します。\")\n",
    "            try:\n",
    "                 llm_force_square = llm.bind_tools([square_number], tool_choice={\"name\": \"square_number\"})\n",
    "                 response = llm_force_square.invoke(current_messages)\n",
    "            except Exception as e2:\n",
    "                print(f\"  ERROR: 詳細な辞書形式でもエラー: {e2}。フォールバックします。\")\n",
    "                # フォールバック: OpenAI以外またはエラー時\n",
    "                print(\"  Fallback: square_number(99) を強制的に呼び出します。 (入力無視)\")\n",
    "                response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"square_number\", args={\"number\": 99.0}, id=f\"ftc_forced_sq_{uuid4()[:4]}\")])\n",
    "    \n",
    "    elif hasattr(llm, 'bind_tools'): # OpenAI以外のLLMでtool_choiceがない場合\n",
    "        print(\"  WARN: このLLMはtool_choiceによる強制ツール呼び出しを直接サポートしていない可能性があります。フォールバックします。\")\n",
    "        print(\"  Fallback: square_number(99) を強制的に呼び出します。 (入力無視)\")\n",
    "        response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"square_number\", args={\"number\": 99.0}, id=f\"ftc_forced_sq_{uuid4()[:4]}\")])\n",
    "    else: # bind_tools もない場合\n",
    "        print(\"  ERROR: このLLMはツールをバインドできません。強制呼び出しは不可能です。フォールバックします。\")\n",
    "        # response = AIMessage(content=\"エラー: ツール利用不可のため強制呼び出しできません。\")\n",
    "        print(\"  Fallback: square_number(99) を強制的に呼び出します。 (入力無視)\") # エラーよりはダミー実行\n",
    "        response = AIMessage(content=\"\", tool_calls=[ToolCall(name=\"square_number\", args={\"number\": 99.0}, id=f\"ftc_forced_sq_{uuid4()[:4]}\")])\n",
    "\n",
    "    print(f\"  エージェント応答: {response}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_node_q4 = ToolNode([square_number])\n",
    "\n",
    "# ルーター関数 (router_functionを再利用)\n",
    "# def router_function(state: BasicToolAgentState) -> str: ... (問題001, 002と同じ)\n",
    "\n",
    "# グラフ構築\n",
    "workflow_q4 = StateGraph(BasicToolAgentState)\n",
    "workflow_q4.add_node(\"agent\", forced_tool_agent_node)\n",
    "workflow_q4.add_node(\"tools\", tool_node_q4)\n",
    "\n",
    "workflow_q4.set_entry_point(\"agent\")\n",
    "workflow_q4.add_conditional_edges(\"agent\", router_function, {\"call_tools\": \"tools\", \"__end__\": END})\n",
    "workflow_q4.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph_q4 = workflow_q4.compile()\n",
    "try:\n",
    "    display(Image(graph_q4.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")\n",
    "\n",
    "# 実行テスト\n",
    "print(\"\n",
    "--- 強制ツール呼び出しテスト (入力は無視され、square_numberが呼ばれるはず) ---\")\n",
    "initial_state_q4 = {\"messages\": [HumanMessage(content=\"こんにちは！今日の天気はどうですか？\")]}\n",
    "thread_q4 = {\"configurable\": {\"thread_id\": f\"test-thread-q4-{uuid4()[:4]}\"}}}\n",
    "final_content_q4 = \"(応答なし)\"\n",
    "for event in graph_q4.stream(initial_state_q4, config=thread_q4, recursion_limit=5):\n",
    "    print(f\"Event: {event}\")\n",
    "    if 'agent' in event:\n",
    "        agent_messages = event['agent'].get('messages', [])\n",
    "        if agent_messages and isinstance(agent_messages[0], AIMessage) and not agent_messages[0].tool_calls:\n",
    "            final_content_q4 = agent_messages[0].content\n",
    "    print(\"----\");\n",
    "print(f\"最終応答内容: {final_content_q4}\")\n",
    "\n",
    "if \"9801\" in final_content_q4 or (LLM_PROVIDER == \"openai\" and final_content_q4): # OpenAIは引数をLLMが生成するので99とは限らない\n",
    "    print(f\"テスト成功の可能性: 応答にsquare_numberの結果らしきものが含まれています。 (LLMプロバイダ: {LLM_PROVIDER})\")\n",
    "else:\n",
    "    print(f\"テスト失敗/確認要: 応答に期待した結果が含まれていません。実際の応答: {final_content_q4} (LLMプロバイダ: {LLM_PROVIDER})\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説004</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **`tool_choice`パラメータ:** OpenAIのモデル（`gpt-3.5-turbo`以降など）では、LLM呼び出し時に`tool_choice`パラメータを指定することで、特定のツールの使用を強制したり、あるいはツール使用を禁止したり（`\"none\"`）、LLMに自動選択させたり（`\"auto\"`、デフォルト）することができます。\n",
    "    *   特定のツールを強制する場合、`tool_choice=\"<ツール名>\"` や `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"<ツール名>\"}}` のような形式で指定します（正確な形式はLangChainのバージョンやOpenAI APIの仕様変更に注意してください）。\n",
    "    *   この解答例では、`llm.bind_tools([square_number], tool_choice=\"square_number\")` のように、`bind_tools`の引数として`tool_choice`を渡すことを試みています。これが機能すれば、LLMは入力メッセージの内容に関わらず、`square_number`ツールを呼び出すための`tool_calls`を生成しようとします（引数`number`の値はLLMが文脈から適当に判断するか、デフォルト値を使うなどします）。\n",
    "*   **LLMによる引数の補完:** ツール呼び出しを強制された場合でも、そのツールが必要とする引数はLLMが現在の会話履歴や文脈から推測して補完しようとします。もし文脈から適切な引数が得られない場合は、LLMがデフォルト値や適当な値を設定することがあります（例: `square_number`なら適当な数値）。\n",
    "*   **対応していないLLMの場合:** `tool_choice`のような強制呼び出し機能は、全てのLLMでサポートされているわけではありません。サポートされていないLLMの場合、この設定は無視されるか、エラーになることがあります。この解答例のフォールバック処理では、そのような場合に手動で特定のツールコール（`square_number(99.0)`）を生成して、グラフのフロー自体は追えるようにしています。\n",
    "*   **ユースケース:**\n",
    "    *   特定の分析ツールを定期的に実行させたい場合。\n",
    "    *   ユーザーの意図が曖昧でも、まず特定の情報収集ツールを使わせたい場合。\n",
    "    *   マルチモーダル入力で、画像が提供されたら必ず画像解析ツールを呼び出す、など。\n",
    "*   **注意点:** 強制ツール呼び出しは便利な機能ですが、LLMの自律的な判断を妨げることにもなるため、その利用は慎重に検討する必要があります。タスクの性質やエージェントの設計思想に合わせて使い分けることが重要です。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題005: 複数ツールの並列実行\n",
    "\n",
    "LLMが一度の応答で複数の異なるツール呼び出しを要求し、それらのツールを並列で実行させたい場合があります（例: 「東京の天気と、大阪の天気の両方を調べて」）。LangGraphの`ToolNode`は、デフォルトで複数のツールコールを並列に実行できます。この問題では、LLMが一つのリクエストに対して複数のツール（例: 2つの異なる検索クエリ）を同時に呼び出すように促し、それらが並行して処理され、結果が統合されてLLMに返される流れを確認します。\n",
    "\n",
    "*   **学習内容:**\n",
    "    *   LLMが応答の`tool_calls`リストに複数のツール呼び出しを含めるようにプロンプト等で仕向ける方法（または、対応モデルの自然な振る舞いとして期待）。\n",
    "    *   `ToolNode`が複数のツール呼び出しをどのように処理するか（通常は並列実行）。\n",
    "    *   複数のツール実行結果（複数の`ToolMessage`）がどのようにエージェントに返され、後続の処理に利用されるか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005 - グラフ構築\n",
    "from langchain_core.tools import tool # 解答例より (既にインポート済みだが明示)\n",
    "from ____.____ import ____, ____ # 解答例より (既にインポート済みだが明示)\n",
    "from ____.____.message import ____ # 解答例より (既にインポート済みだが明示)\n",
    "from langchain_core.messages import ____, ____, ____, ToolMessage, ToolCall # 解答例より (既にインポート済みだが明示)\n",
    "from typing import TypedDict, Annotated, List # 解答例より (既にインポート済みだが明示)\n",
    "from uuid import uuid4 # 解答例より (既にインポート済みだが明示)\n",
    "from langgraph.prebuilt import ToolNode # 解答例より (既にインポート済みだが明示)\n",
    "import re # 解答例より\n",
    "\n",
    "# search_tool は準備セルで初期化済み\n",
    "\n",
    "tools_q5 = [search_tool] # search_tool のみを使用 (複数の呼び出しをLLMに期待) (解答例より)\n",
    "\n",
    "# 状態定義 (BasicToolAgentStateを再利用)\n",
    "# class BasicToolAgentState(TypedDict): messages: Annotated[list, add_messages] (解答例よりコメントアウト)\n",
    "\n",
    "# ノード定義\n",
    "def parallel_tool_agent_node(state: BasicToolAgentState):\n",
    "    print(\"\n",
    "[並列ツール呼び出しエージェントノード]\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    response = None\n",
    "\n",
    "    if hasattr(llm, 'bind_tools') and LLM_PROVIDER != \"fake\" and hasattr(llm.bind_tools(tools_q5), 'model') and not \"Fake\" in llm.bind_tools(tools_q5).model.__class__.__name__ : # 解答例の条件分岐\n",
    "        llm_with_search_q5 = llm.bind_tools(tools_q5) \n",
    "        print(\"  LLMに複数ツール呼び出しを期待して実行します。\") # 解答例より\n",
    "        response = llm_with_search_q5.invoke(current_messages)\n",
    "    else:\n",
    "        print(\"  WARN: LLMがbind_toolsをサポートしていないかFakeLLM、または並列呼び出しを期待できないモデルです。手動で複数ツールコールを模倣します。\") # 解答例より\n",
    "        last_message = current_messages[-1]\n",
    "        if isinstance(last_message, HumanMessage) and (\"と\" in last_message.content or \"and\" in last_message.content.lower()) and (\"天気\" in last_message.content or \"観光\" in last_message.content or \"情報\" in last_message.content): # 解答例の条件分岐\n",
    "            queries = []\n",
    "            parts = re.split(r'[と、]', last_message.content) # 解答例より\n",
    "            for part in parts: # 解答例より\n",
    "                part = part.strip()\n",
    "                if \"天気\" in part: queries.append(part)\n",
    "                elif \"観光\" in part or \"情報\" in part : queries.append(part)\n",
    "            \n",
    "            if not queries and len(parts) > 0: queries = [p.strip() for p in parts if p.strip()] # 解答例より\n",
    "            if not queries: queries.append(last_message.content) # 解答例より\n",
    "\n",
    "            if len(queries) >= 1 and search_tool is not None: # 解答例より\n",
    "                tool_calls_list = []\n",
    "                for i, q in enumerate(queries[:2]): # 最大2つまでの並列呼び出しを模倣 (解答例より)\n",
    "                    print(f\"    FakeLLM: 検索クエリ「{q}」で{search_tool.name}を呼び出します。\") # 解答例より\n",
    "                    tool_calls_list.append(ToolCall(name=search_tool.name, args={\"query\": q}, id=f\"ftc_parallel_{i}_{uuid4()[:4]}\"))\n",
    "                if tool_calls_list: # 解答例より\n",
    "                    response = AIMessage(content=f\"FakeLLM: {len(tool_calls_list)}件の情報を検索します。\", tool_calls=tool_calls_list) # 解答例より\n",
    "                else: # 解答例より\n",
    "                    response = AIMessage(content=\"FakeLLM: 検索クエリを特定できませんでした。\") # 解答例より\n",
    "            else: # 解答例より\n",
    "                 response = AIMessage(content=\"FakeLLM: 検索ツールが利用できないか、クエリがありません。\") # 解答例より\n",
    "        elif isinstance(last_message, ToolMessage):\n",
    "            all_tool_messages = [m.content for m in current_messages if isinstance(m, ToolMessage)] # 解答例より\n",
    "            response_content = f\"FakeLLM: {len(all_tool_messages)}件のツール結果「{', '.join([res[:30] + '...' for res in all_tool_messages])}」を元に最終応答を生成します。\" # 解答例より\n",
    "            response = AIMessage(content=response_content) # 解答例より\n",
    "        else:\n",
    "            response = AIMessage(content=\"FakeLLM: 複数ツールの呼び出し条件にマッチしませんでした。通常の応答を返します。\") # 解答例より\n",
    "\n",
    "    print(f\"  エージェント応答: {response}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_node_q5 = ToolNode(tools_q5) # tools_q5 を使用 (解答例より)\n",
    "\n",
    "# ルーター関数 (router_functionを再利用)\n",
    "# def router_function(state: BasicToolAgentState) -> str: ... (解答例よりコメントアウト)\n",
    "\n",
    "# グラフ構築\n",
    "workflow_q5 = StateGraph(BasicToolAgentState)\n",
    "workflow_q5.add_node(\"agent\", parallel_tool_agent_node)\n",
    "workflow_q5.add_node(\"tools\", tool_node_q5)\n",
    "\n",
    "workflow_q5.set_entry_point(\"agent\")\n",
    "workflow_q5.add_conditional_edges(\"agent\", router_function, {\"call_tools\": \"tools\", \"__end__\": END})\n",
    "workflow_q5.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph_q5 = workflow_q5.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005 - グラフ可視化\n",
    "____ IPython.display ____ Image, display # 解答例より display をインポート\n",
    "\n",
    "____:\n",
    "    display(Image(graph_q5.get_graph().draw_png()))\n",
    "____ Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005 - グラフ実行\n",
    "query_q5 = \"東京の天気と、京都の観光情報を同時に調べてください。\"\n",
    "____ (search_tool ____ ____ ____ (hasattr(search_tool, 'name') and search_tool.name == \"dummy_search_tool\")) and TAVILY_API_KEY is None:\n",
    "    print(\"TAVILY_API_KEY が未設定のため、このテストはダミー応答になります。\")\n",
    "\n",
    "print(f\"\n",
    "--- 並列ツール呼び出しテスト (入力: {query_q5}) ---\")\n",
    "initial_state_q5 = {\"messages\": [____(content=query_q5)]}\n",
    "thread_q5 = {\"configurable\": {\"thread_id\": f\"test-thread-q5-{uuid4()[:4]}\"}}}\n",
    "final_content_q5 = \"(応答なし)\"\n",
    "num_tool_calls_in_response = 0 # 解答例より\n",
    "\n",
    "for event in graph_q5.____(initial_state_q5, config=thread_q5, recursion_limit=5): # 解答例より config 追加, recursion_limit は元の解答から\n",
    "    print(f\"Event: {event}\")\n",
    "    if 'agent' in event: # 解答例より\n",
    "        agent_messages = event['agent'].get('messages', [])\n",
    "        if agent_messages and isinstance(agent_messages[0], ____):\n",
    "            if agent_messages[0].____:\n",
    "                num_tool_calls_in_response = len(agent_messages[0].____)\n",
    "                print(f\"  エージェントが {num_tool_calls_in_response} 件のツール呼び出しを要求しました。\")\n",
    "            elif not agent_messages[0].____ : # 最終応答\n",
    "                final_content_q5 = agent_messages[0].content\n",
    "    if 'tools' in event: # 解答例より\n",
    "        tool_messages = event['tools'].get('messages', [])\n",
    "        if len(tool_messages) > 1:\n",
    "            print(f\"  ToolNodeが複数のToolMessage ({len(tool_messages)}件) を返しました。\")\n",
    "        elif tool_messages:\n",
    "            print(f\"  ToolNodeがToolMessageを返しました: {tool_messages[0].name if tool_messages else ''}\")\n",
    "    print(\"----\");\n",
    "\n",
    "print(f\"\n",
    "LLMが要求したツール呼び出しの数: {num_tool_calls_in_response}\") # 解答例より\n",
    "print(f\"最終応答内容: {final_content_q5}\")\n",
    "if num_tool_calls_in_response >= 2 and LLM_PROVIDER != \"fake\": # 解答例より\n",
    "    print(\"テスト成功の可能性(実際のLLMの場合): LLMが複数のツール呼び出しを要求しました。\") # 解答例より\n",
    "elif num_tool_calls_in_response >=1 and LLM_PROVIDER == \"fake\": # 解答例より\n",
    "    print(\"テスト成功の可能性(FakeLLMの場合): FakeLLMが模倣によりツール呼び出しを生成しました。\") # 解答例より\n",
    "else: # 解答例より\n",
    "    print(f\"テスト確認要: LLMが期待通りに複数のツール呼び出しを要求しませんでした（実際の呼び出し数: {num_tool_calls_in_response}）。プロンプトやLLMの能力を確認してください。\") # 解答例より"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答005</summary>\n",
    "\n",
    "``````python\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, ToolCall\n",
    "from typing import TypedDict, Annotated, List\n",
    "from uuid import uuid4\n",
    "from IPython.display import Image, display\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# search_tool は準備セルで TavilySearchResults またはダミーとして初期化済み\n",
    "\n",
    "tools_q5 = [search_tool] # search_tool のみを使用\n",
    "\n",
    "# 状態定義 (BasicToolAgentStateを再利用)\n",
    "# class BasicToolAgentState(TypedDict): messages: Annotated[list, add_messages]\n",
    "\n",
    "# ノード定義\n",
    "def parallel_tool_agent_node(state: BasicToolAgentState):\n",
    "    print(\"\n",
    "[並列ツール呼び出しエージェントノード]\")\n",
    "    current_messages = state[\"messages\"]\n",
    "    response = None\n",
    "\n",
    "    if hasattr(llm, 'bind_tools') and LLM_PROVIDER != \"fake\" and hasattr(llm.bind_tools(tools_q5), 'model') and not \"Fake\" in llm.bind_tools(tools_q5).model.__class__.__name__ :\n",
    "        # OpenAIのFunction Calling対応モデルなどは、プロンプト次第で複数のツールコールを返すことがある\n",
    "        # 例: 「東京の天気と大阪の天気を調べて」\n",
    "        # この挙動はLLMの能力に大きく依存する\n",
    "        llm_with_search_q5 = llm.bind_tools(tools_q5) \n",
    "        print(\"  LLMに複数ツール呼び出しを期待して実行します。\")\n",
    "        response = llm_with_search_q5.invoke(current_messages)\n",
    "    else:\n",
    "        print(\"  WARN: LLMがbind_toolsをサポートしていないかFakeLLM、または並列呼び出しを期待できないモデルです。手動で複数ツールコールを模倣します。\")\n",
    "        last_message = current_messages[-1]\n",
    "        if isinstance(last_message, HumanMessage) and (\"と\" in last_message.content or \"and\" in last_message.content.lower()) and (\"天気\" in last_message.content or \"観光\" in last_message.content or \"情報\" in last_message.content):\n",
    "            queries = []\n",
    "            # この模倣は非常に単純であり、実際のLLMのNLU能力とは異なります\n",
    "            # 簡易的に「と」や「、」で分割してクエリ候補を生成\n",
    "            parts = re.split(r'[と、]', last_message.content)\n",
    "            for part in parts:\n",
    "                part = part.strip()\n",
    "                if \"天気\" in part: queries.append(part)\n",
    "                elif \"観光\" in part or \"情報\" in part : queries.append(part)\n",
    "            \n",
    "            if not queries and len(parts) > 0: # 上手く抽出できなかった場合、partをそのまま使う\n",
    "                queries = [p.strip() for p in parts if p.strip()]\n",
    "            if not queries: queries.append(last_message.content) # それでもダメなら全体\n",
    "\n",
    "            if len(queries) >= 1 and search_tool is not None:\n",
    "                tool_calls_list = []\n",
    "                for i, q in enumerate(queries[:2]): # 最大2つまでの並列呼び出しを模倣\n",
    "                    print(f\"    FakeLLM: 検索クエリ「{q}」で{search_tool.name}を呼び出します。\")\n",
    "                    tool_calls_list.append(ToolCall(name=search_tool.name, args={\"query\": q}, id=f\"ftc_parallel_{i}_{uuid4()[:4]}\"))\n",
    "                if tool_calls_list:\n",
    "                    response = AIMessage(content=f\"FakeLLM: {len(tool_calls_list)}件の情報を検索します。\", tool_calls=tool_calls_list)\n",
    "                else:\n",
    "                    response = AIMessage(content=\"FakeLLM: 検索クエリを特定できませんでした。\")\n",
    "            else:\n",
    "                 response = AIMessage(content=\"FakeLLM: 検索ツールが利用できないか、クエリがありません。\")\n",
    "        elif isinstance(last_message, ToolMessage):\n",
    "            # 実際には複数のToolMessageがmessagesリストに入ってくるので、それらを考慮して応答を生成する必要がある\n",
    "            # ここでは最後のToolMessageだけを簡単に参照\n",
    "            all_tool_messages = [m.content for m in current_messages if isinstance(m, ToolMessage)]\n",
    "            response_content = f\"FakeLLM: {len(all_tool_messages)}件のツール結果「{', '.join([res[:30] + '...' for res in all_tool_messages])}」を元に最終応答を生成します。\"\n",
    "            response = AIMessage(content=response_content)\n",
    "        else:\n",
    "            response = AIMessage(content=\"FakeLLM: 複数ツールの呼び出し条件にマッチしませんでした。通常の応答を返します。\")\n",
    "\n",
    "    print(f\"  エージェント応答: {response}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_node_q5 = ToolNode(tools_q5)\n",
    "\n",
    "# ルーター関数 (router_functionを再利用)\n",
    "# def router_function(state: BasicToolAgentState) -> str: ...\n",
    "\n",
    "# グラフ構築\n",
    "workflow_q5 = StateGraph(BasicToolAgentState)\n",
    "workflow_q5.add_node(\"agent\", parallel_tool_agent_node)\n",
    "workflow_q5.add_node(\"tools\", tool_node_q5)\n",
    "\n",
    "workflow_q5.set_entry_point(\"agent\")\n",
    "workflow_q5.add_conditional_edges(\"agent\", router_function, {\"call_tools\": \"tools\", \"__end__\": END})\n",
    "workflow_q5.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph_q5 = workflow_q5.compile()\n",
    "try:\n",
    "    display(Image(graph_q5.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")\n",
    "\n",
    "# 実行テスト\n",
    "query_q5 = \"東京の天気と、京都の観光情報を同時に調べてください。\"\n",
    "if (search_tool is None or (hasattr(search_tool, 'name') and search_tool.name == \"dummy_search_tool\")) and TAVILY_API_KEY is None:\n",
    "    print(\"TAVILY_API_KEY が未設定のため、このテストはダミー応答になります。\")\n",
    "\n",
    "print(f\"\n",
    "--- 並列ツール呼び出しテスト (入力: {query_q5}) ---\")\n",
    "initial_state_q5 = {\"messages\": [HumanMessage(content=query_q5)]}\n",
    "thread_q5 = {\"configurable\": {\"thread_id\": f\"test-thread-q5-{uuid4()[:4]}\"}}}\n",
    "final_content_q5 = \"(応答なし)\"\n",
    "num_tool_calls_in_response = 0\n",
    "\n",
    "for event in graph_q5.stream(initial_state_q5, config=thread_q5, recursion_limit=5):\n",
    "    print(f\"Event: {event}\")\n",
    "    if 'agent' in event:\n",
    "        agent_messages = event['agent'].get('messages', [])\n",
    "        if agent_messages and isinstance(agent_messages[0], AIMessage):\n",
    "            if agent_messages[0].tool_calls:\n",
    "                num_tool_calls_in_response = len(agent_messages[0].tool_calls)\n",
    "                print(f\"  エージェントが {num_tool_calls_in_response} 件のツール呼び出しを要求しました。\")\n",
    "            elif not agent_messages[0].tool_calls : # 最終応答\n",
    "                final_content_q5 = agent_messages[0].content\n",
    "    if 'tools' in event:\n",
    "        tool_messages = event['tools'].get('messages', [])\n",
    "        if len(tool_messages) > 1:\n",
    "            print(f\"  ToolNodeが複数のToolMessage ({len(tool_messages)}件) を返しました。\")\n",
    "        elif tool_messages:\n",
    "            print(f\"  ToolNodeがToolMessageを返しました: {tool_messages[0].name if tool_messages else ''}\")\n",
    "    print(\"----\");\n",
    "\n",
    "print(f\"\n",
    "LLMが要求したツール呼び出しの数: {num_tool_calls_in_response}\")\n",
    "print(f\"最終応答内容: {final_content_q5}\")\n",
    "if num_tool_calls_in_response >= 2 and LLM_PROVIDER != \"fake\":\n",
    "    print(\"テスト成功の可能性(実際のLLMの場合): LLMが複数のツール呼び出しを要求しました。\")\n",
    "elif num_tool_calls_in_response >=1 and LLM_PROVIDER == \"fake\":\n",
    "    print(\"テスト成功の可能性(FakeLLMの場合): FakeLLMが模倣によりツール呼び出しを生成しました。\")\n",
    "else:\n",
    "    print(f\"テスト確認要: LLMが期待通りに複数のツール呼び出しを要求しませんでした（実際の呼び出し数: {num_tool_calls_in_response}）。プロンプトやLLMの能力を確認してください。\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説005</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **LLMによる複数ツール呼び出しの要求:**\n",
    "    *   OpenAIのFunction Calling対応モデルなど、一部の高度なLLMは、ユーザーのプロンプトが複数の情報要求を含んでいる場合（例: 「東京の天気と大阪の天気を教えて」）、応答の`tool_calls`リストに複数の`ToolCall`オブジェクトを含める能力があります。これにより、一度に複数のツール（または同じツールを異なる引数で複数回）を呼び出すよう指示できます。\n",
    "    *   この挙動はLLMの能力とプロンプトの設計に大きく依存します。必ずしも全てのLLMが期待通りに複数のツールコールを生成するわけではありません。\n",
    "    *   FakeLLMを使用している場合、この解答例のフォールバックでは、入力文字列に「と」や「天気」「観光」などのキーワードが含まれている場合に、手動で複数の`ToolCall`オブジェクトを生成して模倣しています。\n",
    "*   **`ToolNode`による並列実行:** `ToolNode`は、受け取った`AIMessage`の`tool_calls`リストに含まれる複数のツール呼び出しを（通常は非同期に、つまり実質的に）並列で実行します。各ツール呼び出しは独立して処理され、それぞれの結果が個別の`ToolMessage`として生成されます。\n",
    "*   **結果の集約と再処理:** `ToolNode`が生成した複数の`ToolMessage`は、リストとして次のノード（通常はエージェントノード）の状態の`messages`キーに追加されます。エージェントノードのLLMは、これらの複数のツール実行結果をすべて考慮して、最終的な統合された応答をユーザーに生成します。\n",
    "*   **ユースケース:**\n",
    "    *   複数の異なる情報源からのデータを一度に収集・比較する（例: 複数の都市の天気を同時に調べる、複数の株価を同時に取得する）。\n",
    "    *   一つのタスクを複数のサブタスクに分割し、それぞれを異なるツールで並列処理する。\n",
    "*   **注意点:** LLMが期待通りに複数のツールコールを生成するかどうかは、LLMのモデル、バージョン、プロンプトの書き方などに影響されます。また、あまりにも多くのツールコールを一度に要求すると、LLMのコンテキストウィンドウの制限や処理能力の限界を超える可能性もあります。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題006: シンプルな Plan-and-Execute エージェントの構築\n",
    "\n",
    "Plan-and-Executeは、エージェントが複雑なタスクに取り組むための一つの戦略です。まず「計画（Plan）」を立て、その計画に基づいて個々のステップを「実行（Execute）」していきます。この問題では、この戦略の非常にシンプルな形をLangGraphで実装します。\n",
    "1.  **プランナーノード:** ユーザーの要求を受けて、それを達成するためのステップのリスト（計画）を生成します（LLMを使用）。\n",
    "2.  **エグゼキューターノード:** 計画の各ステップを順番に（またはLLMの指示で）実行します。ここでは、ステップ実行のシミュレーションとして、簡単なツール（例: `square_number` や `reverse_string`、または単にステップ内容をログ出力するだけ）を使用します。\n",
    "3.  計画の全ステップが完了したら終了します。\n",
    "\n",
    "*   **学習内容:**\n",
    "    *   タスクを複数のステップに分解する「プランナー」の役割と、各ステップを実行する「エグゼキューター」の役割を異なるノードに分離する考え方。\n",
    "    *   状態（State）に計画全体、現在のステップ、実行結果などを保持し、それに基づいて処理を進める方法。\n",
    "    *   簡単なループ構造（計画のステップを順に処理する）と条件分岐（計画が完了したかどうかの判断）の組み合わせ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄006 - グラフ構築\n",
    "from typing import ____, List, Optional, ____\n",
    "from ____.____ import ____, ____\n",
    "from ____.____.message import ____ # メッセージ履歴も活用する場合\n",
    "from langchain_core.messages import ____, AIMessage, ToolMessage, ToolCall # ToolMessage, ToolCallは直接は使わないが概念として (解答例より)\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "from uuid import uuid4 # 解答例より\n",
    "\n",
    "# --- ツールの準備 (既存のものを再利用または新規定義) ---\n",
    "# @tool def simple_math_tool(expression: str) -> str: ... (例)\n",
    "# ここでは、square_number と reverse_string を使う想定\n",
    "available_tools_for_plan = {t.name: t for t in [square_number, reverse_string]}\n",
    "\n",
    "# --- 状態定義 ---\n",
    "class PlanExecuteState(TypedDict):\n",
    "    user_request: str\n",
    "    plan: Optional[List[dict]] # 例: [{'tool_name': 'square_number', 'args': {'number': 5}}, ...]\n",
    "    current_step_index: int\n",
    "    step_results: List[str] # 各ステップの実行結果(ToolMessageの内容など)\n",
    "    final_answer: Optional[str]\n",
    "    messages: Annotated[list, add_messages] # 会話ログ用\n",
    "\n",
    "# --- ノード定義 ---\n",
    "def planner_node(state: PlanExecuteState):\n",
    "    print(f\"\n",
    "[プランナーノード] ユーザーリクエスト: {state['user_request']}\")\n",
    "    request = state['user_request'].lower()\n",
    "    generated_plan = []\n",
    "    import re # 解答例より\n",
    "    \n",
    "    if \"二乗\" in request and \"逆順\" in request:\n",
    "        num_match = re.search(r'(\\d+\\.?\\d*|\\.\\d+)', request) # 解答例より\n",
    "        num_for_square = float(num_match.group(1)) if num_match else 0.0 # 解答例より\n",
    "        generated_plan.append({\"tool_name\": \"square_number\", \"args\": {\"number\": num_for_square}, \"step_description\": f\"{num_for_square}を二乗する\"})\n",
    "        generated_plan.append({\"tool_name\": \"reverse_string\", \"args\": {\"text\": \"<前のステップの結果>\"}, \"step_description\": \"前のステップの結果を逆順にする\"})\n",
    "    elif \"二乗\" in request:\n",
    "        num_match = re.search(r'(\\d+\\.?\\d*|\\.\\d+)', request) # 解答例より\n",
    "        num_for_square = float(num_match.group(1)) if num_match else 0.0 # 解答例より\n",
    "        generated_plan.append({\"tool_name\": \"square_number\", \"args\": {\"number\": num_for_square}, \"step_description\": f\"{num_for_square}を二乗する\"})\n",
    "    elif \"逆順\" in request: # 解答例より\n",
    "        str_match = re.search(r\"['\"]([^'\"]*)['\"]\", request) # 解答例より\n",
    "        text_to_rev = str_match.group(1) if str_match else request.replace(\"逆順に\",\"\").replace(\"逆順\",\"\").strip() # 解答例より\n",
    "        generated_plan.append({\"tool_name\": \"reverse_string\", \"args\": {\"text\": text_to_rev}, \"step_description\": f\"「{text_to_rev}」を逆順にする\"}) # 解答例より\n",
    "    else:\n",
    "        generated_plan.append({\"tool_name\": \"direct_answer\", \"args\": {\"text\": \"計画を立てられませんでした。\"}, \"step_description\": \"直接応答\"})\n",
    "        \n",
    "    print(f\"  生成された計画: {generated_plan}\")\n",
    "    return {\"plan\": generated_plan, \"current_step_index\": 0, \"step_results\": [], \"messages\": [AIMessage(content=f\"計画を立てました: {json.dumps(generated_plan)}\")]}\n",
    "\n",
    "def executor_node(state: PlanExecuteState): # 解答例では updated_executor_node は不要で executor_node 内で idx 更新\n",
    "    plan = state[\"plan\"]\n",
    "    idx = state[\"current_step_index\"]\n",
    "    \n",
    "    if not plan or idx >= len(plan):\n",
    "        print(\"[エグゼキューターノード] 実行すべきステップがありません。\") # 解答例より\n",
    "        return {\"current_step_index\": idx} # 変更なし (解答例より)\n",
    "    \n",
    "    current_step = plan[idx]\n",
    "    tool_name = current_step[\"tool_name\"]\n",
    "    tool_args = current_step[\"args\"].copy() \n",
    "    step_desc = current_step[\"step_description\"]\n",
    "    print(f\"\n",
    "[エグゼキューターノード] ステップ {idx + 1}/{len(plan)}: {step_desc} (ツール: {tool_name}, 引数: {tool_args})\")\n",
    "\n",
    "    if tool_args.get(\"text\") == \"<前のステップの結果>\" and state[\"step_results\"]:\n",
    "        tool_args[\"text\"] = str(state[\"step_results\"][-1])\n",
    "        print(f\"    引数更新: text='{tool_args['text']}'\")\n",
    "\n",
    "    result_content = \"\"\n",
    "    if tool_name in available_tools_for_plan:\n",
    "        selected_tool = available_tools_for_plan[tool_name]\n",
    "        try:\n",
    "            tool_output = selected_tool.invoke(tool_args)\n",
    "            result_content = str(tool_output)\n",
    "            print(f\"  ツール実行結果: {result_content}\")\n",
    "        except Exception as e:\n",
    "            result_content = f\"ツール実行エラー: {e}\"\n",
    "            print(f\"  エラー: {result_content}\")\n",
    "    elif tool_name == \"direct_answer\":\n",
    "        result_content = tool_args.get(\"text\", \"エラー: direct_answerにtextがありません\") # 解答例より\n",
    "        print(f\"  直接応答: {result_content}\")\n",
    "    else:\n",
    "        result_content = f\"不明なツール: {tool_name}\"\n",
    "        print(f\"  エラー: {result_content}\")\n",
    "        \n",
    "    updated_step_results = state[\"step_results\"] + [result_content]\n",
    "    simulated_tool_message = ToolMessage(content=result_content, tool_call_id=f\"plan_step_{idx}\", name=tool_name)\n",
    "    return {\n",
    "        \"step_results\": updated_step_results, \n",
    "        \"messages\": [simulated_tool_message],\n",
    "        \"current_step_index\": idx + 1  # 次のステップに進むためにインデックスを更新 (解答例より)\n",
    "    }\n",
    "\n",
    "def final_answer_node(state: PlanExecuteState):\n",
    "    print(\"\n",
    "[最終回答生成ノード]\")\n",
    "    if state[\"step_results\"]:\n",
    "        answer = f\"計画の最終結果: {state['step_results'][-1]}\"\n",
    "    else:\n",
    "        answer = \"計画が実行されなかったか、結果がありませんでした。\"\n",
    "    print(f\"  最終回答: {answer}\")\n",
    "    return {\"final_answer\": answer, \"messages\": [AIMessage(content=answer)]}\n",
    "\n",
    "# --- ルーター関数 ---\n",
    "def route_plan_execution(state: PlanExecuteState):\n",
    "    plan = state.get(\"plan\", [])\n",
    "    current_idx = state.get(\"current_step_index\", 0)\n",
    "    \n",
    "    if current_idx < len(plan):\n",
    "        print(f\"  -> ルーター: 次のステップ {current_idx + 1} を実行します。\")\n",
    "        return \"execute_next_step\"\n",
    "    else:\n",
    "        print(\"  -> ルーター: 全ステップ完了。最終回答へ。\")\n",
    "        return \"generate_final_answer\" # 最終回答生成へのキー (解答例より)\n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow_q6 = StateGraph(PlanExecuteState)\n",
    "workflow_q6.add_node(\"planner\", planner_node)\n",
    "workflow_q6.add_node(\"executor\", executor_node) # updated_executor_nodeは不要、executor_node内でidx更新 (解答例より)\n",
    "workflow_q6.add_node(\"final_answerer\", final_answer_node)\n",
    "\n",
    "workflow_q6.set_entry_point(\"planner\")\n",
    "workflow_q6.add_edge(\"planner\", \"executor\") \n",
    "\n",
    "workflow_q6.add_conditional_edges(\n",
    "    \"executor\",\n",
    "    route_plan_execution,\n",
    "    {\n",
    "        \"execute_next_step\": \"executor\", \n",
    "        \"generate_final_answer\": \"final_answerer\" # 解答例より\n",
    "    }\n",
    ")\n",
    "workflow_q6.add_edge(\"final_answerer\", END)\n",
    "\n",
    "graph_q6 = workflow_q6.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄006 - グラフ可視化\n",
    "____ IPython.display ____ Image, display # 解答例より display をインポート\n",
    "\n",
    "____:\n",
    "    display(Image(graph_q6.get_graph().draw_png()))\n",
    "____ Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄006 - グラフ実行\n",
    "request_q6_complex = \"数値10を二乗して、その結果の数値を文字列にして逆順にしてください。\"\n",
    "request_q6_simple_sq = \"数値7を二乗してください。\"\n",
    "request_q6_simple_rev = \"'apple'を逆順にしてください。\" # 解答例より\n",
    "request_q6_unknown = \"今日の天気は？\" # 解答例より\n",
    "\n",
    "____ req_idx, user_req ____ enumerate([request_q6_complex, request_q6_simple_sq, request_q6_simple_rev, request_q6_unknown]): # 解答例よりテストケース追加\n",
    "    print(f\"\n",
    "--- Plan-____-Executeテスト {req_idx + 1} (リクエスト: {user_req}) ---\")\n",
    "    initial_state_q6 = {\n",
    "        \"user_request\": user_req,\n",
    "        \"messages\": [____(content=user_req)],\n",
    "        \"plan\": ____, \"current_step_index\":0, \"step_results\": [], \"final_answer\": ____\n",
    "    }\n",
    "    thread_q6 = {\"configurable\": {\"thread_id\": f\"test-thread-q6-{req_idx}-{uuid4()[:4]}\"}}}\n",
    "    final_state_q6_val = ____ # 解答例より\n",
    "    ____ event in graph_q6.____(initial_state_q6, config=thread_q6, recursion_limit=10): # 解答例より recursion_limit 変更\n",
    "        print(f\"Event: {event}\")\n",
    "        print(\"----\");\n",
    "    \n",
    "    final_state_q6_full = graph_q6.____(config=thread_q6) # 解答例より\n",
    "    if final_state_q6_full: # 解答例より\n",
    "        final_state_q6_val = final_state_q6_full.values\n",
    "        print(f\"最終回答: {final_state_q6_val.get('final_answer')}\")\n",
    "        print(f\"全ステップ結果: {final_state_q6_val.get('step_results')}\")\n",
    "    else:\n",
    "        print(\"最終状態が取得できませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答006</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, List, Optional, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages \n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, ToolCall \n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "from uuid import uuid4\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# --- ツールの準備 (問題001, 002から再利用) ---\n",
    "# @tool def square_number(number: float) -> float: ...\n",
    "# @tool def reverse_string(text: str) -> str: ...\n",
    "available_tools_for_plan = {t.name: t for t in [square_number, reverse_string]}\n",
    "\n",
    "# --- 状態定義 ---\n",
    "class PlanExecuteState(TypedDict):\n",
    "    user_request: str\n",
    "    plan: Optional[List[dict]] \n",
    "    current_step_index: int\n",
    "    step_results: List[str] \n",
    "    final_answer: Optional[str]\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- ノード定義 ---\n",
    "def planner_node(state: PlanExecuteState):\n",
    "    print(f\"\n",
    "[プランナーノード] ユーザーリクエスト: {state['user_request']}\")\n",
    "    request = state['user_request'].lower()\n",
    "    generated_plan = []\n",
    "    import re\n",
    "    \n",
    "    # このダミープランナーは非常に単純化されています。\n",
    "    # 実際のLLMはもっと複雑なリクエストを解釈し、適切なツールと引数で計画を立てます。\n",
    "    if \"二乗\" in request and \"逆順\" in request:\n",
    "        num_match = re.search(r'(\\d+\\.?\\d*|\\.\\d+)', request)\n",
    "        num_for_square = float(num_match.group(1)) if num_match else 0.0\n",
    "        generated_plan.append({\"tool_name\": \"square_number\", \"args\": {\"number\": num_for_square}, \"step_description\": f\"{num_for_square}を二乗する\"})\n",
    "        generated_plan.append({\"tool_name\": \"reverse_string\", \"args\": {\"text\": \"<前のステップの結果>\"}, \"step_description\": \"前のステップの結果を逆順にする\"})\n",
    "    elif \"二乗\" in request:\n",
    "        num_match = re.search(r'(\\d+\\.?\\d*|\\.\\d+)', request)\n",
    "        num_for_square = float(num_match.group(1)) if num_match else 0.0\n",
    "        generated_plan.append({\"tool_name\": \"square_number\", \"args\": {\"number\": num_for_square}, \"step_description\": f\"{num_for_square}を二乗する\"})\n",
    "    elif \"逆順\" in request:\n",
    "        str_match = re.search(r\"['\"]([^'\"]*)['\"]\", request) # クォートされた文字列\n",
    "        text_to_rev = str_match.group(1) if str_match else request.replace(\"逆順に\",\"\").replace(\"逆順\",\"\").strip()\n",
    "        generated_plan.append({\"tool_name\": \"reverse_string\", \"args\": {\"text\": text_to_rev}, \"step_description\": f\"「{text_to_rev}」を逆順にする\"})\n",
    "    else:\n",
    "        generated_plan.append({\"tool_name\": \"direct_answer\", \"args\": {\"text\": \"計画を立てられませんでした。\"}, \"step_description\": \"直接応答\"})\n",
    "        \n",
    "    print(f\"  生成された計画: {generated_plan}\")\n",
    "    return {\"plan\": generated_plan, \"current_step_index\": 0, \"step_results\": [], \"messages\": [AIMessage(content=f\"計画を立てました: {json.dumps(generated_plan)}\")]}\n",
    "\n",
    "def executor_node(state: PlanExecuteState):\n",
    "    plan = state[\"plan\"]\n",
    "    idx = state[\"current_step_index\"]\n",
    "    \n",
    "    if not plan or idx >= len(plan):\n",
    "        print(\"[エグゼキューターノード] 実行すべきステップがありません。\")\n",
    "        return {\"current_step_index\": idx} \n",
    "    \n",
    "    current_step = plan[idx]\n",
    "    tool_name = current_step[\"tool_name\"]\n",
    "    tool_args = current_step[\"args\"].copy() \n",
    "    step_desc = current_step[\"step_description\"]\n",
    "    print(f\"\n",
    "[エグゼキューターノード] ステップ {idx + 1}/{len(plan)}: {step_desc} (ツール: {tool_name}, 引数: {tool_args})\")\n",
    "\n",
    "    if tool_args.get(\"text\") == \"<前のステップの結果>\" and state[\"step_results\"]:\n",
    "        tool_args[\"text\"] = str(state[\"step_results\"][-1])\n",
    "        print(f\"    引数更新: text='{tool_args['text']}'\")\n",
    "\n",
    "    result_content = \"\"\n",
    "    if tool_name in available_tools_for_plan:\n",
    "        selected_tool = available_tools_for_plan[tool_name]\n",
    "        try:\n",
    "            tool_output = selected_tool.invoke(tool_args)\n",
    "            result_content = str(tool_output)\n",
    "            print(f\"  ツール実行結果: {result_content}\")\n",
    "        except Exception as e:\n",
    "            result_content = f\"ツール実行エラー: {e}\"\n",
    "            print(f\"  エラー: {result_content}\")\n",
    "    elif tool_name == \"direct_answer\":\n",
    "        result_content = tool_args.get(\"text\", \"エラー: direct_answerにtextがありません\")\n",
    "        print(f\"  直接応答: {result_content}\")\n",
    "    else:\n",
    "        result_content = f\"不明なツール: {tool_name}\"\n",
    "        print(f\"  エラー: {result_content}\")\n",
    "        \n",
    "    updated_step_results = state[\"step_results\"] + [result_content]\n",
    "    simulated_tool_message = ToolMessage(content=result_content, tool_call_id=f\"plan_step_{idx}\", name=tool_name)\n",
    "    return {\n",
    "        \"step_results\": updated_step_results, \n",
    "        \"messages\": [simulated_tool_message],\n",
    "        \"current_step_index\": idx + 1  # 次のステップに進むためにインデックスを更新\n",
    "    }\n",
    "\n",
    "def final_answer_node(state: PlanExecuteState):\n",
    "    print(\"\n",
    "[最終回答生成ノード]\")\n",
    "    if state[\"step_results\"]:\n",
    "        answer = f\"計画の最終結果: {state['step_results'][-1]}\"\n",
    "    else:\n",
    "        answer = \"計画が実行されなかったか、結果がありませんでした。\"\n",
    "    print(f\"  最終回答: {answer}\")\n",
    "    return {\"final_answer\": answer, \"messages\": [AIMessage(content=answer)]}\n",
    "\n",
    "# --- ルーター関数 ---\n",
    "def route_plan_execution(state: PlanExecuteState):\n",
    "    plan = state.get(\"plan\", [])\n",
    "    current_idx = state.get(\"current_step_index\", 0)\n",
    "    \n",
    "    if current_idx < len(plan):\n",
    "        print(f\"  -> ルーター: 次のステップ {current_idx + 1} を実行します。\")\n",
    "        return \"execute_next_step\"\n",
    "    else:\n",
    "        print(\"  -> ルーター: 全ステップ完了。最終回答へ。\")\n",
    "        return \"generate_final_answer\" \n",
    "\n",
    "# --- グラフ構築 ---\n",
    "workflow_q6 = StateGraph(PlanExecuteState)\n",
    "workflow_q6.add_node(\"planner\", planner_node)\n",
    "workflow_q6.add_node(\"executor\", executor_node) # updated_executor_nodeは不要、executor_node内でidx更新\n",
    "workflow_q6.add_node(\"final_answerer\", final_answer_node)\n",
    "\n",
    "workflow_q6.set_entry_point(\"planner\")\n",
    "workflow_q6.add_edge(\"planner\", \"executor\") \n",
    "\n",
    "workflow_q6.add_conditional_edges(\n",
    "    \"executor\",\n",
    "    route_plan_execution,\n",
    "    {\n",
    "        \"execute_next_step\": \"executor\", \n",
    "        \"generate_final_answer\": \"final_answerer\" \n",
    "    }\n",
    ")\n",
    "workflow_q6.add_edge(\"final_answerer\", END)\n",
    "\n",
    "graph_q6 = workflow_q6.compile()\n",
    "try:\n",
    "    display(Image(graph_q6.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")\n",
    "\n",
    "# 実行テスト\n",
    "request_q6_complex = \"数値10を二乗して、その結果の数値を文字列にして逆順にしてください。\"\n",
    "request_q6_simple_sq = \"数値7を二乗してください。\"\n",
    "request_q6_simple_rev = \"'apple'を逆順にしてください。\"\n",
    "request_q6_unknown = \"今日の天気は？\"\n",
    "\n",
    "for req_idx, user_req in enumerate([request_q6_complex, request_q6_simple_sq, request_q6_simple_rev, request_q6_unknown]):\n",
    "    print(f\"\n",
    "--- Plan-and-Executeテスト {req_idx + 1} (リクエスト: {user_req}) ---\")\n",
    "    initial_state_q6 = {\n",
    "        \"user_request\": user_req,\n",
    "        \"messages\": [HumanMessage(content=user_req)],\n",
    "        \"plan\": None, \"current_step_index\":0, \"step_results\": [], \"final_answer\": None\n",
    "    }\n",
    "    thread_q6 = {\"configurable\": {\"thread_id\": f\"test-thread-q6-{req_idx}-{uuid4()[:4]}\"}}}\n",
    "    final_state_q6_val = None\n",
    "    for event in graph_q6.stream(initial_state_q6, config=thread_q6, recursion_limit=10):\n",
    "        print(f\"Event: {event}\")\n",
    "        # streamの最後のイベントが最終状態を含むとは限らない (ENDの場合など)\n",
    "        # get_stateで取得するのが確実\n",
    "        print(\"----\");\n",
    "    \n",
    "    final_state_q6_full = graph_q6.get_state(config=thread_q6)\n",
    "    if final_state_q6_full:\n",
    "        final_state_q6_val = final_state_q6_full.values\n",
    "        print(f\"最終回答: {final_state_q6_val.get('final_answer')}\")\n",
    "        print(f\"全ステップ結果: {final_state_q6_val.get('step_results')}\")\n",
    "    else:\n",
    "        print(\"最終状態が取得できませんでした。\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説006</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **Plan-and-Executeの概念:**\n",
    "    *   **プランナー (`planner_node`):** ユーザーの要求を受け取り、それを達成するための一連のステップ（計画）を立案します。この解答例では、LLMの代わりに単純なルールベースで計画（ツールのリストと引数）を生成していますが、実際の高度なエージェントではここがLLMの役割となり、より複雑な推論を行います。\n",
    "    *   **エグゼキューター (`executor_node`):** プランナーが立てた計画の各ステップを順番に実行します。各ステップは通常、特定のツール呼び出しに対応します。このノードは、現在のステップで指定されたツールを実行し、その結果を状態に記録します。\n",
    "*   **状態管理 (`PlanExecuteState`):**\n",
    "    *   `user_request`: 最初のユーザーの要求。\n",
    "    *   `plan`: プランナーが生成したステップのリスト。各ステップは辞書で、使用するツール名、引数、説明などを含みます。\n",
    "    *   `current_step_index`: 現在実行中の計画ステップのインデックス。\n",
    "    *   `step_results`: 各ステップの実行結果を格納するリスト。\n",
    "    *   `final_answer`: 全ステップ完了後に生成される最終的な回答。\n",
    "    *   `messages`: デバッグやログ、あるいはLLMへの入力として会話履歴を保持します。\n",
    "*   **グラフのフロー:**\n",
    "    1.  `planner_node` が計画を生成し、`current_step_index` を0に初期化します。\n",
    "    2.  `executor_node` が `plan[current_step_index]` を実行します。\n",
    "        *   ツール実行結果を `step_results` に追加します。\n",
    "        *   `current_step_index` をインクリメントします。\n",
    "    3.  `route_plan_execution` ルーターが `current_step_index` と `plan` の長さを比較します。\n",
    "        *   まだ実行すべきステップが残っていれば、再び `executor_node` に処理を戻します（ループ）。\n",
    "        *   全ステップが完了していれば、`final_answer_node` に処理を移します。\n",
    "    4.  `final_answer_node` が `step_results` をもとに最終回答を生成し、`END` で終了します。\n",
    "*   **簡易的な実装:** この解答例のプランナーとエグゼキューターは、実際のPlan-and-Executeエージェントと比較して非常に単純化されています。\n",
    "    *   プランナーは固定的なロジックで、LLMの動的な計画能力はありません。\n",
    "    *   エグゼキューターは、前のステップの結果を次のステップの引数として単純に渡す処理（`\"<前のステップの結果>\"`）しか行っていません。実際のシステムでは、より複雑な依存関係の解決やエラーハンドリングが必要です。\n",
    "    *   ツール自体も簡単なものです。\n",
    "    しかし、この基本的な骨組みは、より高度なPlan-and-Executeエージェントを構築する上での出発点となります。\n",
    "\n",
    "---</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}