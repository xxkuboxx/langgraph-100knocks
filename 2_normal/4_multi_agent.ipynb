{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4章: マルチエージェント・ワークフロー\n",
    "\n",
    "## 準備\n",
    "\n",
    "以下のセルを順番に実行して、演習に必要な環境をセットアップします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMプロバイダーの選択\n",
    "\n",
    "このセルでは、使用するLLMプロバイダーを選択します。\n",
    "`LLM_PROVIDER` 変数に、利用したいプロバイダー名を設定してください。\n",
    "選択可能なプロバイダー: `\"openai\"`, `\"azure\"`, `\"google\"` (Vertex AI), `\"google_genai\"` (Gemini API), `\"anthropic\"`, `\"bedrock\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMプロバイダーの選択 ===\n",
    "# 利用したいLLMプロバイダーを以下の変数で指定してください。\n",
    "# \"openai\", \"azure\", \"google\" (Vertex AI), \"google_genai\" (Gemini API), \"anthropic\", \"bedrock\" のいずれかを選択できます。\n",
    "LLM_PROVIDER = \"openai\"  # 例: OpenAI を利用する場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIキー/環境変数の設定\n",
    "\n",
    "以下のセルを実行する前に、選択したLLMプロバイダーに応じたAPIキーまたは環境変数を設定する必要があります。\n",
    "\n",
    "**手順:**\n",
    "1.  `.env.sample` ファイルをコピーして `.env` ファイルを作成します。\n",
    "2.  `.env` ファイルを開き、選択したLLMプロバイダーに対応するAPIキーや必要な情報を記述します。\n",
    "    *   **OpenAI:** `OPENAI_API_KEY`\n",
    "    *   **Azure OpenAI:** `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `OPENAI_API_VERSION`, `AZURE_OPENAI_DEPLOYMENT_NAME`\n",
    "    *   **Google (Vertex AI):** `GOOGLE_CLOUD_PROJECT_ID`, `GOOGLE_CLOUD_LOCATION` (Colab環境外で実行する場合、`GOOGLE_APPLICATION_CREDENTIALS` 環境変数の設定も必要になることがあります)\n",
    "    *   **Google (Gemini API):** `GOOGLE_API_KEY`\n",
    "    *   **Anthropic:** `ANTHROPIC_API_KEY`\n",
    "    *   **AWS Bedrock:** `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION_NAME` (IAMロールを使用する場合は、これらのキー設定は不要な場合がありますが、リージョン名は必須です)\n",
    "3.  ファイルを保存します。\n",
    "\n",
    "**Google Colab を使用している場合:**\n",
    "上記の `.env` ファイルを使用する代わりに、Colabのシークレットマネージャーに必要なキーを登録してください。\n",
    "例えば、OpenAIを使用する場合は `OPENAI_API_KEY` という名前でシークレットを登録します。\n",
    "Vertex AI を利用する場合は、Colab上での認証 (`google.colab.auth.authenticate_user()`) が実行されます。\n",
    "\n",
    "このセルは、設定された情報に基づいて環境変数をロードし、LLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === APIキー/環境変数の設定 ===\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envファイルから環境変数を読み込む (存在する場合)\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# --- OpenAI ---\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY and IS_COLAB:\n",
    "        OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"OpenAI APIキーが設定されていません。環境変数 OPENAI_API_KEY を設定するか、Colab環境の場合はシークレットに OPENAI_API_KEY を設定してください。\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# --- Azure OpenAI ---\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not AZURE_OPENAI_API_KEY: AZURE_OPENAI_API_KEY = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "        if not AZURE_OPENAI_ENDPOINT: AZURE_OPENAI_ENDPOINT = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        if not OPENAI_API_VERSION: OPENAI_API_VERSION = userdata.get(\"OPENAI_API_VERSION\") # 例: \"2023-07-01-preview\"\n",
    "        if not AZURE_OPENAI_DEPLOYMENT_NAME: AZURE_OPENAI_DEPLOYMENT_NAME = userdata.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "    if not AZURE_OPENAI_API_KEY: raise ValueError(\"Azure OpenAI APIキー (AZURE_OPENAI_API_KEY) が設定されていません。\")\n",
    "    if not AZURE_OPENAI_ENDPOINT: raise ValueError(\"Azure OpenAI エンドポイント (AZURE_OPENAI_ENDPOINT) が設定されていません。\")\n",
    "    if not OPENAI_API_VERSION: OPENAI_API_VERSION = \"2023-07-01-preview\" # デフォルトを設定することも可能\n",
    "    if not AZURE_OPENAI_DEPLOYMENT_NAME: raise ValueError(\"Azure OpenAI デプロイメント名 (AZURE_OPENAI_DEPLOYMENT_NAME) が設定されていません。\")\n",
    "\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "    os.environ[\"OPENAI_API_VERSION\"] = OPENAI_API_VERSION\n",
    "\n",
    "# --- Google Cloud Vertex AI (Gemini) ---\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT_ID\") # .env 用に修正\n",
    "    LOCATION = os.environ.get(\"GOOGLE_CLOUD_LOCATION\")\n",
    "\n",
    "    if IS_COLAB:\n",
    "        if not PROJECT_ID: PROJECT_ID = userdata.get(\"GOOGLE_CLOUD_PROJECT_ID\")\n",
    "        if not LOCATION: LOCATION = userdata.get(\"GOOGLE_CLOUD_LOCATION\") # 例: \"us-central1\"\n",
    "        from google.colab import auth as google_auth\n",
    "        google_auth.authenticate_user() # Vertex AI を使う場合は Colab での認証を推奨\n",
    "    else: # Colab外の場合、.envから読み込んだ値で環境変数を設定\n",
    "        if PROJECT_ID: os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID # Vertex AI SDKが参照する標準的な環境変数名\n",
    "        if LOCATION: os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION\n",
    "\n",
    "    if not PROJECT_ID: raise ValueError(\"Google Cloud Project ID が設定されていません。環境変数 GOOGLE_CLOUD_PROJECT_ID を設定するか、Colab環境の場合はシークレットに GOOGLE_CLOUD_PROJECT_ID を設定してください。\")\n",
    "    if not LOCATION: LOCATION = \"us-central1\" # デフォルトロケーション\n",
    "\n",
    "# --- Google Gemini API (langchain-google-genai) ---\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY and IS_COLAB:\n",
    "        GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"Google APIキーが設定されていません。環境変数 GOOGLE_API_KEY を設定するか、Colab環境の場合はシークレットに GOOGLE_API_KEY を設定してください。\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# --- Anthropic (Claude) ---\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY and IS_COLAB:\n",
    "        ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        raise ValueError(\"Anthropic APIキーが設定されていません。環境変数 ANTHROPIC_API_KEY を設定するか、Colab環境の場合はシークレットに ANTHROPIC_API_KEY を設定してください。\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "# --- Amazon Bedrock (Claude) ---\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    AWS_REGION_NAME = os.environ.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if IS_COLAB: \n",
    "        if not AWS_ACCESS_KEY_ID: AWS_ACCESS_KEY_ID = userdata.get(\"AWS_ACCESS_KEY_ID\")\n",
    "        if not AWS_SECRET_ACCESS_KEY: AWS_SECRET_ACCESS_KEY = userdata.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        if not AWS_REGION_NAME: AWS_REGION_NAME = userdata.get(\"AWS_REGION_NAME\")\n",
    "\n",
    "    if not AWS_REGION_NAME:\n",
    "         raise ValueError(\"AWSリージョン名 (AWS_REGION_NAME) が設定されていません。Bedrock利用にはリージョン指定が必要です。\")\n",
    "\n",
    "    # 環境変数に設定 (boto3がこれらを自動で読み込む)\n",
    "    if AWS_ACCESS_KEY_ID: os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "    if AWS_SECRET_ACCESS_KEY: os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION_NAME # boto3が参照する標準的なリージョン環境変数名\n",
    "    os.environ[\"AWS_REGION\"] = AWS_REGION_NAME # いくつかのライブラリはこちらを参照することもある\n",
    "\n",
    "print(f\"APIキー/環境変数の設定完了 (プロバイダー: {LLM_PROVIDER})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMクライアントの初期化\n",
    "\n",
    "このセルは、上で選択・設定したLLMプロバイダーに基づいて、対応するLLMクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLMクライアントの動的初期化 ===\n",
    "llm = None\n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "elif LLM_PROVIDER == \"azure\":\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), # 環境変数から取得\n",
    "        openai_api_version=os.environ.get(\"OPENAI_API_VERSION\"), # 環境変数から取得\n",
    "        temperature=0,\n",
    "    )\n",
    "elif LLM_PROVIDER == \"google\":\n",
    "    from langchain_google_vertexai import ChatVertexAI\n",
    "    # PROJECT_ID, LOCATION は前のセルで環境変数に設定済みか、Colabの場合は直接利用\n",
    "    llm = ChatVertexAI(model_name=\"gemini-2.0-flash\", temperature=0, project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"), location=os.environ.get(\"GOOGLE_CLOUD_LOCATION\"))\n",
    "elif LLM_PROVIDER == \"google_genai\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    from langchain_anthropic import ChatAnthropic\n",
    "    llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
    "elif LLM_PROVIDER == \"bedrock\":\n",
    "    from langchain_aws import ChatBedrock # langchain_community.chat_models から langchain_aws に変更の可能性あり\n",
    "    # AWS_REGION_NAME は前のセルで環境変数 AWS_DEFAULT_REGION に設定済み\n",
    "    llm = ChatBedrock( # BedrockChat ではなく ChatBedrock が一般的\n",
    "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        # region_name=os.environ.get(\"AWS_DEFAULT_REGION\"), # 通常、boto3が環境変数から自動で読み込む\n",
    "        model_kwargs={\"temperature\": 0},\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Unsupported LLM_PROVIDER: {LLM_PROVIDER}. \"\n",
    "        \"Please choose from 'openai', 'azure', 'google', 'google_genai', 'anthropic', or 'bedrock'.\"\n",
    "    )\n",
    "\n",
    "print(f\"LLM Provider: {LLM_PROVIDER}\")\n",
    "if llm:\n",
    "    print(f\"LLM Client Type: {type(llm)}\")\n",
    "    # モデル名取得の試行を汎用的に\n",
    "    model_attr = (\n",
    "                 getattr(llm, 'model', None) or\n",
    "                 getattr(llm, 'model_name', None) or\n",
    "                 getattr(llm, 'model_id', None) or\n",
    "                 (hasattr(llm, 'llm') and getattr(llm.llm, 'model', None)) # 一部のLLMクライアントのネスト構造に対応\n",
    "    )\n",
    "    if hasattr(llm, 'azure_deployment') and not model_attr: # Azure特有の属性\n",
    "        model_attr = llm.azure_deployment\n",
    "        \n",
    "    if model_attr:\n",
    "        print(f\"LLM Model: {model_attr}\")\n",
    "    else:\n",
    "        print(\"LLM Model: (Could not determine model name from client attributes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題001: 基本的な2エージェント会話（リサーチャーとライター）\n",
    "\n",
    "複数のLLMエージェントが協調してタスクを解決する第一歩として、2つの異なる役割を持つエージェント（リサーチャーとライター）を作成し、それらが交互に会話（状態を更新）しながら一つのタスク（例: 特定のトピックに関する短い記事の作成）を進めるグラフを構築します。\n",
    "\n",
    "*   **学習内容:**\n",
    "    *   異なる役割（プロンプトや利用ツールが異なる）を持つ複数のエージェント関数（ノード）を定義する方法。\n",
    "    *   状態（State）に「次に実行すべきエージェント」を示すキー（例: `next_agent`）を持たせ、それに基づいて処理をルーティングする方法。\n",
    "    *   エージェント間で情報を引き継ぎながら（例: リサーチャーの調査結果をライターが利用）、タスクを段階的に進める基本的な流れ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001 - グラフ構築\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import ____, ____\n",
    "from langgraph.graph.message import ____\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from uuid import uuid4 # 解答例より\n",
    "\n",
    "# --- 1. 状態定義 ---\n",
    "class ____(____):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    next_agent: str # 次に実行するエージェント名 (\"Researcher\", \"Writer\", \"FINISH\")\n",
    "    topic: str\n",
    "    research_findings: Optional[str]\n",
    "    draft_article: Optional[str]\n",
    "\n",
    "# --- 2. エージェント（ノード）定義 ---\n",
    "def ____(state: TwoAgentConversationState) -> dict: # 戻り値の型アノテーションをdictに変更 (解答例より)\n",
    "    print(f\"\n",
    "[リサーチャーエージェント] トピック「{state['topic']}」について調査します。\")\n",
    "    findings = f\"「{state['topic']}」に関する詳細な調査結果: 主要なポイントはA, B, Cであり、背景にはDが存在します。今後の展望としてはEが考えられます。\" # 解答例よりメッセージ変更\n",
    "    if LLM_PROVIDER != \"fake\" and llm: # 解答例より llm の存在チェック追加\n",
    "        prompt = f\"あなたは優秀なリサーチャーです。トピック「{state['topic']}」について、架空の調査結果を詳細に（3つの主要ポイント、背景、今後の展望を含めて）生成してください。\" # 解答例よりプロンプト変更\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        findings = response.content\n",
    "    \n",
    "    print(f\"  調査結果: {findings[:100]}...\") # 解答例より出力短縮\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"調査結果報告: {findings}\", name=\"Researcher\")], # 解答例よりメッセージ変更\n",
    "        \"research_findings\": findings,\n",
    "        \"next_agent\": \"Writer\" # 次はライターエージェントへ (解答例より)\n",
    "    }\n",
    "\n",
    "def ____(state: TwoAgentConversationState) -> dict: # 戻り値の型アノテーションをdictに変更 (解答例より)\n",
    "    print(f\"\n",
    "[ライターエージェント] 調査結果を元に記事を作成します。\")\n",
    "    findings = state.get(\"research_findings\", \"調査結果が提供されていません。\") # 解答例より\n",
    "    article = f\"タイトル案: {state['topic']}の深掘り\\n\\n{findings}\\n\\n本稿は上記の調査に基づき構成されました。\" # 解答例よりメッセージ変更\n",
    "    if LLM_PROVIDER != \"fake\" and llm: # 解答例より llm の存在チェック追加\n",
    "        prompt = f\"あなたは熟練のライターです。以下の調査結果に基づいて、読者の関心を引くような、約200字程度の解説記事を作成してください。タイトルも提案してください。\\n調査結果:\\n{findings}\" # 解答例よりプロンプト変更\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        article = response.content\n",
    "\n",
    "    print(f\"  作成された記事: {article[:100]}...\") # 解答例より出力短縮\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"記事ドラフト:\\n{article}\", name=\"Writer\")], # 解答例よりメッセージ変更\n",
    "        \"draft_article\": article,\n",
    "        \"next_agent\": \"FINISH\" # これで終了 (解答例より)\n",
    "    }\n",
    "\n",
    "# --- 3. ルーター関数の定義 ---\n",
    "def ____(state: TwoAgentConversationState) -> str:\n",
    "    next_node_name = state.get(\"next_agent\") # 解答例より変数名変更\n",
    "    print(f\"  -> ルーター: 次のエージェントは「{next_node_name}」です。\") # 解答例より変数名変更\n",
    "    if next_node_name == \"Writer\":\n",
    "        return \"writer_node\" # ノード名と合わせる (解答例より)\n",
    "    elif next_node_name == \"Researcher\": \n",
    "        return \"researcher_node\" # 解答例より\n",
    "    else: \n",
    "        return END\n",
    "\n",
    "# --- 4. グラフの構築 ---\n",
    "workflow_q1_ch4 = StateGraph(TwoAgentConversationState)\n",
    "\n",
    "workflow_q1_ch4.____(\"researcher_node\", researcher_agent) # 解答例よりノード名変更\n",
    "workflow_q1_ch4.add_node(\"writer_node\", writer_agent) # 解答例よりノード名変更\n",
    "\n",
    "workflow_q1_ch4.____(\n",
    "    route_to_next_agent, \n",
    "    {\n",
    "        \"researcher_node\": \"researcher_node\", # 解答例より\n",
    "        \"writer_node\": \"writer_node\", # 解答例より\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow_q1_ch4.add_conditional_edges(\"researcher_node\", route_to_next_agent, {\"writer_node\": \"writer_node\", END: END}) # 解答例よりノード名変更\n",
    "workflow_q1_ch4.add_conditional_edges(\"writer_node\", route_to_next_agent, {END: END}) # 解答例よりノード名変更\n",
    "\n",
    "graph_q1_ch4 = workflow_q1_ch4.____()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001 - グラフ可視化\n",
    "from IPython.display import Image, display # 解答例より\n",
    "\n",
    "try:\n",
    "    display(Image(graph_q1_ch4.____().____()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄001 - グラフ実行\n",
    "topic_q1_ch4 = \"LangGraphを用いた高度な自律エージェントの設計パターン\" # 解答例よりトピック変更\n",
    "thread_id_q1 = f\"thread-2agents-{uuid4()[:4]}\" # 解答例より\n",
    "config_q1 = {\"____\": {\"____\": thread_id_q1}} # 解答例より\n",
    "\n",
    "initial_state_q1_ch4 = {\n",
    "    \"messages\": [____(content=f\"トピック「{topic_q1_ch4}」について記事を作成してください。まず調査からお願いします。初期状態です。\")], # 解答例よりメッセージ変更\n",
    "    \"____\": \"Researcher\", \n",
    "    \"topic\": topic_q1_ch4,\n",
    "    \"____\": None,\n",
    "    \"____\": None\n",
    "}\n",
    "\n",
    "print(f\"--- 2エージェント会話テスト (トピック: {topic_q1_ch4}) ---\")\n",
    "final_state_q1_ch4_values = None # 解答例より\n",
    "for event_chunk in graph_q1_ch4.____(initial_state_q1_ch4, ____=config_q1, recursion_limit=5): # 解答例より config 追加, recursion_limitは元の解答から\n",
    "    print(f\"Event Chunk: {event_chunk}\") # 解答例より\n",
    "    for key, value_dict in event_chunk.items(): # 解答例より\n",
    "        if key == END: # 解答例より\n",
    "            final_state_q1_ch4_values = value_dict # 解答例より\n",
    "    print(\"----\");\n",
    "\n",
    "if not final_state_q1_ch4_values: # 解答例より\n",
    "    print(\"ENDイベントから最終状態を取得できませんでした。get_stateを試みます。\") # 解答例より\n",
    "    current_state_obj = graph_q1_ch4.____(config=config_q1) # 解答例より\n",
    "    if current_state_obj: # 解答例より\n",
    "        final_state_q1_ch4_values = current_state_obj.values # 解答例より\n",
    "\n",
    "if final_state_q1_ch4_values: # 解答例より\n",
    "    print(\"\n",
    "--- 最終結果 ---\")\n",
    "    print(f\"トピック: {final_state_q1_ch4_values.get('topic')}\")\n",
    "    print(f\"調査結果: {final_state_q1_ch4_values.get('research_findings')}\")\n",
    "    print(f\"最終記事: {final_state_q1_ch4_values.get('draft_article')}\")\n",
    "else:\n",
    "    print(\"最終状態が取得できませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答001</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "from uuid import uuid4\n",
    "\n",
    "# --- 1. 状態定義 ---\n",
    "class TwoAgentConversationState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    next_agent: str \n",
    "    topic: str\n",
    "    research_findings: Optional[str]\n",
    "    draft_article: Optional[str]\n",
    "\n",
    "# --- 2. エージェント（ノード）定義 ---\n",
    "def researcher_agent(state: TwoAgentConversationState) -> dict:\n",
    "    print(f\"\n",
    "[リサーチャーエージェント] トピック「{state['topic']}」について調査します。\")\n",
    "    findings = f\"「{state['topic']}」に関する詳細な調査結果: 主要なポイントはA, B, Cであり、背景にはDが存在します。今後の展望としてはEが考えられます。\"\n",
    "    if LLM_PROVIDER != \"fake\" and llm:\n",
    "        prompt = f\"あなたは優秀なリサーチャーです。トピック「{state['topic']}」について、架空の調査結果を詳細に（3つの主要ポイント、背景、今後の展望を含めて）生成してください。\"\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        findings = response.content\n",
    "    \n",
    "    print(f\"  調査結果: {findings[:100]}...\")\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"調査結果報告: {findings}\", name=\"Researcher\")],\n",
    "        \"research_findings\": findings,\n",
    "        \"next_agent\": \"Writer\" \n",
    "    }\n",
    "\n",
    "def writer_agent(state: TwoAgentConversationState) -> dict:\n",
    "    print(f\"\n",
    "[ライターエージェント] 調査結果を元に記事を作成します。\")\n",
    "    findings = state.get(\"research_findings\", \"調査結果が提供されていません。\")\n",
    "    article = f\"タイトル案: {state['topic']}の深掘り\\n\\n{findings}\\n\\n本稿は上記の調査に基づき構成されました。\"\n",
    "    if LLM_PROVIDER != \"fake\" and llm:\n",
    "        prompt = f\"あなたは熟練のライターです。以下の調査結果に基づいて、読者の関心を引くような、約200字程度の解説記事を作成してください。タイトルも提案してください。\\n調査結果:\\n{findings}\"\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        article = response.content\n",
    "\n",
    "    print(f\"  作成された記事: {article[:100]}...\")\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"記事ドラフト:\\n{article}\", name=\"Writer\")],\n",
    "        \"draft_article\": article,\n",
    "        \"next_agent\": \"FINISH\" \n",
    "    }\n",
    "\n",
    "# --- 3. ルーター関数の定義 ---\n",
    "def route_to_next_agent(state: TwoAgentConversationState) -> str:\n",
    "    next_node_name = state.get(\"next_agent\")\n",
    "    print(f\"  -> ルーター: 次のエージェントは「{next_node_name}」です。\")\n",
    "    if next_node_name == \"Writer\":\n",
    "        return \"writer_node\" # ノード名と合わせる\n",
    "    elif next_node_name == \"Researcher\":\n",
    "        return \"researcher_node\"\n",
    "    else: # FINISH または不明な場合は終了\n",
    "        return END\n",
    "\n",
    "# --- 4. グラフの構築 ---\n",
    "workflow_q1_ch4 = StateGraph(TwoAgentConversationState)\n",
    "\n",
    "workflow_q1_ch4.add_node(\"researcher_node\", researcher_agent)\n",
    "workflow_q1_ch4.add_node(\"writer_node\", writer_agent)\n",
    "\n",
    "workflow_q1_ch4.set_conditional_entry_point(\n",
    "    route_to_next_agent,\n",
    "    {\n",
    "        \"researcher_node\": \"researcher_node\",\n",
    "        \"writer_node\": \"writer_node\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow_q1_ch4.add_conditional_edges(\"researcher_node\", route_to_next_agent, {\"writer_node\": \"writer_node\", END: END})\n",
    "workflow_q1_ch4.add_conditional_edges(\"writer_node\", route_to_next_agent, {END: END})\n",
    "\n",
    "graph_q1_ch4 = workflow_q1_ch4.compile()\n",
    "try:\n",
    "    display(Image(graph_q1_ch4.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")\n",
    "\n",
    "# --- 5. グラフの実行 ---\n",
    "topic_q1_ch4 = \"LangGraphを用いた高度な自律エージェントの設計パターン\"\n",
    "thread_id_q1 = f\"thread-2agents-{uuid4()[:4]}\"\n",
    "config_q1 = {\"configurable\": {\"thread_id\": thread_id_q1}}\n",
    "\n",
    "initial_state_q1_ch4 = {\n",
    "    \"messages\": [HumanMessage(content=f\"トピック「{topic_q1_ch4}」について記事を作成してください。まず調査からお願いします。初期状態です。\")],\n",
    "    \"next_agent\": \"Researcher\",\n",
    "    \"topic\": topic_q1_ch4,\n",
    "    \"research_findings\": None,\n",
    "    \"draft_article\": None\n",
    "}\n",
    "\n",
    "print(f\"--- 2エージェント会話テスト (トピック: {topic_q1_ch4}) ---\")\n",
    "final_state_q1_ch4_values = None\n",
    "for event_chunk in graph_q1_ch4.stream(initial_state_q1_ch4, config=config_q1, recursion_limit=5):\n",
    "    # streamから返る各chunkは {'node_name': {'state_key': value, ...}} の形式\n",
    "    # 最後のイベントがENDの場合、そのキーに対応する値が最終状態全体になる\n",
    "    print(f\"Event Chunk: {event_chunk}\")\n",
    "    for key, value_dict in event_chunk.items():\n",
    "        if key == END:\n",
    "            final_state_q1_ch4_values = value_dict\n",
    "    print(\"----\");\n",
    "\n",
    "if not final_state_q1_ch4_values: # ENDイベントでキャッチできなかった場合 (例: recursion_limit)\n",
    "    print(\"ENDイベントから最終状態を取得できませんでした。get_stateを試みます。\")\n",
    "    current_state_obj = graph_q1_ch4.get_state(config=config_q1)\n",
    "    if current_state_obj:\n",
    "        final_state_q1_ch4_values = current_state_obj.values\n",
    "\n",
    "if final_state_q1_ch4_values:\n",
    "    print(\"\n",
    "--- 最終結果 ---\")\n",
    "    print(f\"トピック: {final_state_q1_ch4_values.get('topic')}\")\n",
    "    print(f\"調査結果: {final_state_q1_ch4_values.get('research_findings')}\")\n",
    "    print(f\"最終記事: {final_state_q1_ch4_values.get('draft_article')}\")\n",
    "    # print(f\"最終メッセージ履歴: {final_state_q1_ch4_values.get('messages')}\") # 必要なら表示\n",
    "else:\n",
    "    print(\"最終状態が取得できませんでした。\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説001</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **状態によるエージェントの切り替え:** `TwoAgentConversationState` に `next_agent` というキーを設け、このキーの値を \"Researcher\", \"Writer\", \"FINISH\" のように変更することで、次にどのアクション（ノード）を実行するかを制御します。\n",
    "*   **エージェントノードの役割分担:**\n",
    "    *   `researcher_agent`: 指定されたトピックについて（ダミーの）調査を行い、結果を `research_findings` に保存し、次に `Writer` を指定します。\n",
    "    *   `writer_agent`: `research_findings` を使って（ダミーの）記事を作成し、`draft_article` に保存し、次に `FINISH` を指定します。\n",
    "    *   実際のアプリケーションでは、これらのノード内部でLLMを呼び出し、より高度な調査や記事作成を行います。この解答例では、`LLM_PROVIDER != \"fake\"` の場合にLLMを呼び出すようにしています。\n",
    "*   **ルーター (`route_to_next_agent`):** `next_agent` の値を見て、次に実行すべきノード名（`\"researcher_node\"` や `\"writer_node\"`）または特別な終了マーカー `END` を返します。これにより、グラフの実行フローが動的に決定されます。\n",
    "*   **`set_conditional_entry_point`:** グラフの開始点を固定せず、初期状態の `next_agent` の値に基づいて最初のノードを決定するために使用しています。これにより、例えば途中から処理を再開するようなシナリオにも対応しやすくなります（ただし、この問題では初期状態は常にリサーチャーから開始）。\n",
    "*   **情報の引き継ぎ:** リサーチャーが生成した `research_findings` は状態を通じてライターに引き継がれ、ライターはそれを利用して記事を作成します。このように、状態オブジェクトがエージェント間の情報共有媒体として機能します。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題002: エージェントスウォームの基本 - スーパーバイザーによるタスク割り振り\n",
    "\n",
    "より複雑なタスクでは、複数の専門エージェントチーム（スウォーム）を統括するスーパーバイザー（監督者）エージェントを導入するアプローチが有効です。スーパーバイザーは、全体のタスクを分析し、適切なサブタスクを特定のエージェント（またはエージェントチーム）に割り振ります。この問題では、スーパーバイザーがユーザーの要求に応じて、「リサーチャー」または「ライター」のいずれか一方のエージェントを選択して処理を委任する、基本的なタスク割り振りグラフを構築します。\n",
    "\n",
    "*   **学習内容:**\n",
    "    *   スーパーバイザー役のLLMノードが、ユーザー入力に基づいて次に実行すべきエージェント（または処理）を決定する方法。\n",
    "    *   状態に「選択されたエージェント」や「タスク指示」を格納し、それに基づいて条件付きエッジで処理を分岐させる方法。\n",
    "    *   委任されたエージェントが処理を実行し、その結果をスーパーバイザー（または次のステップ）に返す流れ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002 - グラフ構築\n",
    "from langchain_core.messages import SystemMessage\n",
    "from typing import TypedDict, Annotated, List, Optional, ____ # Literalを追加 (解答例より)\n",
    "from langgraph.graph import ____, END # (既にインポート済みだが明示)\n",
    "from langgraph.graph.message import add_messages # (既にインポート済みだが明示)\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage # (既にインポート済みだが明示)\n",
    "from langchain_community.tools.tavily_search import ____ # search_tool用 (解答例より)\n",
    "from langchain_core.tools import tool # dummy_search_tool用 (解答例より)\n",
    "from uuid import uuid4 # 解答例より\n",
    "import re # 解答例より\n",
    "\n",
    "# --- 1. 状態定義 ---\n",
    "class ____(____):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    user_request: str\n",
    "    next_action: Optional[Literal[\"DelegateToResearcher\", \"DelegateToWriter\", \"RespondToUser\", \"FINISH\"]] # 解答例よりOptional追加\n",
    "    research_task_description: Optional[str]\n",
    "    research_result: Optional[str]\n",
    "    writing_task_description: Optional[str]\n",
    "    final_draft: Optional[str]\n",
    "    supervisor_response: Optional[str] \n",
    "\n",
    "# --- 2. エージェント（ノード）定義 ---\n",
    "def ____(state: SupervisorAgentState) -> dict:\n",
    "    print(f\"\n",
    "[スーパーバイザーエージェント]\")\n",
    "    user_req = state.get(\"user_request\") or (state[\"messages\"][-1].content if state.get(\"messages\") else \"\") # 解答例より修正\n",
    "    print(f\"  ユーザーリクエスト: {user_req}\")\n",
    "    \n",
    "    next_act: Optional[Literal[\"DelegateToResearcher\", \"DelegateToWriter\", \"RespondToUser\", \"FINISH\"]] = None # 解答例よりOptional\n",
    "    research_desc, write_desc, sup_resp = None, None, None\n",
    "\n",
    "    if LLM_PROVIDER != \"fake\" and llm:\n",
    "        system_prompt = (\n",
    "            \"あなたはタスクを分析し、リサーチャー、ライター、または自分自身（ユーザーへの応答）のいずれに \"\n",
    "            \"処理を割り振るかを決定するスーパーバイザーです。\"\n",
    "            \"可能なアクションは、「DelegateToResearcher: [調査指示]」、「DelegateToWriter: [執筆指示]」、\"\n",
    "            \"「RespondToUser: [ユーザーへの直接応答内容]」、「FINISH」のいずれかの形式で答えてください。\"\n",
    "            \"指示や応答内容には必ず具体的な内容を入れてください。\"\n",
    "            \"例1: ユーザーが「明日の東京の天気を調べて」と依頼したら、「DelegateToResearcher: 明日の東京の天気調査」と応答します。\"\n",
    "            \"例2: ユーザーが「AI倫理に関する記事を書いて」と依頼したら、「DelegateToWriter: AI倫理に関する記事執筆」と応答します。\"\n",
    "            \"例3: ユーザーが「こんにちは」と挨拶したら、「RespondToUser: こんにちは！ご用件は何でしょう？」と応答します。\"\n",
    "        )\n",
    "        response = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=user_req)])\n",
    "        decision_text = response.content.strip()\n",
    "        print(f\"  LLMによる判断: {decision_text}\")\n",
    "        if decision_text.startswith(\"DelegateToResearcher:\"):\n",
    "            next_act = \"DelegateToResearcher\"\n",
    "            research_desc = decision_text.replace(\"DelegateToResearcher:\", \"\").strip() or user_req # 解答例より\n",
    "        elif decision_text.startswith(\"DelegateToWriter:\"):\n",
    "            next_act = \"DelegateToWriter\"\n",
    "            write_desc = decision_text.replace(\"DelegateToWriter:\", \"\").strip() or user_req # 解答例より\n",
    "        elif decision_text.startswith(\"RespondToUser:\"):\n",
    "            next_act = \"RespondToUser\"\n",
    "            sup_resp = decision_text.replace(\"RespondToUser:\", \"\").strip() or \"ごめんなさい、よくわかりませんでした。\" # 解答例より\n",
    "        elif decision_text == \"FINISH\":\n",
    "            next_act = \"FINISH\"\n",
    "        else: \n",
    "            print(\"  WARN: LLMが期待した形式で判断を返しませんでした。フォールバックロジックを使用します。\") # 解答例より\n",
    "            if any(kw in user_req.lower() for kw in [\"調べ\", \"調査\", \"research\", \"find out\"]): # 解答例より\n",
    "                next_act = \"DelegateToResearcher\"; research_desc = user_req\n",
    "            elif any(kw in user_req.lower() for kw in [\"書い\", \"記事\", \"作成\", \"write\", \"article\"]): # 解答例より\n",
    "                next_act = \"DelegateToWriter\"; write_desc = user_req\n",
    "            else:\n",
    "                next_act = \"RespondToUser\"; sup_resp = \"ご要望を理解できませんでした。具体的な指示をいただけますか？\" # 解答例より\n",
    "    else: \n",
    "        if any(kw in user_req.lower() for kw in [\"調べ\", \"調査\", \"research\", \"find out\"]): # 解答例より\n",
    "            next_act = \"DelegateToResearcher\"; research_desc = user_req\n",
    "        elif any(kw in user_req.lower() for kw in [\"書い\", \"記事\", \"作成\", \"write\", \"article\"]): # 解答例より\n",
    "            next_act = \"DelegateToWriter\"; write_desc = user_req\n",
    "        else:\n",
    "            next_act = \"RespondToUser\"; sup_resp = \"FakeSupervisor: ご要望を理解できませんでした。\"\n",
    "            \n",
    "    print(f\"  決定された次のアクション: {next_act}\")\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"スーパーバイザー判断: {next_act}. 指示: {research_desc or write_desc or sup_resp}\", name=\"Supervisor\")], # 解答例より\n",
    "        \"next_action\": next_act,\n",
    "        \"research_task_description\": research_desc,\n",
    "        \"writing_task_description\": write_desc,\n",
    "        \"supervisor_response\": sup_resp\n",
    "    }\n",
    "\n",
    "def ____(state: SupervisorAgentState) -> dict:\n",
    "    task = state.get(\"research_task_description\", \"指定なし\")\n",
    "    print(f\"\n",
    "[簡易リサーチャーノード] タスク: {task}\")\n",
    "    result = f\"「{task}」に関するダミー調査結果です。非常に詳細な情報が得られました。\"\n",
    "    if search_tool and task != \"指定なし\" and TAVILY_API_KEY and search_tool.name != \"dummy_search_tool\": # 解答例より条件修正\n",
    "        try: result = search_tool.invoke(task)\n",
    "        except Exception as e: print(f\"  リサーチャーツールエラー: {e}\") # 解答例より\n",
    "    elif search_tool and search_tool.name == \"dummy_search_tool\": result = search_tool.invoke(task) # 解答例より\n",
    "    print(f\"  調査結果: {str(result)[:100]}...\") # 解答例より str() で囲む\n",
    "    return {\"messages\": [AIMessage(content=str(result), name=\"Researcher\") ], \"research_result\": str(result), \"next_action\": \"FINISH\"} \n",
    "\n",
    "def ____(state: SupervisorAgentState) -> dict:\n",
    "    task_desc = state.get(\"writing_task_description\", \"トピック指定なし\")\n",
    "    research_data = state.get(\"research_result\") # 解答例より\n",
    "    input_for_writing = f\"執筆指示: {task_desc}\\n\" # 解答例より\n",
    "    if research_data: input_for_writing += f\"利用可能な調査結果: {research_data[:150]}...\\n\" # 解答例より\n",
    "    print(f\"\n",
    "[簡易ライターノード] {input_for_writing}\") # 解答例より\n",
    "    draft = f\"「{task_desc}」についての素晴らしい記事が完成しました。内容は次の通り...\"\n",
    "    if LLM_PROVIDER != \"fake\" and llm: # 解答例より\n",
    "        response = llm.invoke(f\"以下の指示と情報に基づいて記事を作成してください。\\n{input_for_writing}\") # 解答例より\n",
    "        draft = response.content # 解答例より\n",
    "    print(f\"  作成ドラフト: {draft[:100]}...\")\n",
    "    return {\"messages\": [AIMessage(content=draft, name=\"Writer\")], \"final_draft\": draft, \"next_action\": \"FINISH\"}\n",
    "\n",
    "def user_responder_node(state: SupervisorAgentState) -> dict:\n",
    "    response = state.get(\"supervisor_response\", \"エラーが発生したか、応答がありませんでした。\")\n",
    "    print(f\"\n",
    "[ユーザー応答ノード] スーパーバイザーからの応答: {response}\")\n",
    "    return {\"messages\": [AIMessage(content=response, name=\"SupervisorDirectResponse\")], \"next_action\": \"FINISH\"}\n",
    "\n",
    "# --- 3. ルーター関数の定義 ---\n",
    "def ____(state: SupervisorAgentState) -> str:\n",
    "    decision = state.get(\"next_action\")\n",
    "    print(f\"  -> ルーター(SupervisorDecision): 次のアクションは「{decision}」です。\")\n",
    "    if decision == \"DelegateToResearcher\": return \"researcher\"\n",
    "    if decision == \"DelegateToWriter\": return \"writer\"\n",
    "    if decision == \"RespondToUser\": return \"user_responder\"\n",
    "    return END # 終了 (解答例より)\n",
    "\n",
    "# --- 4. グラフの構築 ---\n",
    "workflow_q2_ch4 = StateGraph(SupervisorAgentState)\n",
    "workflow_q2_ch4.____(\"supervisor\", supervisor_agent)\n",
    "workflow_q2_ch4.add_node(\"researcher\", simple_researcher_node)\n",
    "workflow_q2_ch4.add_node(\"writer\", simple_writer_node)\n",
    "workflow_q2_ch4.add_node(\"user_responder\", user_responder_node)\n",
    "\n",
    "workflow_q2_ch4.____(\"supervisor\")\n",
    "\n",
    "workflow_q2_ch4.____(\n",
    "    \"supervisor\", route_by_supervisor_decision,\n",
    "    {\n",
    "        \"researcher\": \"researcher\", \"writer\": \"writer\", \n",
    "        \"user_responder\": \"user_responder\", END: END\n",
    "    }\n",
    ")\n",
    "workflow_q2_ch4.add_conditional_edges(\"researcher\", route_by_supervisor_decision, {END: END}) \n",
    "workflow_q2_ch4.add_conditional_edges(\"writer\", route_by_supervisor_decision, {END: END})\n",
    "workflow_q2_ch4.add_conditional_edges(\"user_responder\", route_by_supervisor_decision, {END: END})\n",
    "\n",
    "graph_q2_ch4 = workflow_q2_ch4.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002 - グラフ可視化\n",
    "from IPython.display import Image, display # 解答例より\n",
    "\n",
    "try:\n",
    "    display(Image(graph_q2_ch4.____().____()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄002 - グラフ実行\n",
    "test_requests_q2 = [\n",
    "    \"LangGraphの分散型エージェントアーキテクチャについて調査し、その利点をまとめてください。\", # 解答例より\n",
    "    \"AIが創造性を発揮する事例について、感動的なブログ記事を執筆してください。\", # 解答例より\n",
    "    \"今日の天気は良いですね！\" # 解答例より\n",
    "]\n",
    "\n",
    "for i, req in enumerate(test_requests_q2):\n",
    "    print(f\"\n",
    "--- スーパーバイザーテスト {i+1} (リクエスト: {req}) ---\")\n",
    "    initial_state_q2_ch4 = {\n",
    "        \"messages\": [____(content=req)], \"____\": req, # 解答例より\n",
    "        \"____\": None, \"____\": None, \"research_result\": None,\n",
    "        \"____\": None, \"final_draft\": None, \"____\": None\n",
    "    }\n",
    "    thread_q2 = {\"____\": {\"thread_id\": f\"supervisor-test-{i}-{uuid4()[:4]}\"}}}\n",
    "    final_run_state_q2 = None # 解答例より\n",
    "    for event in graph_q2_ch4.____(initial_state_q2_ch4, config=thread_q2, ____=5): # 解答例より config 追加, recursion_limit は元の解答から\n",
    "        print(f\"Event: {event}\")\n",
    "        if END in event: final_run_state_q2 = event[END] # 解答例より\n",
    "        print(\"----\");\n",
    "    \n",
    "    if not final_run_state_q2: final_run_state_q2 = graph_q2_ch4.get_state(thread_q2).values # 解答例より\n",
    "\n",
    "    print(\"\n",
    "  最終結果:\")\n",
    "    if final_run_state_q2.get(\"research_result\"): # 解答例より\n",
    "        print(f\"    調査結果: {final_run_state_q2['research_result'][:100]}...\")\n",
    "    if final_run_state_q2.get(\"final_draft\"): # 解答例より\n",
    "        print(f\"    最終ドラフト: {final_run_state_q2['final_draft'][:100]}...\")\n",
    "    if final_run_state_q2.get(\"supervisor_response\") and not final_run_state_q2.get(\"research_result\") and not final_run_state_q2.get(\"final_draft\"): # 解答例より\n",
    "        print(f\"    スーパーバイザーからの直接応答: {final_run_state_q2['supervisor_response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答002</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, Annotated, List, Optional, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults # search_tool用\n",
    "from langchain_core.tools import tool # dummy_search_tool用\n",
    "from IPython.display import Image, display\n",
    "from uuid import uuid4\n",
    "import re\n",
    "\n",
    "# --- 1. 状態定義 ---\n",
    "class SupervisorAgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    user_request: str\n",
    "    next_action: Optional[Literal[\"DelegateToResearcher\", \"DelegateToWriter\", \"RespondToUser\", \"FINISH\"]]\n",
    "    research_task_description: Optional[str]\n",
    "    research_result: Optional[str]\n",
    "    writing_task_description: Optional[str]\n",
    "    final_draft: Optional[str]\n",
    "    supervisor_response: Optional[str] \n",
    "\n",
    "# --- 2. エージェント（ノード）定義 ---\n",
    "def supervisor_agent(state: SupervisorAgentState) -> dict:\n",
    "    print(f\"\n",
    "[スーパーバイザーエージェント]\")\n",
    "    user_req = state.get(\"user_request\") or (state[\"messages\"][-1].content if state.get(\"messages\") else \"\")\n",
    "    print(f\"  ユーザーリクエスト: {user_req}\")\n",
    "    \n",
    "    next_act: Optional[Literal[\"DelegateToResearcher\", \"DelegateToWriter\", \"RespondToUser\", \"FINISH\"]] = None\n",
    "    research_desc, write_desc, sup_resp = None, None, None\n",
    "\n",
    "    if LLM_PROVIDER != \"fake\" and llm:\n",
    "        system_prompt = (\n",
    "            \"あなたはタスクを分析し、リサーチャー、ライター、または自分自身（ユーザーへの応答）のいずれに \"\n",
    "            \"処理を割り振るかを決定するスーパーバイザーです。\"\n",
    "            \"可能なアクションは、「DelegateToResearcher: [調査指示]」、「DelegateToWriter: [執筆指示]」、\"\n",
    "            \"「RespondToUser: [ユーザーへの直接応答内容]」、「FINISH」のいずれかの形式で答えてください。\"\n",
    "            \"指示や応答内容には必ず具体的な内容を入れてください。\"\n",
    "            \"例1: ユーザーが「明日の東京の天気を調べて」と依頼したら、「DelegateToResearcher: 明日の東京の天気調査」と応答します。\"\n",
    "            \"例2: ユーザーが「AI倫理に関する記事を書いて」と依頼したら、「DelegateToWriter: AI倫理に関する記事執筆」と応答します。\"\n",
    "            \"例3: ユーザーが「こんにちは」と挨拶したら、「RespondToUser: こんにちは！ご用件は何でしょう？」と応答します。\"\n",
    "        )\n",
    "        response = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=user_req)])\n",
    "        decision_text = response.content.strip()\n",
    "        print(f\"  LLMによる判断: {decision_text}\")\n",
    "        if decision_text.startswith(\"DelegateToResearcher:\"):\n",
    "            next_act = \"DelegateToResearcher\"\n",
    "            research_desc = decision_text.replace(\"DelegateToResearcher:\", \"\").strip() or user_req # 指示が空なら元のリクエスト\n",
    "        elif decision_text.startswith(\"DelegateToWriter:\"):\n",
    "            next_act = \"DelegateToWriter\"\n",
    "            write_desc = decision_text.replace(\"DelegateToWriter:\", \"\").strip() or user_req\n",
    "        elif decision_text.startswith(\"RespondToUser:\"):\n",
    "            next_act = \"RespondToUser\"\n",
    "            sup_resp = decision_text.replace(\"RespondToUser:\", \"\").strip() or \"ごめんなさい、よくわかりませんでした。\"\n",
    "        elif decision_text == \"FINISH\":\n",
    "            next_act = \"FINISH\"\n",
    "        else: \n",
    "            print(\"  WARN: LLMが期待した形式で判断を返しませんでした。フォールバックロジックを使用します。\")\n",
    "            # フォールバックロジック (FakeLLMと同様)\n",
    "            if any(kw in user_req.lower() for kw in [\"調べ\", \"調査\", \"research\", \"find out\"]):\n",
    "                next_act = \"DelegateToResearcher\"; research_desc = user_req\n",
    "            elif any(kw in user_req.lower() for kw in [\"書い\", \"記事\", \"作成\", \"write\", \"article\"]):\n",
    "                next_act = \"DelegateToWriter\"; write_desc = user_req\n",
    "            else:\n",
    "                next_act = \"RespondToUser\"; sup_resp = \"ご要望を理解できませんでした。具体的な指示をいただけますか？\"\n",
    "    else: \n",
    "        if any(kw in user_req.lower() for kw in [\"調べ\", \"調査\", \"research\", \"find out\"]):\n",
    "            next_act = \"DelegateToResearcher\"; research_desc = user_req\n",
    "        elif any(kw in user_req.lower() for kw in [\"書い\", \"記事\", \"作成\", \"write\", \"article\"]):\n",
    "            next_act = \"DelegateToWriter\"; write_desc = user_req\n",
    "        else:\n",
    "            next_act = \"RespondToUser\"; sup_resp = \"FakeSupervisor: ご要望を理解できませんでした。\"\n",
    "            \n",
    "    print(f\"  決定された次のアクション: {next_act}\")\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"スーパーバイザー判断: {next_act}. 指示: {research_desc or write_desc or sup_resp}\", name=\"Supervisor\")],\n",
    "        \"next_action\": next_act,\n",
    "        \"research_task_description\": research_desc,\n",
    "        \"writing_task_description\": write_desc,\n",
    "        \"supervisor_response\": sup_resp,\n",
    "        # 他のエージェントの結果フィールドはクリアしない（後続のスーパーバイザー判断で使う可能性があるため）\n",
    "    }\n",
    "\n",
    "def simple_researcher_node(state: SupervisorAgentState) -> dict:\n",
    "    task = state.get(\"research_task_description\", \"指定なし\")\n",
    "    print(f\"\n",
    "[簡易リサーチャーノード] タスク: {task}\")\n",
    "    result = f\"「{task}」に関するダミー調査結果です。非常に詳細な情報が得られました。\"\n",
    "    if search_tool and task != \"指定なし\" and TAVILY_API_KEY and search_tool.name != \"dummy_search_tool\":\n",
    "        try: result = search_tool.invoke(task)\n",
    "        except Exception as e: print(f\"  リサーチャーツールエラー: {e}\")\n",
    "    elif search_tool and search_tool.name == \"dummy_search_tool\": result = search_tool.invoke(task)\n",
    "    print(f\"  調査結果: {str(result)[:100]}...\")\n",
    "    return {\"messages\": [AIMessage(content=str(result), name=\"Researcher\") ], \"research_result\": str(result), \"next_action\": \"FINISH\"} \n",
    "\n",
    "def simple_writer_node(state: SupervisorAgentState) -> dict:\n",
    "    task_desc = state.get(\"writing_task_description\", \"トピック指定なし\")\n",
    "    research_data = state.get(\"research_result\") # リサーチャーの結果も使えるように\n",
    "    input_for_writing = f\"執筆指示: {task_desc}\\n\" \n",
    "    if research_data: input_for_writing += f\"利用可能な調査結果: {research_data[:150]}...\\n\"\n",
    "    print(f\"\n",
    "[簡易ライターノード] {input_for_writing}\")\n",
    "    draft = f\"「{task_desc}」についての素晴らしい記事が完成しました。内容は次の通り...\"\n",
    "    if LLM_PROVIDER != \"fake\" and llm:\n",
    "        response = llm.invoke(f\"以下の指示と情報に基づいて記事を作成してください。\\n{input_for_writing}\")\n",
    "        draft = response.content\n",
    "    print(f\"  作成ドラフト: {draft[:100]}...\")\n",
    "    return {\"messages\": [AIMessage(content=draft, name=\"Writer\")], \"final_draft\": draft, \"next_action\": \"FINISH\"}\n",
    "\n",
    "def user_responder_node(state: SupervisorAgentState) -> dict:\n",
    "    response = state.get(\"supervisor_response\", \"エラーが発生したか、応答がありませんでした。\")\n",
    "    print(f\"\n",
    "[ユーザー応答ノード] スーパーバイザーからの応答: {response}\")\n",
    "    return {\"messages\": [AIMessage(content=response, name=\"SupervisorDirectResponse\")], \"next_action\": \"FINISH\"}\n",
    "\n",
    "# --- 3. ルーター関数の定義 ---\n",
    "def route_by_supervisor_decision(state: SupervisorAgentState) -> str:\n",
    "    decision = state.get(\"next_action\")\n",
    "    print(f\"  -> ルーター(SupervisorDecision): 次のアクションは「{decision}」です。\")\n",
    "    if decision == \"DelegateToResearcher\": return \"researcher\"\n",
    "    if decision == \"DelegateToWriter\": return \"writer\"\n",
    "    if decision == \"RespondToUser\": return \"user_responder\"\n",
    "    return END \n",
    "\n",
    "# --- 4. グラフの構築 ---\n",
    "workflow_q2_ch4 = StateGraph(SupervisorAgentState)\n",
    "workflow_q2_ch4.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow_q2_ch4.add_node(\"researcher\", simple_researcher_node)\n",
    "workflow_q2_ch4.add_node(\"writer\", simple_writer_node)\n",
    "workflow_q2_ch4.add_node(\"user_responder\", user_responder_node)\n",
    "\n",
    "workflow_q2_ch4.set_entry_point(\"supervisor\")\n",
    "\n",
    "workflow_q2_ch4.add_conditional_edges(\n",
    "    \"supervisor\", route_by_supervisor_decision,\n",
    "    {\n",
    "        \"researcher\": \"researcher\", \"writer\": \"writer\", \n",
    "        \"user_responder\": \"user_responder\", END: END\n",
    "    }\n",
    ")\n",
    "workflow_q2_ch4.add_conditional_edges(\"researcher\", route_by_supervisor_decision, {END: END}) # 実行後、next_action='FINISH'で終了\n",
    "workflow_q2_ch4.add_conditional_edges(\"writer\", route_by_supervisor_decision, {END: END})\n",
    "workflow_q2_ch4.add_conditional_edges(\"user_responder\", route_by_supervisor_decision, {END: END})\n",
    "\n",
    "graph_q2_ch4 = workflow_q2_ch4.compile()\n",
    "try: display(Image(graph_q2_ch4.get_graph().draw_png()))\n",
    "except Exception as e: print(f\"グラフ描画失敗: {e}\")\n",
    "\n",
    "# --- 5. グラフの実行 ---\n",
    "test_requests_q2 = [\n",
    "    \"LangGraphの分散型エージェントアーキテクチャについて調査し、その利点をまとめてください。\",\n",
    "    \"AIが創造性を発揮する事例について、感動的なブログ記事を執筆してください。\",\n",
    "    \"今日の天気は良いですね！\"\n",
    "]\n",
    "\n",
    "for i, req in enumerate(test_requests_q2):\n",
    "    print(f\"\n",
    "--- スーパーバイザーテスト {i+1} (リクエスト: {req}) ---\")\n",
    "    initial_state_q2_ch4 = {\n",
    "        \"messages\": [HumanMessage(content=req)], \"user_request\": req,\n",
    "        \"next_action\": None, \"research_task_description\": None, \"research_result\": None,\n",
    "        \"writing_task_description\": None, \"final_draft\": None, \"supervisor_response\": None\n",
    "    }\n",
    "    thread_q2 = {\"configurable\": {\"thread_id\": f\"supervisor-test-{i}-{uuid4()[:4]}\"}}}\n",
    "    final_run_state_q2 = None\n",
    "    for event in graph_q2_ch4.stream(initial_state_q2_ch4, config=thread_q2, recursion_limit=5):\n",
    "        print(f\"Event: {event}\")\n",
    "        # streamの各要素は {'node_name': state_update_dict } という形式\n",
    "        # 最後のイベントがENDノードからのものであれば、そのvalueが最終状態\n",
    "        if END in event:\n",
    "            final_run_state_q2 = event[END]\n",
    "        print(\"----\");\n",
    "    \n",
    "    if not final_run_state_q2: # ENDから取得できなかった場合 (例: recursion limit)\n",
    "        final_run_state_q2 = graph_q2_ch4.get_state(thread_q2).values\n",
    "\n",
    "    print(\"\n",
    "  最終結果:\")\n",
    "    if final_run_state_q2.get(\"research_result\"):\n",
    "        print(f\"    調査結果: {final_run_state_q2['research_result'][:100]}...\")\n",
    "    if final_run_state_q2.get(\"final_draft\"):\n",
    "        print(f\"    最終ドラフト: {final_run_state_q2['final_draft'][:100]}...\")\n",
    "    if final_run_state_q2.get(\"supervisor_response\") and not final_run_state_q2.get(\"research_result\") and not final_run_state_q2.get(\"final_draft\"):\n",
    "        print(f\"    スーパーバイザーからの直接応答: {final_run_state_q2['supervisor_response']}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説002</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **スーパーバイザーの役割:** `supervisor_agent` ノードが、ユーザーの要求 (`user_request`) を解釈し、次にどのアクションを取るべきか（`next_action`）、そしてそのための具体的な指示（`research_task_description` や `writing_task_description`）を決定します。この判断は、実際のシステムではLLMの推論能力に大きく依存します。解答例では、LLMが期待する形式で判断を返すようにプロンプトで指示し、もし期待通りでなければフォールバックとしてキーワードベースの単純なロジックで割り振りを試みています。\n",
    "*   **状態によるタスク情報の伝達:** スーパーバイザーが決定したタスク指示は、状態オブジェクトの対応するキー（例: `research_task_description`）に格納され、委任先の専門エージェントノード（`simple_researcher_node` など）はその情報を読み取って処理を実行します。\n",
    "*   **ルーターによる処理分岐:** `route_by_supervisor_decision` ルーターが、スーパーバイザーの決定（`state['next_action']`）に基づいて、次に実行する専門エージェントのノード、またはユーザーへの直接応答ノード、あるいは終了（`END`）へと処理を振り分けます。\n",
    "*   **専門エージェントの処理:** `simple_researcher_node` や `simple_writer_node` は、割り当てられたタスクを実行し、その結果を状態に書き戻します。この問題では、各専門エージェントは一度処理を実行したら `next_action` を `\"FINISH\"` に設定し、スーパーバイザーには戻らずに処理を終了する単純な流れになっています。\n",
    "*   **拡張性:** この基本構造は、より多くの専門エージェントを追加したり、専門エージェントの処理後に再度スーパーバイザーに結果を報告させて次の指示を仰ぐような、より複雑な協調ワークフローへと拡張する際の基礎となります。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題003: スーパーバイザーによる逐次連携ワークフロー\n",
    "\n",
    "問題002のスーパーバイザーモデルを拡張し、複数の専門エージェントがスーパーバイザーの指示のもとで逐次的に連携するワークフローを構築します。例えば、「トピックについて調査（リサーチャー）し、その結果を元に記事を作成（ライター）し、最後に記事をレビュー（レビュアー）する」といった流れです。スーパーバイザーは各ステップの完了を確認し、次のエージェントに必要な情報を渡しながらタスクを進めます。\n",
    "\n",
    "*   **学習内容:**\n",
    "    *   スーパーバイザーがタスクの進行状況を管理し、複数のエージェントに順番に処理を委任していく方法。\n",
    "    *   状態（State）に現在のタスクフェーズ（例: \"RESEARCHING\", \"WRITING\", \"REVIEWING\"）や、各エージェントの成果物を保持し、それらを次のエージェントに引き渡す方法。\n",
    "    *   スーパーバイザーが全工程の完了を判断し、最終成果物を生成（または選択）して終了するロジック。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003 - グラフ構築\n",
    "from typing import TypedDict, List, Optional, Annotated, Literal # Literal をインポート (解答例より)\n",
    "from langgraph.graph import ____, END # (既にインポート済みだが明示)\n",
    "from langgraph.graph.message import add_messages # (既にインポート済みだが明示)\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage # (既にインポート済みだが明示)\n",
    "from uuid import uuid4 # 解答例より\n",
    "import re # 解答例より\n",
    "\n",
    "# --- 1. 状態定義 ---\n",
    "class ____(____):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    user_request: str\n",
    "    ____: Optional[Literal[\"PLANNING\", \"RESEARCHING\", \"WRITING\", \"REVIEWING\", \"DONE\", \"ERROR\"]] # 解答例よりOptional追加\n",
    "    ____: Optional[str]\n",
    "    research_findings: Optional[str]\n",
    "    ____: Optional[str]\n",
    "    review_comments: Optional[str]\n",
    "    final_product: Optional[str]\n",
    "    error_message: Optional[str]\n",
    "\n",
    "# --- 2. エージェント（ノード）定義 ---\n",
    "def ____(state: SequentialWorkflowState) -> dict:\n",
    "    current_phase = state.get(\"current_phase\") # 解答例より\n",
    "    user_req = state.get(\"user_request\", \"\") # 解答例より\n",
    "    print(f\"\n",
    "[スーパーバイザー・プランナー] 現在フェーズ: {current_phase}, リクエスト: {user_req[:50]}...\") # 解答例より\n",
    "    \n",
    "    if not current_phase: # 初回実行 (解答例より)\n",
    "        topic = user_req # 解答例より\n",
    "        match = re.search(r\"「([^」]+)」について\", user_req) or re.search(r\"『([^』]+)』について\", user_req) # 解答例より\n",
    "        if match: topic = match.group(1) # 解答例より\n",
    "        else: topic = user_req.replace(\"に関する記事を作成\",\"\").replace(\"について記事を書いて\",\"\").strip(\"。 \")[:30] # 解答例より\n",
    "        \n",
    "        print(f\"  -> 計画: リサーチフェーズ開始。トピック: {topic}\")\n",
    "        return {\"current_phase\": \"RESEARCHING\", \"research_topic\": topic, \"messages\": [AIMessage(content=f\"計画: 「{topic}」の調査を開始します。\", name=\"Supervisor\")]}\n",
    "    \n",
    "    next_phase: Optional[Literal[\"PLANNING\", \"RESEARCHING\", \"WRITING\", \"REVIEWING\", \"DONE\", \"ERROR\"]] = None # 解答例より\n",
    "    supervisor_log = \"\"\n",
    "    update_dict = {} # 解答例より\n",
    "\n",
    "    if current_phase == \"RESEARCHING\": # 解答例より\n",
    "        if state.get(\"research_findings\"):\n",
    "            next_phase = \"WRITING\"\n",
    "            supervisor_log = \"調査完了。執筆フェーズへ。\"\n",
    "        elif state.get(\"error_message\"): next_phase = \"ERROR\"\n",
    "    elif current_phase == \"WRITING\": # 解答例より\n",
    "        if state.get(\"article_draft\"):\n",
    "            next_phase = \"REVIEWING\"\n",
    "            supervisor_log = \"執筆完了。レビューフェーズへ。\"\n",
    "        elif state.get(\"error_message\"): next_phase = \"ERROR\"\n",
    "    elif current_phase == \"REVIEWING\": # 解答例より\n",
    "        if state.get(\"review_comments\"):\n",
    "            next_phase = \"DONE\"\n",
    "            supervisor_log = f\"レビュー完了。コメント: 「{state['review_comments']}」。処理を終了します。\"\n",
    "            update_dict[\"final_product\"] = state.get(\"article_draft\") \n",
    "        elif state.get(\"error_message\"): next_phase = \"ERROR\"\n",
    "    \n",
    "    if state.get(\"error_message\") and not next_phase: # 解答例より\n",
    "        next_phase = \"ERROR\"\n",
    "        supervisor_log = f\"エラー発生のため処理を中断します: {state['error_message']}\"\n",
    "        \n",
    "    if next_phase: # 解答例より\n",
    "        print(f\"  -> スーパーバイザー判断: 次のフェーズ「{next_phase}」へ。ログ: {supervisor_log}\")\n",
    "        update_dict[\"current_phase\"] = next_phase\n",
    "        update_dict[\"messages\"] = [AIMessage(content=supervisor_log if supervisor_log else f\"次のフェーズ: {next_phase}\", name=\"Supervisor\")]\n",
    "        return update_dict\n",
    "    \n",
    "    print(\"  -> スーパーバイザー判断: 現状維持または不明な状態。\") # 解答例より\n",
    "    return {} # 変更なし (解答例より)\n",
    "\n",
    "def ____(state: SequentialWorkflowState) -> dict:\n",
    "    topic = state[\"research_topic\"]\n",
    "    print(f\"\n",
    "[リサーチノード] トピック: {topic}\")\n",
    "    findings = f\"「{topic}」に関するダミー調査結果。ポイントX, Y, Z。\" # 解答例より\n",
    "    print(f\"  -> 調査結果: {findings[:100]}...\")\n",
    "    return {\"research_findings\": findings, \"messages\": [AIMessage(content=findings, name=\"Researcher\")]}\n",
    "\n",
    "def ____(state: SequentialWorkflowState) -> dict:\n",
    "    findings = state[\"research_findings\"]\n",
    "    topic = state[\"research_topic\"]\n",
    "    print(f\"\n",
    "[ライティングノード] 調査結果に基づいて「{topic}」の記事を作成します。\")\n",
    "    draft = f\"タイトル: {topic}の全貌\\n\\n{findings}\\n\\nこの記事は、提供された情報に基づきAIによって生成されました。\" # 解答例より\n",
    "    print(f\"  -> 作成ドラフト: {draft[:100]}...\")\n",
    "    return {\"article_draft\": draft, \"messages\": [AIMessage(content=draft, name=\"Writer\")]}\n",
    "\n",
    "def ____(state: SequentialWorkflowState) -> dict:\n",
    "    draft = state[\"article_draft\"]\n",
    "    print(f\"\n",
    "[レビューノード] ドラフトをレビューします: {draft[:50]}...\")\n",
    "    comments = \"素晴らしい内容です。特に導入部分が読者の興味を引きます。改善点は特に見当たりません。\" # 解答例より\n",
    "    print(f\"  -> レビューコメント: {comments}\")\n",
    "    return {\"review_comments\": comments, \"messages\": [AIMessage(content=comments, name=\"Reviewer\")]}\n",
    "\n",
    "def error_node(state: SequentialWorkflowState) -> dict:\n",
    "    err_msg = state.get('error_message', '不明なエラーが発生しました。処理を終了します。') # 解答例より\n",
    "    print(f\"\n",
    "[エラー処理ノード] エラーメッセージ: {err_msg}\")\n",
    "    return {\"messages\": [AIMessage(content=f\"エラー発生: {err_msg}\", name=\"Error Handler\")]} # 解答例より\n",
    "\n",
    "# --- 3. ルーター関数の定義 ---\n",
    "def ____(state: SequentialWorkflowState) -> str:\n",
    "    phase = state.get(\"current_phase\")\n",
    "    print(f\"  -> ルーター(Phase): 現在のフェーズは「{phase}」です。\")\n",
    "    if phase == \"RESEARCHING\": return \"researcher\"\n",
    "    if phase == \"WRITING\": return \"writer\"\n",
    "    if phase == \"REVIEWING\": return \"reviewer\"\n",
    "    if phase == \"DONE\": return END\n",
    "    if phase == \"ERROR\": return \"error_handler\"\n",
    "    print(f\"  WARN: 不明なフェーズ「{phase}」です。スーパーバイザーに戻します。\") # 解答例より\n",
    "    return \"supervisor_planner\" # 不明なフェーズはスーパーバイザーに戻す (解答例より)\n",
    "\n",
    "# --- 4. グラフの構築 ---\n",
    "workflow_q3_ch4 = StateGraph(SequentialWorkflowState)\n",
    "workflow_q3_ch4.add_node(\"supervisor_planner\", supervisor_planner_node)\n",
    "workflow_q3_ch4.add_node(\"researcher\", research_node)\n",
    "workflow_q3_ch4.add_node(\"writer\", writing_node)\n",
    "workflow_q3_ch4.add_node(\"reviewer\", review_node)\n",
    "workflow_q3_ch4.add_node(\"error_handler\", error_node)\n",
    "\n",
    "workflow_q3_ch4.set_entry_point(\"supervisor_planner\") \n",
    "\n",
    "workflow_q3_ch4.add_conditional_edges(\n",
    "    \"supervisor_planner\", route_by_phase,\n",
    "    {\n",
    "        \"researcher\": \"researcher\", \"writer\": \"writer\", \"reviewer\": \"reviewer\",\n",
    "        \"supervisor_planner\": \"supervisor_planner\", # ルーターが不明なフェーズと判断した場合など (解答例より)\n",
    "        END: END, \"error_handler\": \"error_handler\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow_q3_ch4.____(\"researcher\", \"supervisor_planner\") # 解答例より\n",
    "workflow_q3_ch4.add_edge(\"writer\", \"supervisor_planner\") # 解答例より\n",
    "workflow_q3_ch4.add_edge(\"reviewer\", \"supervisor_planner\") # 解答例より\n",
    "\n",
    "workflow_q3_ch4.add_edge(\"error_handler\", END) \n",
    "\n",
    "graph_q3_ch4 = workflow_q3_ch4.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003 - グラフ可視化\n",
    "from IPython.display import Image, display # 解答例より\n",
    "\n",
    "try:\n",
    "    display(Image(graph_q3_ch4.____().____()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄003 - グラフ実行\n",
    "user_req_q3 = \"LangGraphの条件付きエッジ機能について、その利点と簡単な使用例を含む技術ブログ記事を作成してください。\"\n",
    "initial_state_q3_ch4 = {\n",
    "    \"messages\": [____(content=user_req_q3)], \"____\": user_req_q3, # 解答例より\n",
    "    \"____\": None, \"research_topic\": None, \"____\": None, \n",
    "    \"____\": None, \"____\": None, \"____\": None, \"error_message\": None\n",
    "}\n",
    "thread_q3 = {\"____\": {\"thread_id\": f\"seq-workflow-{uuid4()[:4]}\"}}}\n",
    "\n",
    "print(f\"--- 逐次連携ワークフローテスト (リクエスト: {user_req_q3}) ---\")\n",
    "final_q3_state_val = None\n",
    "for event in graph_q3_ch4.____(initial_state_q3_ch4, config=thread_q3, recursion_limit=10): # 解答例より recursion_limit 変更\n",
    "    print(f\"Event: {event}\")\n",
    "    if END in event: final_q3_state_val = event[END]\n",
    "    print(\"----\");\n",
    "\n",
    "if not final_q3_state_val: final_q3_state_val = graph_q3_ch4.____(thread_q3).values\n",
    "\n",
    "print(\"\n",
    "  最終成果物:\")\n",
    "if final_q3_state_val.get(\"final_product\"):\n",
    "    print(f\"    {final_q3_state_val['final_product']}\")\n",
    "elif final_q3_state_val.get(\"error_message\"):\n",
    "    print(f\"    エラー: {final_q3_state_val['error_message']}\")\n",
    "else:\n",
    "    print(f\"    最終成果物がありませんでした。最終フェーズ: {final_q3_state_val.get('current_phase')}\") # 解答例より"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答003</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, List, Optional, Annotated, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "from uuid import uuid4\n",
    "import re\n",
    "\n",
    "# --- 1. 状態定義 ---\n",
    "class SequentialWorkflowState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    user_request: str\n",
    "    current_phase: Optional[Literal[\"PLANNING\", \"RESEARCHING\", \"WRITING\", \"REVIEWING\", \"DONE\", \"ERROR\"]]\n",
    "    research_topic: Optional[str]\n",
    "    research_findings: Optional[str]\n",
    "    article_draft: Optional[str]\n",
    "    review_comments: Optional[str]\n",
    "    final_product: Optional[str]\n",
    "    error_message: Optional[str]\n",
    "\n",
    "# --- 2. エージェント（ノード）定義 ---\n",
    "def supervisor_planner_node(state: SequentialWorkflowState) -> dict:\n",
    "    current_phase = state.get(\"current_phase\")\n",
    "    user_req = state.get(\"user_request\", \"\")\n",
    "    print(f\"\n",
    "[スーパーバイザー・プランナー] 現在フェーズ: {current_phase}, リクエスト: {user_req[:50]}...\")\n",
    "    \n",
    "    if not current_phase: # 初回実行\n",
    "        # 簡単なリクエスト解析（実際はLLMでトピック抽出）\n",
    "        topic = user_req\n",
    "        match = re.search(r\"「([^」]+)」について\", user_req) or re.search(r\"『([^』]+)』について\", user_req)\n",
    "        if match: topic = match.group(1)\n",
    "        else: topic = user_req.replace(\"に関する記事を作成\",\"\").replace(\"について記事を書いて\",\"\").strip(\"。 \")[:30] # 短縮\n",
    "        \n",
    "        print(f\"  -> 計画: リサーチフェーズ開始。トピック: {topic}\")\n",
    "        return {\"current_phase\": \"RESEARCHING\", \"research_topic\": topic, \"messages\": [AIMessage(content=f\"計画: 「{topic}」の調査を開始します。\", name=\"Supervisor\")]}\n",
    "    \n",
    "    next_phase: Optional[Literal[\"PLANNING\", \"RESEARCHING\", \"WRITING\", \"REVIEWING\", \"DONE\", \"ERROR\"]] = None\n",
    "    supervisor_log = \"\"\n",
    "    update_dict = {}\n",
    "\n",
    "    if current_phase == \"RESEARCHING\":\n",
    "        if state.get(\"research_findings\"):\n",
    "            next_phase = \"WRITING\"\n",
    "            supervisor_log = \"調査完了。執筆フェーズへ。\"\n",
    "        elif state.get(\"error_message\"): next_phase = \"ERROR\"\n",
    "    elif current_phase == \"WRITING\":\n",
    "        if state.get(\"article_draft\"):\n",
    "            next_phase = \"REVIEWING\"\n",
    "            supervisor_log = \"執筆完了。レビューフェーズへ。\"\n",
    "        elif state.get(\"error_message\"): next_phase = \"ERROR\"\n",
    "    elif current_phase == \"REVIEWING\":\n",
    "        if state.get(\"review_comments\"):\n",
    "            next_phase = \"DONE\"\n",
    "            supervisor_log = f\"レビュー完了。コメント: 「{state['review_comments']}」。処理を終了します。\"\n",
    "            update_dict[\"final_product\"] = state.get(\"article_draft\") # レビュー後のドラフトを最終成果物とする\n",
    "        elif state.get(\"error_message\"): next_phase = \"ERROR\"\n",
    "    \n",
    "    if state.get(\"error_message\") and not next_phase:\n",
    "        next_phase = \"ERROR\"\n",
    "        supervisor_log = f\"エラー発生のため処理を中断します: {state['error_message']}\"\n",
    "        \n",
    "    if next_phase:\n",
    "        print(f\"  -> スーパーバイザー判断: 次のフェーズ「{next_phase}」へ。ログ: {supervisor_log}\")\n",
    "        update_dict[\"current_phase\"] = next_phase\n",
    "        update_dict[\"messages\"] = [AIMessage(content=supervisor_log if supervisor_log else f\"次のフェーズ: {next_phase}\", name=\"Supervisor\")]\n",
    "        return update_dict\n",
    "    \n",
    "    print(\"  -> スーパーバイザー判断: 現状維持または不明な状態。\")\n",
    "    return {} # 変更なし\n",
    "\n",
    "def research_node(state: SequentialWorkflowState) -> dict:\n",
    "    topic = state[\"research_topic\"]\n",
    "    print(f\"\n",
    "[リサーチノード] トピック: {topic}\")\n",
    "    findings = f\"「{topic}」に関するダミー調査結果。ポイントX, Y, Z。\"\n",
    "    # if search_tool and TAVILY_API_KEY and search_tool.name != \"dummy_search_tool\":\n",
    "    #     try: findings = search_tool.invoke(topic)\n",
    "    #     except Exception as e: return {\"error_message\": str(e), \"current_phase\": state[\"current_phase\"]}\n",
    "    print(f\"  -> 調査結果: {findings[:100]}...\")\n",
    "    return {\"research_findings\": findings, \"messages\": [AIMessage(content=findings, name=\"Researcher\")]}\n",
    "\n",
    "def writing_node(state: SequentialWorkflowState) -> dict:\n",
    "    findings = state[\"research_findings\"]\n",
    "    topic = state[\"research_topic\"]\n",
    "    print(f\"\n",
    "[ライティングノード] 調査結果に基づいて「{topic}」の記事を作成します。\")\n",
    "    draft = f\"タイトル: {topic}の全貌\\n\\n{findings}\\n\\nこの記事は、提供された情報に基づきAIによって生成されました。\"\n",
    "    print(f\"  -> 作成ドラフト: {draft[:100]}...\")\n",
    "    return {\"article_draft\": draft, \"messages\": [AIMessage(content=draft, name=\"Writer\")]}\n",
    "\n",
    "def review_node(state: SequentialWorkflowState) -> dict:\n",
    "    draft = state[\"article_draft\"]\n",
    "    print(f\"\n",
    "[レビューノード] ドラフトをレビューします: {draft[:50]}...\")\n",
    "    comments = \"素晴らしい内容です。特に導入部分が読者の興味を引きます。改善点は特に見当たりません。\"\n",
    "    print(f\"  -> レビューコメント: {comments}\")\n",
    "    return {\"review_comments\": comments, \"messages\": [AIMessage(content=comments, name=\"Reviewer\")]}\n",
    "\n",
    "def error_node(state: SequentialWorkflowState) -> dict:\n",
    "    err_msg = state.get('error_message', '不明なエラーが発生しました。処理を終了します。')\n",
    "    print(f\"\n",
    "[エラー処理ノード] エラーメッセージ: {err_msg}\")\n",
    "    # final_product にエラー情報を入れてもよい\n",
    "    return {\"messages\": [AIMessage(content=f\"エラー発生: {err_msg}\", name=\"Error Handler\")]}\n",
    "\n",
    "# --- 3. ルーター関数の定義 ---\n",
    "def route_by_phase(state: SequentialWorkflowState) -> str:\n",
    "    phase = state.get(\"current_phase\")\n",
    "    print(f\"  -> ルーター(Phase): 現在のフェーズは「{phase}」です。\")\n",
    "    if phase == \"RESEARCHING\": return \"researcher\"\n",
    "    if phase == \"WRITING\": return \"writer\"\n",
    "    if phase == \"REVIEWING\": return \"reviewer\"\n",
    "    if phase == \"DONE\": return END\n",
    "    if phase == \"ERROR\": return \"error_handler\"\n",
    "    # 初期状態や予期せぬフェーズの場合はスーパーバイザーに戻すか、エラーにする\n",
    "    # ここでは、supervisor_plannerが最初のフェーズを設定するので、基本的には上記に分岐するはず\n",
    "    print(f\"  WARN: 不明なフェーズ「{phase}」です。スーパーバイザーに戻します。\")\n",
    "    return \"supervisor_planner\" \n",
    "\n",
    "# --- 4. グラフの構築 ---\n",
    "workflow_q3_ch4 = StateGraph(SequentialWorkflowState)\n",
    "workflow_q3_ch4.add_node(\"supervisor_planner\", supervisor_planner_node)\n",
    "workflow_q3_ch4.add_node(\"researcher\", research_node)\n",
    "workflow_q3_ch4.add_node(\"writer\", writing_node)\n",
    "workflow_q3_ch4.add_node(\"reviewer\", review_node)\n",
    "workflow_q3_ch4.add_node(\"error_handler\", error_node)\n",
    "\n",
    "workflow_q3_ch4.set_entry_point(\"supervisor_planner\")\n",
    "\n",
    "workflow_q3_ch4.add_conditional_edges(\n",
    "    \"supervisor_planner\", route_by_phase,\n",
    "    {\n",
    "        \"researcher\": \"researcher\", \"writer\": \"writer\", \"reviewer\": \"reviewer\",\n",
    "        \"supervisor_planner\": \"supervisor_planner\", # ルーターが不明なフェーズと判断した場合など\n",
    "        END: END, \"error_handler\": \"error_handler\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow_q3_ch4.add_edge(\"researcher\", \"supervisor_planner\")\n",
    "workflow_q3_ch4.add_edge(\"writer\", \"supervisor_planner\")\n",
    "workflow_q3_ch4.add_edge(\"reviewer\", \"supervisor_planner\")\n",
    "workflow_q3_ch4.add_edge(\"error_handler\", END)\n",
    "\n",
    "graph_q3_ch4 = workflow_q3_ch4.compile()\n",
    "try: display(Image(graph_q3_ch4.get_graph().draw_png()))\n",
    "except Exception as e: print(f\"グラフ描画失敗: {e}\")\n",
    "\n",
    "# --- 5. グラフの実行 ---\n",
    "user_req_q3 = \"LangGraphの条件付きエッジ機能について、その利点と簡単な使用例を含む技術ブログ記事を作成してください。\"\n",
    "initial_state_q3_ch4 = {\n",
    "    \"messages\": [HumanMessage(content=user_req_q3)], \"user_request\": user_req_q3,\n",
    "    \"current_phase\": None, \"research_topic\": None, \"research_findings\": None, \n",
    "    \"article_draft\": None, \"review_comments\": None, \"final_product\": None, \"error_message\": None\n",
    "}\n",
    "thread_q3 = {\"configurable\": {\"thread_id\": f\"seq-workflow-{uuid4()[:4]}\"}}}\n",
    "\n",
    "print(f\"--- 逐次連携ワークフローテスト (リクエスト: {user_req_q3}) ---\")\n",
    "final_q3_state_val = None\n",
    "for event in graph_q3_ch4.stream(initial_state_q3_ch4, config=thread_q3, recursion_limit=10):\n",
    "    print(f\"Event: {event}\")\n",
    "    if END in event: final_q3_state_val = event[END]\n",
    "    print(\"----\");\n",
    "\n",
    "if not final_q3_state_val: final_q3_state_val = graph_q3_ch4.get_state(thread_q3).values\n",
    "\n",
    "print(\"\n",
    "  最終成果物:\")\n",
    "if final_q3_state_val.get(\"final_product\"):\n",
    "    print(f\"    {final_q3_state_val['final_product']}\")\n",
    "elif final_q3_state_val.get(\"error_message\"):\n",
    "    print(f\"    エラー: {final_q3_state_val['error_message']}\")\n",
    "else:\n",
    "    print(f\"    最終成果物がありませんでした。最終フェーズ: {final_q3_state_val.get('current_phase')}\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説003</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **スーパーバイザーによるフェーズ管理:** `supervisor_planner_node` がワークフロー全体の進行管理を行います。状態キー `current_phase` を見て、次にどの専門エージェントを呼び出すべきか（または処理を終了すべきか）を決定します。\n",
    "    *   初回実行時は、`user_request` からトピックを（簡易的に）抽出し、`current_phase` を \"RESEARCHING\" に設定してリサーチャーを起動します。\n",
    "    *   各専門エージェント（リサーチャー、ライター、レビュアー）の処理が完了すると、エッジは再び `supervisor_planner_node` に戻ります。スーパーバイザーは、その時点での状態（例: `research_findings` が存在するか）を確認し、次のフェーズ（例: \"WRITING\"）に進むよう `current_phase` を更新します。\n",
    "*   **専門エージェントの役割:**\n",
    "    *   `research_node`: `research_topic` に基づいて調査を行い、結果を `research_findings` に格納します。\n",
    "    *   `writing_node`: `research_findings` を利用して記事ドラフトを `article_draft` に作成します。\n",
    "    *   `review_node`: `article_draft` をレビューし、コメントを `review_comments` に格納します。\n",
    "    *   各専門ノードは、自身の処理結果を状態に書き込んだ後、制御をスーパーバイザーに戻します。\n",
    "*   **ルーター (`route_by_phase`):** スーパーバイザーが更新した `current_phase` の値に基づいて、実際に次に実行する専門エージェントのノード、またはエラー処理ノード、あるいは終了 (`END`) へと処理をルーティングします。\n",
    "*   **逐次処理の実現:** 「専門エージェント処理 → スーパーバイザーによる次のフェーズ決定 → ルーターによる分岐」というサイクルを繰り返すことで、リサーチ、執筆、レビューという一連のタスクが順番に実行されます。\n",
    "*   **エラーハンドリング:** 各専門エージェントの処理中にエラーが発生した場合（この解答例ではシミュレートしていませんが、発生しうる）、`error_message` に情報を記録し、スーパーバイザーがそれを検知して `error_handler_node` に処理を移すような拡張が考えられます（解答例では簡易的なエラーパスのみ）。\n",
    "*   **最終成果物:** 全てのフェーズが正常に完了すると（この例ではレビュー完了後）、スーパーバイザーは `current_phase` を \"DONE\" に設定し、`final_product` に最終的な成果物（ここではレビュー後の記事ドラフト）を格納して終了します。\n",
    "\n",
    "このパターンは、複数のステップからなる複雑なタスクを、各ステップの専門家（エージェント）に分担させ、スーパーバイザーが全体を統括する、という現実世界のプロジェクト進行に近い形で自動化する際に有効です。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題004: 階層型エージェント - タスクの委任と報告\n",
    "\n",
    "エージェントの組織構造を階層的にすることで、より複雑な問題解決に対応できます。この問題では、上位の「マネージャーエージェント」がタスクを受け取り、それをより具体的なサブタスクに分解して、下位の「ワーカーエージェント」に委任します。ワーカーエージェントはサブタスクを実行し、その結果をマネージャーに報告。マネージャーは全ワーカーの結果を統合して最終的な成果を出す、という階層的な協調作業をシミュレートします。\n",
    "\n",
    "*   **学習内容:**\n",
    "    *   異なる階層レベルのエージェント（マネージャー、ワーカー）を定義し、それぞれの役割（タスク分解、サブタスク実行、結果統合）を実装する方法。\n",
    "    *   マネージャーがタスクを複数のサブタスクに分割し、それらを状態を通じてワーカーに渡す方法。\n",
    "    *   ワーカーが並列または逐次でサブタスクを実行し、結果を状態に格納する方法（ファンアウト・ファンインの応用）。\n",
    "    *   マネージャーがワーカーからの報告を集約し、最終的な応答を生成するプロセス。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004 - グラフ構築\n",
    "from typing import TypedDict, List, Optional, Annotated, Dict, Any\n",
    "from langgraph.graph import StateGraph, END # 解答例より (既にインポート済みだが明示)\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage # 解答例より (既にインポート済みだが明示)\n",
    "from langchain_core.tools import tool # 解答例より (既にインポート済みだが明示)\n",
    "from uuid import uuid4 # 解答例より (既にインポート済みだが明示)\n",
    "import re # 解答例より (既にインポート済みだが明示)\n",
    "import json # 解答例より (既にインポート済みだが明示)\n",
    "\n",
    "# --- 1. 状態定義 ---\n",
    "class ____(____):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    ____: str\n",
    "    ____: Optional[List[Dict[str, Any]]] \n",
    "    ____: Optional[str]\n",
    "    # current_worker_index: Optional[int] # マネージャーの実行調整ノード内で管理・更新されるので、状態としては必須ではない (解答例より)\n",
    "    # error_message: Optional[str] # エラー処理を追加する場合 (解答例より)\n",
    "\n",
    "# --- 2. ワーカーエージェントのシミュレーション関数 ---\n",
    "def ____(worker_id: str, instruction: str, dependencies_results: Optional[Dict[str, str]] = None) -> str: # 解答例より\n",
    "    print(f\"    [ワーカー {worker_id}] 指示: 「{instruction}」を実行開始。\")\n",
    "    if dependencies_results:\n",
    "        print(f\"      依存結果: {dependencies_results}\")\n",
    "    \n",
    "    result = f\"ワーカー「{worker_id}」による「{instruction[:20]}...」の高品質な実行結果。\"\n",
    "    if \"天気\" in instruction.lower() and search_tool and search_tool.name != \"dummy_search_tool\" and TAVILY_API_KEY:\n",
    "        try: result = search_tool.invoke(instruction)\n",
    "        except Exception as e: result = f\"天気検索エラー: {e}\"\n",
    "    elif \"レポート作成\" in instruction and dependencies_results:\n",
    "        report_content = \"レポート:\\n\"\n",
    "        for task_id, dep_res in dependencies_results.items():\n",
    "            report_content += f\"  - {task_id}の結果: {str(dep_res)[:50]}...\\n\"\n",
    "        result = report_content + \"上記を統合しました。\"\n",
    "    \n",
    "    print(f\"    -> ワーカー {worker_id} の結果: {str(result)[:80]}...\")\n",
    "    return str(result)\n",
    "\n",
    "# --- 3. マネージャーエージェント（ノード）定義 ---\n",
    "def ____(state: HierarchicalAgentState) -> dict: # 解答例よりノード名変更\n",
    "    print(f\"\n",
    "[マネージャー: タスク分解] メインタスク: {state['main_task_description']}\")\n",
    "    sub_tasks_list: List[Dict[str, Any]] = [] # 解答例より\n",
    "    main_task_lower = state['main_task_description'].lower()\n",
    "    \n",
    "    if \"天気\" in main_task_lower and (\"観光\" in main_task_lower or \"スポット\" in main_task_lower) and \"レポート\" in main_task_lower: # 解答例の条件分岐\n",
    "        location = \"東京\" \n",
    "        if \"大阪\" in main_task_lower: location = \"大阪\"\n",
    "        sub_tasks_list.append({'id': 'task_weather', 'instruction': f'{location}の今日の天気調査', 'assigned_to': 'WeatherWorker', 'status': 'pending', 'result': None, 'dependencies': []}) # 解答例より dependencies 追加\n",
    "        sub_tasks_list.append({'id': 'task_spots', 'instruction': f'{location}の主要観光スポット3箇所調査', 'assigned_to': 'TourismWorker', 'status': 'pending', 'result': None, 'dependencies': []}) # 解答例より dependencies 追加\n",
    "        sub_tasks_list.append({'id': 'task_report', 'instruction': f'{location}の天気と観光スポットに関する統合レポート作成', 'assigned_to': 'ReportWriter', 'status': 'pending', 'result': None, '____': ['task_weather', 'task_spots']}) # 解答例より dependencies 追加\n",
    "    else:\n",
    "        sub_tasks_list.append({'id': 'task_general', 'instruction': state['main_task_description'], 'assigned_to': 'GeneralWorker', 'status': 'pending', 'result': None, 'dependencies': []}) # 解答例より dependencies 追加\n",
    "        \n",
    "    print(f\"  分解されたサブタスク: {json.dumps(sub_tasks_list, ensure_ascii=False, indent=2)}\") # 解答例より\n",
    "    return {\"sub_tasks\": sub_tasks_list, \"messages\": [AIMessage(content=f\"タスクを{len(sub_tasks_list)}個のサブタスクに分解しました。\", name=\"ManagerDecomposer\")]} # current_worker_index は coordinator で管理 (解答例より)\n",
    "\n",
    "def ____(state: HierarchicalAgentState) -> dict: # 解答例よりノード名変更\n",
    "    print(f\"\n",
    "[マネージャー: 実行調整・ワーカー呼び出し]\")\n",
    "    current_sub_tasks = state.get(\"sub_tasks\", [])\n",
    "    updated_sub_tasks = [st.copy() for st in current_sub_tasks] # 解答例より\n",
    "    all_done = True # 解答例より\n",
    "    \n",
    "    for i, task in enumerate(updated_sub_tasks): # 解答例より\n",
    "        if task[\"status\"] == \"pending\":\n",
    "            all_done = False \n",
    "            can_execute = True\n",
    "            dependency_results_for_worker = {}\n",
    "            if task.get(\"dependencies\"):\n",
    "                for dep_id in task[\"dependencies\"]:\n",
    "                    dep_task = next((st for st in updated_sub_tasks if st[\"id\"] == dep_id), None)\n",
    "                    if not dep_task or dep_task[\"status\"] != \"completed\":\n",
    "                        can_execute = False\n",
    "                        print(f\"  サブタスク「{task['id']}」は依存先「{dep_id}」が未完了のため待機します。\")\n",
    "                        break\n",
    "                    dependency_results_for_worker[dep_id] = dep_task.get(\"result\")\n",
    "            \n",
    "            if can_execute:\n",
    "                print(f\"  サブタスク「{task['id']}」({task['instruction']}) をワーカー「{task['assigned_to']}」に実行させます。\")\n",
    "                task_result = simulated_worker_execution(task['assigned_to'], task['instruction'], dependency_results_for_worker)\n",
    "                updated_sub_tasks[i][\"result\"] = task_result\n",
    "                updated_sub_tasks[i][\"status\"] = \"completed\"\n",
    "                break \n",
    "    \n",
    "    if all_done and any(st[\"status\"] == \"completed\" for st in updated_sub_tasks): # 解答例より\n",
    "         print(\"  全サブタスクの処理が完了したか、これ以上進められるタスクがありません。\")\n",
    "         return {\"sub_tasks\": updated_sub_tasks, \"messages\": [AIMessage(content=\"全サブタスクの実行調整が完了。\", name=\"ManagerCoordinator\")]}\n",
    "    \n",
    "    return {\"sub_tasks\": updated_sub_tasks, \"messages\": [AIMessage(content=\"サブタスク実行調整中...\", name=\"ManagerCoordinator\")]}\n",
    "\n",
    "def ____(state: HierarchicalAgentState) -> dict: # 解答例よりノード名変更\n",
    "    print(\"\n",
    "[マネージャー: 結果集約]\")\n",
    "    sub_tasks = state.get(\"sub_tasks\", [])\n",
    "    final_report_parts = [] # 解答例より\n",
    "    for task in sub_tasks:\n",
    "        if task[\"status\"] == \"completed\" and task[\"result\"] is not None: # 解答例より is not None 追加\n",
    "            final_report_parts.append(f\"- 「{task['instruction'][:30]}...」の結果: {task['result'][:70]}...\") # 解答例より\n",
    "        else: # 解答例より\n",
    "            final_report_parts.append(f\"- 「{task['instruction'][:30]}...」は未完了または結果なし (状態: {task['status']})。\") # 解答例より\n",
    "    \n",
    "    final_report = \"統合最終報告書:\\n\" + \"\\n\".join(final_report_parts) # 解答例より\n",
    "    if LLM_PROVIDER != \"fake\" and llm and any(st[\"status\"] == \"completed\" for st in sub_tasks): # 解答例より\n",
    "        prompt = f\"以下のサブタスクの実行結果を元に、ユーザーへの最終報告をまとめてください。\\n{final_report}\"\n",
    "        final_report = llm.invoke(prompt).content\n",
    "\n",
    "    print(f\"  最終報告: {final_report[:200]}...\") # 解答例より出力文字数変更\n",
    "    return {\"final_aggregated_result\": final_report, \"messages\": [AIMessage(content=final_report, name=\"ManagerAggregator\")]} # 解答例より name 変更\n",
    "\n",
    "# --- 4. ルーター関数 ---\n",
    "def ____(state: HierarchicalAgentState) -> str: # 解答例より関数名変更\n",
    "    sub_tasks = state.get(\"sub_tasks\")\n",
    "    if not sub_tasks: \n",
    "        print(\"  -> ルーター(Hierarchical): 計画未作成のため、タスク分解ノードへ。\") # 解答例より\n",
    "        return \"manager_decomposer_node\" \n",
    "    \n",
    "    all_completed = all(task.get(\"status\") == \"completed\" for task in sub_tasks) # 解答例より\n",
    "    \n",
    "    if all_completed: # 解答例より\n",
    "        print(\"  -> ルーター(Hierarchical): 全サブタスク完了。結果集約ノードへ。\")\n",
    "        return \"manager_aggregator_node\"\n",
    "    else: # 解答例より\n",
    "        print(\"  -> ルーター(Hierarchical): 未完了のサブタスクあり。実行調整ノードへ。\")\n",
    "        return \"manager_coordinator_node\" # 実行調整ノードへのキー (解答例より)\n",
    "\n",
    "# --- 5. グラフの構築 ---\n",
    "workflow_q4_ch4 = StateGraph(HierarchicalAgentState)\n",
    "workflow_q4_ch4.add_node(\"manager_decomposer_node\", manager_decomposer_node) # 解答例よりノード名変更\n",
    "workflow_q4_ch4.add_node(\"manager_coordinator_node\", manager_executor_coordinator_node) \n",
    "workflow_q4_ch4.add_node(\"manager_aggregator_node\", manager_aggregator_node) # 解答例よりノード名変更\n",
    "\n",
    "workflow_q4_ch4.____( # 解答例よりエントリーポイント変更\n",
    "    route_hierarchical_manager_actions,\n",
    "    {\n",
    "        \"manager_decomposer_node\": \"manager_decomposer_node\",\n",
    "        \"manager_coordinator_node\": \"manager_coordinator_node\",\n",
    "        \"manager_aggregator_node\": \"manager_aggregator_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow_q4_ch4.add_edge(\"manager_decomposer_node\", \"manager_coordinator_node\") # 解答例より\n",
    "\n",
    "workflow_q4_ch4.add_conditional_edges(\n",
    "    \"manager_coordinator_node\",\n",
    "    route_hierarchical_manager_actions, \n",
    "    {\n",
    "        \"manager_coordinator_node\": \"manager_coordinator_node\", \n",
    "        \"manager_aggregator_node\": \"manager_aggregator_node\",\n",
    "        \"manager_decomposer_node\": \"manager_decomposer_node\" \n",
    "    }\n",
    ")\n",
    "workflow_q4_ch4.add_edge(\"manager_aggregator_node\", END)\n",
    "\n",
    "graph_q4_ch4 = workflow_q4_ch4.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004 - グラフ可視化\n",
    "from IPython.display import Image, display # 解答例より\n",
    "\n",
    "try:\n",
    "    display(Image(graph_q4_ch4.____().____()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフ描画に失敗: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄004 - グラフ実行\n",
    "main_task_q4 = \"東京の天気と主要観光スポット（3箇所）を調べて、それらをまとめた短いレポートを作成してください。\" # 解答例より\n",
    "initial_state_q4_ch4 = {\n",
    "    \"messages\": [____(content=main_task_q4)],\n",
    "    \"____\": main_task_q4,\n",
    "    \"____\": None, \"____\": None # current_worker_index, error_message は削除 (解答例より)\n",
    "}\n",
    "thread_q4 = {\"____\": {\"____\": f\"hierarchical-agent-{uuid4()[:4]}\"}}}\n",
    "\n",
    "print(f\"--- 階層型エージェントテスト (メインタスク: {main_task_q4}) ---\")\n",
    "final_q4_state_val = None\n",
    "for ____, event in ____(graph_q4_ch4.____(initial_state_q4_ch4, config=thread_q4, recursion_limit=15)): # 解答例より event_idx 追加\n",
    "    print(f\"Event {event_idx}: {event}\") # 解答例より event_idx 追加\n",
    "    if END in event: final_q4_state_val = event[END]\n",
    "    print(\"----\");\n",
    "\n",
    "if not final_q4_state_val: final_q4_state_val = graph_q4_ch4.____(thread_q4).values\n",
    "\n",
    "print(\"\n",
    "  最終成果物:\")\n",
    "if final_q4_state_val.get(\"final_aggregated_result\"):\n",
    "    print(f\"    {final_q4_state_val['final_aggregated_result']}\")\n",
    "else:\n",
    "    print(\"    最終成果物が生成されませんでした。\")\n",
    "print(\"\n",
    "  サブタスク状況:\") # 解答例より追加\n",
    "for st in final_q4_state_val.get(\"sub_tasks\",[]): # 解答例より追加\n",
    "    print(f\"    - ID: {st['id']}, ステータス: {st['status']}, 結果: {str(st.get('result','N/A'))[:50]}...\") # 解答例より追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答004</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, List, Optional, Annotated, Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, ToolCall # ToolMessage, ToolCallは直接は使わないが概念として\n",
    "from langchain_core.tools import tool\n",
    "from IPython.display import Image, display\n",
    "from uuid import uuid4\n",
    "import re\n",
    "import json\n",
    "\n",
    "# --- 1. 状態定義 ---\n",
    "class HierarchicalAgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    main_task_description: str\n",
    "    sub_tasks: Optional[List[Dict[str, Any]]]\n",
    "    final_aggregated_result: Optional[str]\n",
    "    # current_worker_indexはマネージャーの実行調整ノード内で管理・更新されるので、状態としては必須ではない\n",
    "    # error_message: Optional[str] # エラー処理を追加する場合\n",
    "\n",
    "# --- 2. ワーカーエージェントのシミュレーション関数 ---\n",
    "# 実際のワーカーは独立したグラフやLLMChainである可能性が高い\n",
    "def simulated_worker_execution(worker_id: str, instruction: str, dependencies_results: Optional[Dict[str, str]] = None) -> str:\n",
    "    print(f\"    [ワーカー {worker_id}] 指示: 「{instruction}」を実行開始。\")\n",
    "    if dependencies_results:\n",
    "        print(f\"      依存結果: {dependencies_results}\")\n",
    "    \n",
    "    # ダミー処理 (LLM呼び出しやツール実行を模倣)\n",
    "    result = f\"ワーカー「{worker_id}」による「{instruction[:20]}...」の高品質な実行結果。\"\n",
    "    if \"天気\" in instruction.lower() and search_tool and search_tool.name != \"dummy_search_tool\" and TAVILY_API_KEY:\n",
    "        try: result = search_tool.invoke(instruction)\n",
    "        except Exception as e: result = f\"天気検索エラー: {e}\"\n",
    "    elif \"レポート作成\" in instruction and dependencies_results:\n",
    "        # 依存結果を使ってレポート作成を模倣\n",
    "        report_content = \"レポート:\\n\"\n",
    "        for task_id, dep_res in dependencies_results.items():\n",
    "            report_content += f\"  - {task_id}の結果: {str(dep_res)[:50]}...\\n\"\n",
    "        result = report_content + \"上記を統合しました。\"\n",
    "    \n",
    "    print(f\"    -> ワーカー {worker_id} の結果: {str(result)[:80]}...\")\n",
    "    return str(result)\n",
    "\n",
    "# --- 3. マネージャーエージェントのノード定義 ---\n",
    "def manager_decomposer_node(state: HierarchicalAgentState) -> dict:\n",
    "    print(f\"\n",
    "[マネージャー: タスク分解] メインタスク: {state['main_task_description']}\")\n",
    "    sub_tasks_list: List[Dict[str, Any]] = []\n",
    "    main_task_lower = state['main_task_description'].lower()\n",
    "    \n",
    "    # ダミータスク分解ロジック (実際はLLMが行う)\n",
    "    if \"天気\" in main_task_lower and (\"観光\" in main_task_lower or \"スポット\" in main_task_lower) and \"レポート\" in main_task_lower:\n",
    "        location = \"東京\" # デフォルト\n",
    "        if \"大阪\" in main_task_lower: location = \"大阪\"\n",
    "        sub_tasks_list.append({'id': 'task_weather', 'instruction': f'{location}の今日の天気調査', 'assigned_to': 'WeatherWorker', 'status': 'pending', 'result': None, 'dependencies': []})\n",
    "        sub_tasks_list.append({'id': 'task_spots', 'instruction': f'{location}の主要観光スポット3箇所調査', 'assigned_to': 'TourismWorker', 'status': 'pending', 'result': None, 'dependencies': []})\n",
    "        sub_tasks_list.append({'id': 'task_report', 'instruction': f'{location}の天気と観光スポットに関する統合レポート作成', 'assigned_to': 'ReportWriter', 'status': 'pending', 'result': None, 'dependencies': ['task_weather', 'task_spots']})\n",
    "    else:\n",
    "        sub_tasks_list.append({'id': 'task_general', 'instruction': state['main_task_description'], 'assigned_to': 'GeneralWorker', 'status': 'pending', 'result': None, 'dependencies': []})\n",
    "        \n",
    "    print(f\"  分解されたサブタスク: {json.dumps(sub_tasks_list, ensure_ascii=False, indent=2)}\")\n",
    "    return {\"sub_tasks\": sub_tasks_list, \"messages\": [AIMessage(content=f\"タスクを{len(sub_tasks_list)}個のサブタスクに分解しました。\", name=\"ManagerDecomposer\")]}\n",
    "\n",
    "def manager_executor_coordinator_node(state: HierarchicalAgentState) -> dict:\n",
    "    print(f\"\n",
    "[マネージャー: 実行調整・ワーカー呼び出し]\")\n",
    "    current_sub_tasks = state.get(\"sub_tasks\", [])\n",
    "    updated_sub_tasks = [st.copy() for st in current_sub_tasks] # 変更用にコピー\n",
    "    all_done = True\n",
    "    \n",
    "    for i, task in enumerate(updated_sub_tasks):\n",
    "        if task[\"status\"] == \"pending\":\n",
    "            all_done = False # まだ保留中のタスクがある\n",
    "            # 依存関係チェック\n",
    "            can_execute = True\n",
    "            dependency_results_for_worker = {}\n",
    "            if task.get(\"dependencies\"):\n",
    "                for dep_id in task[\"dependencies\"]:\n",
    "                    dep_task = next((st for st in updated_sub_tasks if st[\"id\"] == dep_id), None)\n",
    "                    if not dep_task or dep_task[\"status\"] != \"completed\":\n",
    "                        can_execute = False\n",
    "                        print(f\"  サブタスク「{task['id']}」は依存先「{dep_id}」が未完了のため待機します。\")\n",
    "                        break\n",
    "                    dependency_results_for_worker[dep_id] = dep_task.get(\"result\")\n",
    "            \n",
    "            if can_execute:\n",
    "                print(f\"  サブタスク「{task['id']}」({task['instruction']}) をワーカー「{task['assigned_to']}」に実行させます。\")\n",
    "                # ワーカー実行のシミュレーション\n",
    "                task_result = simulated_worker_execution(task['assigned_to'], task['instruction'], dependency_results_for_worker)\n",
    "                updated_sub_tasks[i][\"result\"] = task_result\n",
    "                updated_sub_tasks[i][\"status\"] = \"completed\"\n",
    "                # このループでは1ステップに1タスク実行（または全実行可能タスク実行）とする\n",
    "                # ここでは1つ実行したら一度マネージャーに戻る形にするためbreak\n",
    "                # (より高度な並列実行や依存関係解決は複雑になる)\n",
    "                break \n",
    "    \n",
    "    if all_done and any(st[\"status\"] == \"completed\" for st in updated_sub_tasks): # 全て完了（または初期からタスクなし）\n",
    "         print(\"  全サブタスクの処理が完了したか、これ以上進められるタスクがありません。\")\n",
    "         # next_action を設定してルーターに判断させる (このノードでは直接FINISHさせない)\n",
    "         return {\"sub_tasks\": updated_sub_tasks, \"messages\": [AIMessage(content=\"全サブタスクの実行調整が完了。\", name=\"ManagerCoordinator\")]}\n",
    "    \n",
    "    return {\"sub_tasks\": updated_sub_tasks, \"messages\": [AIMessage(content=\"サブタスク実行調整中...\", name=\"ManagerCoordinator\")]}\n",
    "\n",
    "def manager_aggregator_node(state: HierarchicalAgentState) -> dict:\n",
    "    print(\"\n",
    "[マネージャー: 結果集約]\")\n",
    "    sub_tasks = state.get(\"sub_tasks\", [])\n",
    "    final_report_parts = []\n",
    "    for task in sub_tasks:\n",
    "        if task[\"status\"] == \"completed\" and task[\"result\"] is not None:\n",
    "            final_report_parts.append(f\"- 「{task['instruction'][:30]}...」の結果: {task['result'][:70]}...\")\n",
    "        else:\n",
    "            final_report_parts.append(f\"- 「{task['instruction'][:30]}...」は未完了または結果なし (状態: {task['status']})。\")\n",
    "    \n",
    "    final_report = \"統合最終報告書:\\n\" + \"\\n\".join(final_report_parts)\n",
    "    if LLM_PROVIDER != \"fake\" and llm and any(st[\"status\"] == \"completed\" for st in sub_tasks):\n",
    "        prompt = f\"以下のサブタスクの実行結果を元に、ユーザーへの最終報告をまとめてください。\\n{final_report}\"\n",
    "        final_report = llm.invoke(prompt).content\n",
    "\n",
    "    print(f\"  最終報告: {final_report[:200]}...\")\n",
    "    return {\"final_aggregated_result\": final_report, \"messages\": [AIMessage(content=final_report, name=\"ManagerAggregator\")]}\n",
    "\n",
    "# --- 4. ルーター関数 ---\n",
    "def route_hierarchical_manager_actions(state: HierarchicalAgentState) -> str:\n",
    "    sub_tasks = state.get(\"sub_tasks\")\n",
    "    if not sub_tasks: # 初回、または計画がまだない\n",
    "        print(\"  -> ルーター(Hierarchical): 計画未作成のため、タスク分解ノードへ。\")\n",
    "        return \"manager_decomposer_node\" \n",
    "    \n",
    "    # すべてのタスクが 'completed' になっているか確認\n",
    "    all_completed = all(task.get(\"status\") == \"completed\" for task in sub_tasks)\n",
    "    \n",
    "    if all_completed:\n",
    "        print(\"  -> ルーター(Hierarchical): 全サブタスク完了。結果集約ノードへ。\")\n",
    "        return \"manager_aggregator_node\"\n",
    "    else:\n",
    "        print(\"  -> ルーター(Hierarchical): 未完了のサブタスクあり。実行調整ノードへ。\")\n",
    "        return \"manager_coordinator_node\"\n",
    "\n",
    "# --- 5. グラフの構築 ---\n",
    "workflow_q4_ch4 = StateGraph(HierarchicalAgentState)\n",
    "workflow_q4_ch4.add_node(\"manager_decomposer_node\", manager_decomposer_node)\n",
    "workflow_q4_ch4.add_node(\"manager_coordinator_node\", manager_executor_coordinator_node)\n",
    "workflow_q4_ch4.add_node(\"manager_aggregator_node\", manager_agent_aggregator)\n",
    "\n",
    "# エントリポイントはルーターにし、初期状態に基づいて最初のノードを決定\n",
    "workflow_q4_ch4.set_conditional_entry_point(\n",
    "    route_hierarchical_manager_actions,\n",
    "    {\n",
    "        \"manager_decomposer_node\": \"manager_decomposer_node\",\n",
    "        \"manager_coordinator_node\": \"manager_coordinator_node\", # 通常ここには直接来ないはず\n",
    "        \"manager_aggregator_node\": \"manager_aggregator_node\" # 通常ここには直接来ないはず\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow_q4_ch4.add_edge(\"manager_decomposer_node\", \"manager_coordinator_node\") # 分解後は必ず調整へ\n",
    "workflow_q4_ch4.add_conditional_edges(\n",
    "    \"manager_coordinator_node\",\n",
    "    route_hierarchical_manager_actions, # 実行調整後、再度ルーターで判断\n",
    "    {\n",
    "        \"manager_coordinator_node\": \"manager_coordinator_node\", # 未完了タスクあれば再度調整 (ループ)\n",
    "        \"manager_aggregator_node\": \"manager_aggregator_node\",\n",
    "        \"manager_decomposer_node\": \"manager_decomposer_node\" # 基本ここには来ない想定\n",
    "    }\n",
    ")\n",
    "workflow_q4_ch4.add_edge(\"manager_aggregator_node\", END)\n",
    "\n",
    "graph_q4_ch4 = workflow_q4_ch4.compile()\n",
    "try: display(Image(graph_q4_ch4.get_graph().draw_png()))\n",
    "except Exception as e: print(f\"グラフ描画失敗: {e}\")\n",
    "\n",
    "# --- 6. グラフの実行 ---\n",
    "main_task_q4 = \"東京の天気と主要観光スポット（3箇所）を調べて、それらをまとめた短いレポートを作成してください。\"\n",
    "initial_state_q4_ch4 = {\n",
    "    \"messages\": [HumanMessage(content=main_task_q4)],\n",
    "    \"main_task_description\": main_task_q4,\n",
    "    \"sub_tasks\": None, \"final_aggregated_result\": None\n",
    "}\n",
    "thread_q4 = {\"configurable\": {\"thread_id\": f\"hierarchical-agent-{uuid4()[:4]}\"}}}\n",
    "\n",
    "print(f\"--- 階層型エージェントテスト (メインタスク: {main_task_q4}) ---\")\n",
    "final_q4_state_val = None\n",
    "for event_idx, event in enumerate(graph_q4_ch4.stream(initial_state_q4_ch4, config=thread_q4, recursion_limit=15)):\n",
    "    print(f\"Event {event_idx}: {event}\")\n",
    "    if END in event: final_q4_state_val = event[END]\n",
    "    print(\"----\");\n",
    "\n",
    "if not final_q4_state_val: final_q4_state_val = graph_q4_ch4.get_state(thread_q4).values\n",
    "\n",
    "print(\"\n",
    "  最終成果物:\")\n",
    "if final_q4_state_val.get(\"final_aggregated_result\"):\n",
    "    print(f\"    {final_q4_state_val['final_aggregated_result']}\")\n",
    "else:\n",
    "    print(\"    最終成果物が生成されませんでした。\")\n",
    "print(\"\n",
    "  サブタスク状況:\")\n",
    "for st in final_q4_state_val.get(\"sub_tasks\",[]):\n",
    "    print(f\"    - ID: {st['id']}, ステータス: {st['status']}, 結果: {str(st.get('result','N/A'))[:50]}...\")\n",
    "``````\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解説004</summary>\n",
    "\n",
    "#### この問題のポイント\n",
    "\n",
    "*   **階層構造の役割分担:**\n",
    "    *   **マネージャーエージェント:** 複数のノード（`manager_decomposer_node`、`manager_executor_coordinator_node`、`manager_aggregator_node`）にまたがって実装されています。\n",
    "        *   `manager_decomposer_node`: メインのタスクを受け取り、それを実行可能なサブタスクのリストに分解（プランニング）します。各サブタスクには、指示内容、担当ワーカー（想定）、依存関係などが含まれます。\n",
    "        *   `manager_executor_coordinator_node`: 分解されたサブタスクの実行を調整・管理します。どのサブタスクを次に実行すべきか（依存関係を考慮しつつ）、どのワーカーに委任するかを決定し、ワーカーの実行を（この例ではシミュレーションで）トリガーします。全てのサブタスクが完了するまで、この調整役を繰り返します。\n",
    "        *   `manager_aggregator_node`: 全てのサブタスクが完了した後、それらの結果を収集・統合し、最終的な成果物（レポートなど）を生成します。\n",
    "    *   **ワーカーエージェント（`simulated_worker_execution`）:** マネージャーから委任された特定のサブタスクを実行します。この解答例では、ワーカーは独立したノードではなく、マネージャーの調整ノード内で呼び出されるシミュレーション関数として実装されていますが、実際のシステムでは各ワーカーが専用のツールや特化LLMを持つ独立したグラフやLangChainのChainとして実装されることが多いです。\n",
    "*   **状態 (`HierarchicalAgentState`):**\n",
    "    *   `main_task_description`: マネージャーが最初に受け取るタスク。\n",
    "    *   `sub_tasks`: マネージャーが生成したサブタスクのリスト。各要素は辞書で、サブタスクID、指示、担当、現在の状態（pending, completed）、実行結果、依存タスクIDリストなどを持ちます。\n",
    "    *   `final_aggregated_result`: マネージャーが集約して作成した最終成果物。\n",
    "*   **グラフのフローとロジック:**\n",
    "    1.  エントリーポイントはルーター `route_hierarchical_manager_actions` ですが、初期状態では `sub_tasks` がないため、`manager_decomposer_node` に処理が移ります。\n",
    "    2.  `manager_decomposer_node` がサブタスクリストを作成します。\n",
    "    3.  その後、`manager_coordinator_node` が呼び出されます。このノードは `sub_tasks` リストをイテレートし、実行可能な（依存関係が解決された）ペンディング中のタスクを見つけて、（シミュレーションで）ワーカーに実行させ、結果を `sub_tasks` 内の対応するタスクに記録し、ステータスを \"completed\" に更新します。この調整処理は、全てのタスクが完了するまでルーターを介して繰り返されます。\n",
    "    4.  `route_hierarchical_manager_actions` ルーターは、全てのサブタスクが完了したと判断すると、`manager_aggregator_node` に処理を移します。\n",
    "    5.  `manager_aggregator_node` が全サブタスクの結果を統合し、最終成果物を作成して `END` で終了します。\n",
    "*   **依存関係の簡易処理:** この解答例では、サブタスク間の依存関係（例: レポート作成は天気調査と観光スポット調査が完了してから）を `dependencies` キーで表現し、`manager_executor_coordinator_node` が簡易的にチェックしています。より堅牢なシステムでは、タスクスケジューリングや依存関係解決のためのより洗練されたロジックが必要になります。\n",
    "*   **ワーカーの抽象化:** `simulated_worker_execution` 関数は、様々な種類のワーカーの処理を模倣しています。実際には、各ワーカー（例: `WeatherWorker`, `TourismWorker`, `ReportWriter`）は、それぞれ特化したツールやプロンプトを持つLLM、あるいは専用のLangGraphサブグラフとして実装されるでしょう。\n",
    "\n",
    "この階層型アプローチは、複雑な問題を管理しやすく、各コンポーネントの専門性を高めることができるため、高度なAIエージェントシステムの設計において非常に強力なパラダイムです。\n",
    "\n",
    "---</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 問題005: 第4章のまとめ - 複数エージェントによる協調型リサーチボット\n",
    "\n",
    "第4章で学んだマルチエージェントのコンセプト（基本的な2エージェント会話、スーパーバイザーによるタスク割り振り、逐次連携、階層型委任）を組み合わせ、より洗練された「協調型リサーチボット」を構築します。このボットは以下の役割を持つエージェントで構成されることを目指します（全てを完全に実装する必要はなく、主要な連携フローを示すことを目標とします）。\n",
    "\n",
    "1.  **ユーザーインターフェースエージェント（UI Agent）:** ユーザーからのリサーチ要求を受け付け、明確化する。\n",
    "2.  **プランニングスーパーバイザーエージェント（Planner Supervisor）:** 明確化された要求に基づき、リサーチ計画（必要な情報収集タスク、分析タスクなど）を立案し、適切な専門エージェントにタスクを割り振る。\n",
    "3.  **専門リサーチエージェント（Specialist Researcher(s)）:** 特定の種類の情報収集（例: ウェブ検索、データベース検索、特定APIからのデータ取得など）を担当する。複数のリサーチャーが並行して動作することも考えられる。\n",
    "4.  **データ分析エージェント（Data Analyst）:** 収集された情報を分析・統合し、洞察を抽出する。\n",
    "5.  **レポート生成エージェント（Report Generator）:** 分析結果と洞察に基づいて、最終的なレポートを作成する。\n",
    "6.  （オプション）**レビュー・品質管理エージェント（Reviewer Agent）:** 生成されたレポートの品質をチェックし、必要であれば修正を指示する。\n",
    "\n",
    "*   **学習内容:**\n",
    "    *   複数の異なる役割と能力を持つエージェントを定義し、それらを一つの状態（State）とグラフ定義の下で連携させる複雑なワークフローの設計。\n",
    "    *   スーパーバイザーを中心としたタスクの委任、専門エージェントによる処理、結果の報告と統合、というマルチエージェントシステムの典型的なパターンの実装。\n",
    "    *   状態（State）の設計が、エージェント間の情報共有とワークフロー全体の制御においていかに重要であるかの再確認。\n",
    "    *   （概念的に）エラーハンドリングや、特定条件下での処理の分岐（例: リサーチ結果が不十分なら追加リサーチ）なども組み込む余地があることの理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005 - グラフ構築\n",
    "from typing import TypedDict, List, Optional, Annotated, Dict, Any, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "from uuid import uuid4\n",
    "import re # 解答例より\n",
    "\n",
    "# --- 0. 共通ツールの準備 (search_toolは準備セルで定義済み) ---\n",
    "# @tool def summarize_text(text: str) -> str: ... (必要なら定義)\n",
    "\n",
    "# --- 1. 状態定義 ---\n",
    "____ = Literal[\n",
    "    \"CLARIFYING_REQUEST\", \"PLANNING\", \"RESEARCHING\", \"ANALYZING\", \n",
    "    \"REPORTING\", \"REVIEWING\", \"FINALIZING\", \"DONE\", \"ERROR\"\n",
    "]\n",
    "\n",
    "class ____(____):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    user_original_request: str\n",
    "    ____: Optional[str]\n",
    "    ____: Optional[List[Dict[str, Any]]] \n",
    "    ____: Dict[str, Any] \n",
    "    analysis_summary: Optional[str]\n",
    "    report_draft: Optional[str]\n",
    "    review_feedback: Optional[str]\n",
    "    final_report: Optional[str]\n",
    "    current_phase: Optional[ResearchPhase]\n",
    "    ____: Optional[str] \n",
    "    error_message_for_chapter4: Optional[str] \n",
    "\n",
    "# --- 2. 各エージェント/ノードの簡易的な実装 ---\n",
    "def ____(state: CollaborativeResearchBotState) -> dict:\n",
    "    print(f\"\n",
    "[UIエージェント] 元のリクエスト: {state['user_original_request']}\")\n",
    "    clarified = state['user_original_request']\n",
    "    print(f\"  -> 明確化されたリクエスト (今回は変更なし): {clarified}\") # 解答例より\n",
    "    return {\"clarified_request\": clarified, \"current_phase\": \"PLANNING\", \"messages\": [AIMessage(content=f\"リクエスト「{clarified}」として承りました。計画を作成します。\", name=\"UIAgent\")]}\n",
    "\n",
    "def ____(state: CollaborativeResearchBotState) -> dict:\n",
    "    req = state.get(\"clarified_request\", state[\"user_original_request\"]) # 解答例より\n",
    "    print(f\"\n",
    "[プランニングスーパーバイザー] リクエスト「{req}」の計画立案開始。\")\n",
    "    plan: List[Dict[str, Any]] = [] # 解答例より\n",
    "    if \"LangGraph\" in req and \"マルチエージェント\" in req and (\"ベストプラクティス\" in req or \"レポート\" in req): # 解答例より\n",
    "        plan.append({'task_id': 'lg_ma_bp_search', 'instruction': 'LangGraphマルチエージェントのベストプラクティスに関する情報をウェブで検索', 'assigned_to': 'WebSearcher', 'status': 'pending', 'result': None, 'dependencies':[]})\n",
    "        plan.append({'task_id': 'lg_ma_ex_search', 'instruction': 'LangGraphマルチエージェントの具体的なコード例や応用事例を検索', 'assigned_to': 'WebSearcher', 'status': 'pending', 'result': None, 'dependencies':[]})\n",
    "        plan.append({'task_id': 'lg_ma_analysis', 'instruction': '上記検索結果(ベストプラクティスと事例)を統合・分析し、主要な設計パターンと利点を抽出', 'assigned_to': 'DataAnalyst', 'status': 'pending', 'result': None, 'dependencies':['lg_ma_bp_search', 'lg_ma_ex_search']})\n",
    "        plan.append({'task_id': 'lg_ma_reporting', 'instruction': '分析結果に基づき、LangGraphマルチエージェントのベストプラクティスに関する簡潔なレポートを作成', 'assigned_to': 'ReportGenerator', 'status': 'pending', 'result': None, 'dependencies':['lg_ma_analysis']})\n",
    "    else:\n",
    "        plan.append({'task_id': 'generic_search_main', 'instruction': f'{req}についてウェブで包括的に検索', 'assigned_to': 'WebSearcher', 'status': 'pending', 'result': None, 'dependencies':[]}) # 解答例より\n",
    "        plan.append({'task_id': 'generic_report_main', 'instruction': f'{req}の検索結果を元に要点をまとめたレポートを作成', 'assigned_to': 'ReportGenerator', 'status': 'pending', 'result': None, 'dependencies':['generic_search_main']}) # 解答例より\n",
    "    print(f\"  -> 生成された計画: {json.dumps(plan, ensure_ascii=False, indent=2)}\")\n",
    "    return {\"research_plan\": plan, \"current_phase\": \"RESEARCHING\", \"collected_data\": {}, \"messages\": [AIMessage(content=f\"計画を立案しました: {len(plan)}ステップ。リサーチを開始します。\", name=\"PlannerSupervisor\")]} # 解答例より\n",
    "\n",
    "def web_searcher_worker(state: CollaborativeResearchBotState) -> dict:\n",
    "    task_id = state.get(\"current_task_id_being_processed\")\n",
    "    plan = state.get(\"research_plan\", [])\n",
    "    task = next((t for t in plan if t['task_id'] == task_id), None)\n",
    "    if not task: return {\"error_message_for_chapter4\": f\"タスクID {task_id} がプランに見つかりません。\"}\n",
    "    \n",
    "    print(f\"\n",
    "[WebSearcherワーカー] タスク「{task['instruction']}」を実行します。\")\n",
    "    search_query = task['instruction']\n",
    "    search_result_content = f\"「{search_query}」に関するウェブ検索結果: 多数の関連情報が見つかりました。主要な情報源はA, B, Cです。\" # 解答例より\n",
    "    if search_tool and search_tool.name != \"dummy_search_tool\" and TAVILY_API_KEY:\n",
    "        try: search_result_content = str(search_tool.invoke({\"query\": search_query})) # 解答例より\n",
    "        except Exception as e: search_result_content = f\"検索エラー({search_query}): {e}\"\n",
    "    elif search_tool: search_result_content = str(search_tool.invoke(search_query)) # 解答例より\n",
    "    \n",
    "    print(f\"  -> 検索結果 ({task_id}): {search_result_content[:100]}...\")\n",
    "    updated_collected_data = state.get(\"collected_data\", {}).copy()\n",
    "    updated_collected_data[task_id] = search_result_content\n",
    "    updated_plan = [p.copy() for p in plan] # 解答例より\n",
    "    for p_task in updated_plan: \n",
    "        if p_task['task_id'] == task_id: p_task['status'] = 'completed'; p_task['result'] = search_result_content; break\n",
    "    return {\"collected_data\": updated_collected_data, \"research_plan\": updated_plan, \"messages\": [AIMessage(content=f\"検索タスク「{task_id}」完了。結果を保存しました。\", name=\"WebSearcher\")]} # 解答例より\n",
    "\n",
    "def data_analyst_worker(state: CollaborativeResearchBotState) -> dict: \n",
    "    task_id = state.get(\"current_task_id_being_processed\")\n",
    "    plan = state.get(\"research_plan\", [])\n",
    "    task = next((t for t in plan if t['task_id'] == task_id), None)\n",
    "    if not task: return {\"error_message_for_chapter4\": f\"タスクID {task_id} がプランに見つかりません。\"} # 解答例より\n",
    "    print(f\"\n",
    "[DataAnalystワーカー] タスク「{task['instruction']}」を実行します。\")\n",
    "    input_for_analysis = \"\"\n",
    "    for dep_id in task.get('dependencies', []):\n",
    "        input_for_analysis += f\"依存データ({dep_id}): {str(state.get('collected_data',{}).get(dep_id,'データなし'))[:100]}...\\n\" # 解答例より\n",
    "    analysis = f\"分析結果 ({task['instruction']}):\\n{input_for_analysis}上記データから、重要な洞察として「パターンX」と「傾向Y」が抽出されました。これらは相互に関連しています。\" # 解答例より\n",
    "    print(f\"  -> 分析結果 ({task_id}): {analysis}\")\n",
    "    updated_collected_data = state.get(\"collected_data\", {}).copy()\n",
    "    updated_collected_data[task_id] = analysis \n",
    "    updated_plan = [p.copy() for p in plan] # 解答例より\n",
    "    for p_task in updated_plan: \n",
    "        if p_task['task_id'] == task_id: p_task['status'] = 'completed'; p_task['result'] = analysis; break\n",
    "    return {\"collected_data\": updated_collected_data, \"research_plan\": updated_plan, \"analysis_summary\": analysis, \"messages\": [AIMessage(content=f\"分析タスク「{task_id}」完了。サマリーを生成しました。\", name=\"DataAnalyst\")]} # 解答例より\n",
    "\n",
    "def report_generator_worker(state: CollaborativeResearchBotState) -> dict:\n",
    "    task_id = state.get(\"current_task_id_being_processed\")\n",
    "    plan = state.get(\"research_plan\", [])\n",
    "    task = next((t for t in plan if t['task_id'] == task_id), None)\n",
    "    if not task: return {\"error_message_for_chapter4\": f\"タスクID {task_id} がプランに見つかりません。\"} # 解答例より\n",
    "    print(f\"\n",
    "[ReportGeneratorワーカー] タスク「{task['instruction']}」を実行します。\")\n",
    "    input_for_report = \"\"\n",
    "    for dep_id in task.get('dependencies', []):\n",
    "        input_for_report += f\"参照データ({dep_id}): {str(state.get('collected_data',{}).get(dep_id,'データなし'))[:100]}...\\n\" # 解答例より\n",
    "    report = f\"最終レポート: {task['instruction']}\\n{input_for_report}上記情報を総合的に判断し、以下に要点をまとめました。\\n1. 主要な発見点A\\n2. 注目すべき傾向B\\n3. 今後の課題C\\n以上が本件に関する報告です。\" # 解答例より\n",
    "    print(f\"  -> レポートドラフト ({task_id}): {report[:150]}...\")\n",
    "    updated_plan = [p.copy() for p in plan] # 解答例より\n",
    "    for p_task in updated_plan: \n",
    "        if p_task['task_id'] == task_id: p_task['status'] = 'completed'; p_task['result'] = report; break\n",
    "    return {\"report_draft\": report, \"research_plan\": updated_plan, \"messages\": [AIMessage(content=f\"レポート生成タスク「{task_id}」完了。\", name=\"ReportGenerator\")]} # 解答例より\n",
    "\n",
    "def ____(state: CollaborativeResearchBotState) -> dict:\n",
    "    print(f\"\n",
    "[統括スーパーバイザー] 現在フェーズ: {state.get('current_phase')}, 直前処理タスクID: {state.get('current_task_id_being_processed')}\") # 解答例より\n",
    "    current_phase = state.get(\"current_phase\")\n",
    "    plan = state.get(\"research_plan\", [])\n",
    "    next_phase: Optional[ResearchPhase] = None\n",
    "    next_task_id_to_process: Optional[str] = None # 解答例より変数名変更\n",
    "    supervisor_log = \"\" # 解答例より\n",
    "\n",
    "    if state.get(\"error_message_for_chapter4\"): next_phase = \"ERROR\"; supervisor_log = f\"エラー発生: {state['error_message_for_chapter4']}\" # 解答例より\n",
    "    elif current_phase == \"PLANNING\": next_phase = \"RESEARCHING\"; supervisor_log = \"計画に基づきリサーチ開始。\"\n",
    "    elif current_phase == \"RESEARCHING\":\n",
    "        pending_task = next((t for t in plan if t['status'] == 'pending' and t['assigned_to'] == 'WebSearcher'), None)\n",
    "        if pending_task: next_task_id_to_process = pending_task['task_id']; next_phase = \"RESEARCHING\"; supervisor_log = f\"リサーチタスク「{next_task_id_to_process}」へ。\"\n",
    "        else: next_phase = \"ANALYZING\"; supervisor_log = \"全リサーチ完了。分析へ。\"\n",
    "    elif current_phase == \"ANALYZING\":\n",
    "        pending_task = next((t for t in plan if t['status'] == 'pending' and t['assigned_to'] == \"DataAnalyst\"), None)\n",
    "        if pending_task:\n",
    "            deps_completed = all(state.get(\"collected_data\",{}).get(dep_id) for dep_id in pending_task.get('dependencies',[])) # 解答例より\n",
    "            if deps_completed: next_task_id_to_process = pending_task['task_id']; next_phase = \"ANALYZING\"; supervisor_log = f\"分析タスク「{next_task_id_to_process}」へ。\"\n",
    "            else: supervisor_log = f\"分析タスク「{pending_task['task_id']}」の依存未解決。待機。\"\n",
    "        else: next_phase = \"REPORTING\"; supervisor_log = \"全分析完了。レポート作成へ。\"\n",
    "    elif current_phase == \"REPORTING\":\n",
    "        pending_task = next((t for t in plan if t['status'] == 'pending' and t['assigned_to'] == \"ReportGenerator\"), None)\n",
    "        if pending_task:\n",
    "            deps_completed = all(state.get(\"collected_data\",{}).get(dep_id) for dep_id in pending_task.get('dependencies',[])) # 解答例より\n",
    "            if deps_completed: next_task_id_to_process = pending_task['task_id']; next_phase = \"REPORTING\"; supervisor_log = f\"レポートタスク「{next_task_id_to_process}」へ。\"\n",
    "            else: supervisor_log = f\"レポートタスク「{pending_task['task_id']}」の依存未解決。待機。\"\n",
    "        else: next_phase = \"DONE\"; supervisor_log = \"全レポート作成完了。終了。\"; return {\"current_phase\": next_phase, \"final_report\": state.get(\"report_draft\"), \"messages\":[AIMessage(content=supervisor_log, name=\"OverallSupervisor\")]} # 解答例より\n",
    "    \n",
    "    if next_phase:\n",
    "        print(f\"  -> 統括SV判断: 次フェーズ「{next_phase}」{', タスクID「'+next_task_id_to_process+'」' if next_task_id_to_process else ''}. ログ: {supervisor_log}\") # 解答例より\n",
    "        return {\"current_phase\": next_phase, \"current_task_id_being_processed\": next_task_id_to_process, \"messages\": [AIMessage(content=supervisor_log, name=\"OverallSupervisor\")]}\n",
    "    \n",
    "    print(f\"  -> 統括SV判断: フェーズ「{current_phase}」で待機または進展なし。\") # 解答例より\n",
    "    return {\"messages\": [AIMessage(content=f\"フェーズ「{current_phase}」で待機中。\", name=\"OverallSupervisor\")]} # 解答例より\n",
    "\n",
    "# --- 3. ルーター定義 ---\n",
    "def ____(state: CollaborativeResearchBotState) -> str:\n",
    "    phase = state.get(\"current_phase\")\n",
    "    task_id = state.get(\"current_task_id_being_processed\")\n",
    "    plan = state.get(\"research_plan\", [])\n",
    "    print(f\"  -> ルーター(PhaseRouter): フェーズ「{phase}」、タスクID「{task_id}」\")\n",
    "\n",
    "    if phase == \"ERROR\": return END \n",
    "    if phase == \"DONE\": return END\n",
    "\n",
    "    if task_id: # 解答例より\n",
    "        task_to_run = next((t for t in plan if t['task_id'] == task_id and t['status'] == 'pending'), None)\n",
    "        if task_to_run:\n",
    "            assigned_worker = task_to_run['assigned_to']\n",
    "            if assigned_worker == \"WebSearcher\": return \"web_searcher\"\n",
    "            if assigned_worker == \"DataAnalyst\": return \"data_analyst\"\n",
    "            if assigned_worker == \"ReportGenerator\": return \"report_generator\"\n",
    "    \n",
    "    return \"overall_supervisor\" # 解答例より\n",
    "\n",
    "# --- 4. グラフ構築 ---\n",
    "workflow_q5_ch4 = StateGraph(CollaborativeResearchBotState)\n",
    "workflow_q5_ch4.add_node(\"ui_agent\", ui_agent_clarifier)\n",
    "workflow_q5_ch4.add_node(\"planner_supervisor\", planning_supervisor_agent)\n",
    "workflow_q5_ch4.add_node(\"overall_supervisor\", overall_supervisor_node)\n",
    "workflow_q5_ch4.add_node(\"web_searcher\", web_searcher_worker)\n",
    "workflow_q5_ch4.add_node(\"data_analyst\", data_analyst_worker)\n",
    "workflow_q5_ch4.add_node(\"report_generator\", report_generator_worker)\n",
    "\n",
    "workflow_q5_ch4.set_entry_point(\"ui_agent\")\n",
    "workflow_q5_ch4.add_edge(\"ui_agent\", \"planner_supervisor\") \n",
    "workflow_q5_ch4.add_edge(\"planner_supervisor\", \"overall_supervisor\") \n",
    "\n",
    "workflow_q5_ch4.add_conditional_edges(\"overall_supervisor\", route_by_current_phase,\n",
    "    {\"web_searcher\": \"web_searcher\", \"data_analyst\": \"data_analyst\", \n",
    "     \"report_generator\": \"report_generator\", \"overall_supervisor\": \"overall_supervisor\", END: END}) # 解答例より\n",
    "\n",
    "workflow_q5_ch4.add_edge(\"web_searcher\", \"overall_supervisor\") # 解答例より\n",
    "workflow_q5_ch4.add_edge(\"data_analyst\", \"overall_supervisor\") # 解答例より\n",
    "workflow_q5_ch4.add_edge(\"report_generator\", \"overall_supervisor\") # 解答例より\n",
    "\n",
    "graph_q5_ch4 = workflow_q5_ch4.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005 - グラフ可視化\n",
    "from IPython.display import Image, display # 解答例より\n",
    "\n",
    "try:\n",
    "    display(Image(graph_q5_ch4.____().____()))\n",
    "except Exception as e:\n",
    "    print(f\"グラフの可視化に失敗しました。Graphvizが正しくインストールされているか確認してください。エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答欄005 - グラフ実行\n",
    "user_main_request_q5 = \"LangGraphを使ったマルチエージェントシステム構築のベストプラクティスについて包括的に調査し、その結果を詳細な技術レポートとしてまとめてください。\" # 解答例より\n",
    "initial_state_q5_ch4 = {\n",
    "    \"messages\": [____(content=user_main_request_q5)], \"____\": user_main_request_q5, # 解答例より\n",
    "    \"clarified_request\": None, \"____\": None, \"____\": {},\n",
    "    \"____\": None, \"____\": None, \"review_feedback\": None,\n",
    "    \"____\": None, \"____\": None, \n",
    "    \"current_task_id_being_processed\": None, \"error_message_for_chapter4\": None\n",
    "}\n",
    "thread_q5 = {\"____\": {\"thread_id\": f\"collab-research-bot-{uuid4()[:4]}\"}}}\n",
    "\n",
    "print(f\"--- 協調型リサーチボットテスト (リクエスト: {user_main_request_q5}) ---\")\n",
    "final_q5_val = None\n",
    "for event_idx, event in enumerate(graph_q5_ch4.____(initial_state_q5_ch4, config=thread_q5, recursion_limit=25)): # 解答例より recursion_limit変更\n",
    "    print(f\"Event {event_idx}: {event}\") # 解答例より\n",
    "    if END in event: final_q5_val = event[END]\n",
    "    print(\"----\");\n",
    "\n",
    "if not final_q5_val: final_q5_val = graph_q5_ch4.get_state(thread_q5).values\n",
    "\n",
    "print(\"\n",
    "  最終成果物:\")\n",
    "if final_q5_val.get(\"final_report\"):\n",
    "    print(f\"    {final_q5_val['final_report']}\")\n",
    "elif final_q5_val.get(\"error_message_for_chapter4\"):\n",
    "    print(f\"    エラー終了: {final_q5_val['error_message_for_chapter4']}\")\n",
    "else:\n",
    "    print(f\"    最終成果物がありませんでした。最終フェーズ: {final_q5_val.get('current_phase')}\")\n",
    "print(\"\n",
    "  収集データ概要:\")\n",
    "for task_id, data_item in final_q5_val.get(\"collected_data\", {}).items():\n",
    "    print(f\"    - {task_id}: {str(data_item)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>解答005</summary>\n",
    "\n",
    "``````python\n",
    "from typing import TypedDict, List, Optional, Annotated, Dict, Any, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "from uuid import uuid4\n",
    "from IPython.display import Image, display\n",
    "import re\n",
    "\n",
    "# --- 0. 共通ツールの準備 (search_toolは準備セルで定義済み) ---\n",
    "\n",
    "# --- 1. 状態定義 ---\n",
    "ResearchPhase = Literal[\n",
    "    \"CLARIFYING_REQUEST\", \"PLANNING\", \"RESEARCHING\", \"ANALYZING\", \n",
    "    \"REPORTING\", \"REVIEWING\", \"FINALIZING\", \"DONE\", \"ERROR\"\n",
    "]\n",
    "\n",
    "class CollaborativeResearchBotState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    user_original_request: str\n",
    "    clarified_request: Optional[str]\n",
    "    research_plan: Optional[List[Dict[str, Any]]]\n",
    "    collected_data: Dict[str, Any] \n",
    "    analysis_summary: Optional[str]\n",
    "    report_draft: Optional[str]\n",
    "    review_feedback: Optional[str]\n",
    "    final_report: Optional[str]\n",
    "    current_phase: Optional[ResearchPhase]\n",
    "    current_task_id_being_processed: Optional[str]\n",
    "    error_message_for_chapter4: Optional[str]\n",
    "\n",
    "# --- 2. 各エージェント/ノードの簡易的な実装 ---\n",
    "def ui_agent_clarifier(state: CollaborativeResearchBotState) -> dict:\n",
    "    print(f\"\n",
    "[UIエージェント] 元のリクエスト: {state['user_original_request']}\")\n",
    "    clarified = state['user_original_request']\n",
    "    # if \"あいまい\" in clarified.lower(): clarified = clarified.replace(\"あいまいな\", \"明確な\") + \" (明確化済み)\"\n",
    "    print(f\"  -> 明確化されたリクエスト (今回は変更なし): {clarified}\")\n",
    "    return {\"clarified_request\": clarified, \"current_phase\": \"PLANNING\", \"messages\": [AIMessage(content=f\"リクエスト「{clarified}」として承りました。計画を作成します。\", name=\"UIAgent\")]}\n",
    "\n",
    "def planning_supervisor_agent(state: CollaborativeResearchBotState) -> dict:\n",
    "    req = state.get(\"clarified_request\", state[\"user_original_request\"])\n",
    "    print(f\"\n",
    "[プランニングスーパーバイザー] リクエスト「{req}」の計画立案開始。\")\n",
    "    plan: List[Dict[str, Any]] = []\n",
    "    if \"LangGraph\" in req and \"マルチエージェント\" in req and (\"ベストプラクティス\" in req or \"レポート\" in req):\n",
    "        plan.append({'task_id': 'lg_ma_bp_search', 'instruction': 'LangGraphマルチエージェントのベストプラクティスに関する情報をウェブで検索', 'assigned_to': 'WebSearcher', 'status': 'pending', 'result': None, 'dependencies':[]})\n",
    "        plan.append({'task_id': 'lg_ma_ex_search', 'instruction': 'LangGraphマルチエージェントの具体的なコード例や応用事例を検索', 'assigned_to': 'WebSearcher', 'status': 'pending', 'result': None, 'dependencies':[]})\n",
    "        plan.append({'task_id': 'lg_ma_analysis', 'instruction': '上記検索結果(ベストプラクティスと事例)を統合・分析し、主要な設計パターンと利点を抽出', 'assigned_to': 'DataAnalyst', 'status': 'pending', 'result': None, 'dependencies':['lg_ma_bp_search', 'lg_ma_ex_search']})\n",
    "        plan.append({'task_id': 'lg_ma_reporting', 'instruction': '分析結果に基づき、LangGraphマルチエージェントのベストプラクティスに関する簡潔なレポートを作成', 'assigned_to': 'ReportGenerator', 'status': 'pending', 'result': None, 'dependencies':['lg_ma_analysis']})\n",
    "    else:\n",
    "        plan.append({'task_id': 'generic_search_main', 'instruction': f'{req}についてウェブで包括的に検索', 'assigned_to': 'WebSearcher', 'status': 'pending', 'result': None, 'dependencies':[]})\n",
    "        plan.append({'task_id': 'generic_report_main', 'instruction': f'{req}の検索結果を元に要点をまとめたレポートを作成', 'assigned_to': 'ReportGenerator', 'status': 'pending', 'result': None, 'dependencies':['generic_search_main']})\n",
    "    print(f\"  -> 生成された計画: {json.dumps(plan, ensure_ascii=False, indent=2)}\")\n",
    "    return {\"research_plan\": plan, \"current_phase\": \"RESEARCHING\", \"collected_data\": {}, \"messages\": [AIMessage(content=f\"計画を立案しました: {len(plan)}ステップ。リサーチを開始します。\", name=\"PlannerSupervisor\")]}\n",
    "\n",
    "def web_searcher_worker(state: CollaborativeResearchBotState) -> dict:\n",
    "    task_id = state.get(\"current_task_id_being_processed\")\n",
    "    plan = state.get(\"research_plan\", [])\n",
    "    task = next((t for t in plan if t['task_id'] == task_id), None)\n",
    "    if not task: return {\"error_message_for_chapter4\": f\"タスクID {task_id} がプランに見つかりません。\"}\n",
    "    \n",
    "    print(f\"\n",
    "[WebSearcherワーカー] タスク「{task['instruction']}」を実行します。\")\n",
    "    search_query = task['instruction']\n",
    "    search_result_content = f\"「{search_query}」に関するウェブ検索結果: 多数の関連情報が見つかりました。主要な情報源はA, B, Cです。\"\n",
    "    if search_tool and search_tool.name != \"dummy_search_tool\" and TAVILY_API_KEY:\n",
    "        try: search_result_content = str(search_tool.invoke({\"query\": search_query})) # Tavilyはdictを期待\n",
    "        except Exception as e: search_result_content = f\"検索エラー({search_query}): {e}\"\n",
    "    elif search_tool: search_result_content = str(search_tool.invoke(search_query))\n",
    "    \n",
    "    print(f\"  -> 検索結果 ({task_id}): {search_result_content[:100]}...\")\n",
    "    updated_collected_data = state.get(\"collected_data\", {}).copy()\n",
    "    updated_collected_data[task_id] = search_result_content\n",
    "    updated_plan = [p.copy() for p in plan]\n",
    "    for p_task in updated_plan: \n",
    "        if p_task['task_id'] == task_id: p_task['status'] = 'completed'; p_task['result'] = search_result_content; break\n",
    "    return {\"collected_data\": updated_collected_data, \"research_plan\": updated_plan, \"messages\": [AIMessage(content=f\"検索タスク「{task_id}」完了。結果を保存しました。\", name=\"WebSearcher\")]}\n",
    "\n",
    "def data_analyst_worker(state: CollaborativeResearchBotState) -> dict: \n",
    "    task_id = state.get(\"current_task_id_being_processed\")\n",
    "    plan = state.get(\"research_plan\", [])\n",
    "    task = next((t for t in plan if t['task_id'] == task_id), None)\n",
    "    if not task: return {\"error_message_for_chapter4\": f\"タスクID {task_id} がプランに見つかりません。\"}\n",
    "    print(f\"\n",
    "[DataAnalystワーカー] タスク「{task['instruction']}」を実行します。\")\n",
    "    input_for_analysis = \"\"\n",
    "    for dep_id in task.get('dependencies', []):\n",
    "        input_for_analysis += f\"依存データ({dep_id}): {str(state.get('collected_data',{}).get(dep_id,'データなし'))[:100]}...\\n\"\n",
    "    analysis = f\"分析結果 ({task['instruction']}):\\n{input_for_analysis}上記データから、重要な洞察として「パターンX」と「傾向Y」が抽出されました。これらは相互に関連しています。\"\n",
    "    print(f\"  -> 分析結果 ({task_id}): {analysis}\")\n",
    "    updated_collected_data = state.get(\"collected_data\", {}).copy()\n",
    "    updated_collected_data[task_id] = analysis \n",
    "    updated_plan = [p.copy() for p in plan]\n",
    "    for p_task in updated_plan: \n",
    "        if p_task['task_id'] == task_id: p_task['status'] = 'completed'; p_task['result'] = analysis; break\n",
    "    return {\"collected_data\": updated_collected_data, \"research_plan\": updated_plan, \"analysis_summary\": analysis, \"messages\": [AIMessage(content=f\"分析タスク「{task_id}」完了。サマリーを生成しました。\", name=\"DataAnalyst\")]}\n",
    "\n",
    "def report_generator_worker(state: CollaborativeResearchBotState) -> dict:\n",
    "    task_id = state.get(\"current_task_id_being_processed\")\n",
    "    plan = state.get(\"research_plan\", [])\n",
    "    task = next((t for t in plan if t['task_id'] == task_id), None)\n",
    "    if not task: return {\"error_message_for_chapter4\": f\"タスクID {task_id} がプランに見つかりません。\"}\n",
    "    print(f\"\n",
    "[ReportGeneratorワーカー] タスク「{task['instruction']}」を実行します。\")\n",
    "    input_for_report = \"\"\n",
    "    for dep_id in task.get('dependencies', []):\n",
    "        input_for_report += f\"参照データ({dep_id}): {str(state.get('collected_data',{}).get(dep_id,'データなし'))[:100]}...\\n\"\n",
    "    report = f\"最終レポート: {task['instruction']}\\n{input_for_report}上記情報を総合的に判断し、以下に要点をまとめました。\\n1. 主要な発見点A\\n2. 注目すべき傾向B\\n3. 今後の課題C\\n以上が本件に関する報告です。\"\n",
    "    print(f\"  -> レポートドラフト ({task_id}): {report[:150]}...\")\n",
    "    updated_plan = [p.copy() for p in plan]\n",
    "    for p_task in updated_plan: \n",
    "        if p_task['task_id'] == task_id: p_task['status'] = 'completed'; p_task['result'] = report; break\n",
    "    return {\"report_draft\": report, \"research_plan\": updated_plan, \"messages\": [AIMessage(content=f\"レポート生成タスク「{task_id}」完了。\", name=\"ReportGenerator\")]}\n",
    "\n",
    "def overall_supervisor_node(state: CollaborativeResearchBotState) -> dict:\n",
    "    print(f\"\n",
    "[統括スーパーバイザー] 現在フェーズ: {state.get('current_phase')}, 直前処理タスクID: {state.get('current_task_id_being_processed')}\")\n",
    "    current_phase = state.get(\"current_phase\")\n",
    "    plan = state.get(\"research_plan\", [])\n",
    "    next_phase: Optional[ResearchPhase] = None\n",
    "    next_task_id_to_process: Optional[str] = None\n",
    "    supervisor_log = \"\"\n",
    "\n",
    "    if state.get(\"error_message_for_chapter4\"): next_phase = \"ERROR\"; supervisor_log = f\"エラー発生: {state['error_message_for_chapter4']}\"\n",
    "    elif current_phase == \"PLANNING\": next_phase = \"RESEARCHING\"; supervisor_log = \"計画に基づきリサーチ開始。\"\n",
    "    elif current_phase == \"RESEARCHING\":\n",
    "        pending_task = next((t for t in plan if t['status'] == 'pending' and t['assigned_to'] == 'WebSearcher'), None)\n",
    "        if pending_task: next_task_id_to_process = pending_task['task_id']; next_phase = \"RESEARCHING\"; supervisor_log = f\"リサーチタスク「{next_task_id_to_process}」へ。\"\n",
    "        else: next_phase = \"ANALYZING\"; supervisor_log = \"全リサーチ完了。分析へ。\"\n",
    "    elif current_phase == \"ANALYZING\":\n",
    "        pending_task = next((t for t in plan if t['status'] == 'pending' and t['assigned_to'] == 'DataAnalyst'), None)\n",
    "        if pending_task:\n",
    "            deps_completed = all(state.get(\"collected_data\",{}).get(dep_id) for dep_id in pending_task.get('dependencies',[]))\n",
    "            if deps_completed: next_task_id_to_process = pending_task['task_id']; next_phase = \"ANALYZING\"; supervisor_log = f\"分析タスク「{next_task_id_to_process}」へ。\"\n",
    "            else: supervisor_log = f\"分析タスク「{pending_task['task_id']}」の依存未解決。待機。\"\n",
    "        else: next_phase = \"REPORTING\"; supervisor_log = \"全分析完了。レポート作成へ。\"\n",
    "    elif current_phase == \"REPORTING\":\n",
    "        pending_task = next((t for t in plan if t['status'] == 'pending' and t['assigned_to'] == 'ReportGenerator'), None)\n",
    "        if pending_task:\n",
    "            deps_completed = all(state.get(\"collected_data\",{}).get(dep_id) for dep_id in pending_task.get('dependencies',[]))\n",
    "            if deps_completed: next_task_id_to_process = pending_task['task_id']; next_phase = \"REPORTING\"; supervisor_log = f\"レポートタスク「{next_task_id_to_process}」へ。\"\n",
    "            else: supervisor_log = f\"レポートタスク「{pending_task['task_id']}」の依存未解決。待機。\"\n",
    "        else: next_phase = \"DONE\"; supervisor_log = \"全レポート作成完了。終了。\"; return {\"current_phase\": next_phase, \"final_report\": state.get(\"report_draft\"), \"messages\":[AIMessage(content=supervisor_log, name=\"OverallSupervisor\")]}\n",
    "    \n",
    "    if next_phase:\n",
    "        print(f\"  -> 統括SV判断: 次フェーズ「{next_phase}」{', タスクID「'+next_task_id_to_process+'」' if next_task_id_to_process else ''}. ログ: {supervisor_log}\")\n",
    "        return {\"current_phase\": next_phase, \"current_task_id_being_processed\": next_task_id_to_process, \"messages\": [AIMessage(content=supervisor_log, name=\"OverallSupervisor\")]}\n",
    "    \n",
    "    print(f\"  -> 統括SV判断: フェーズ「{current_phase}」で待機または進展なし。\")\n",
    "    return {\"messages\": [AIMessage(content=f\"フェーズ「{current_phase}」で待機中。\", name=\"OverallSupervisor\")]}\n",
    "\n",
    "# --- 3. ルーター定義 ---\n",
    "def route_by_current_phase(state: CollaborativeResearchBotState) -> str:\n",
    "    phase = state.get(\"current_phase\")\n",
    "    task_id = state.get(\"current_task_id_being_processed\")\n",
    "    plan = state.get(\"research_plan\", [])\n",
    "    print(f\"  -> ルーター(PhaseRouter): フェーズ「{phase}」、タスクID「{task_id}」\")\n",
    "\n",
    "    if phase == \"ERROR\": return END # エラーなら即終了\n",
    "    if phase == \"DONE\": return END\n",
    "\n",
    "    # どの専門エージェントを呼び出すか決定\n",
    "    if task_id:\n",
    "        task_to_run = next((t for t in plan if t['task_id'] == task_id and t['status'] == 'pending'), None)\n",
    "        if task_to_run:\n",
    "            assigned_worker = task_to_run['assigned_to']\n",
    "            if assigned_worker == \"WebSearcher\": return \"web_searcher\"\n",
    "            if assigned_worker == \"DataAnalyst\": return \"data_analyst\"\n",
    "            if assigned_worker == \"ReportGenerator\": return \"report_generator\"\n",
    "    \n",
    "    # 適切なワーカーが見つからない、またはタスクIDがない場合は統括スーパーバイザーに戻す\n",
    "    return \"overall_supervisor\"\n",
    "\n",
    "# --- 4. グラフ構築 ---\n",
    "workflow_q5_ch4 = StateGraph(CollaborativeResearchBotState)\n",
    "workflow_q5_ch4.add_node(\"ui_agent\", ui_agent_clarifier)\n",
    "workflow_q5_ch4.add_node(\"planner_supervisor\", planning_supervisor_agent)\n",
    "workflow_q5_ch4.add_node(\"overall_supervisor\", overall_supervisor_node)\n",
    "workflow_q5_ch4.add_node(\"web_searcher\", web_searcher_worker)\n",
    "workflow_q5_ch4.add_node(\"data_analyst\", data_analyst_worker)\n",
    "workflow_q5_ch4.add_node(\"report_generator\", report_generator_worker)\n",
    "\n",
    "workflow_q5_ch4.set_entry_point(\"ui_agent\")\n",
    "workflow_q5_ch4.add_edge(\"ui_agent\", \"planner_supervisor\")\n",
    "workflow_q5_ch4.add_edge(\"planner_supervisor\", \"overall_supervisor\")\n",
    "\n",
    "workflow_q5_ch4.add_conditional_edges(\"overall_supervisor\", route_by_current_phase,\n",
    "    {\"web_searcher\": \"web_searcher\", \"data_analyst\": \"data_analyst\", \n",
    "     \"report_generator\": \"report_generator\", \"overall_supervisor\": \"overall_supervisor\", END: END})\n",
    "\n",
    "workflow_q5_ch4.add_edge(\"web_searcher\", \"overall_supervisor\")\n",
    "workflow_q5_ch4.add_edge(\"data_analyst\", \"overall_supervisor\")\n",
    "workflow_q5_ch4.add_edge(\"report_generator\", \"overall_supervisor\")\n",
    "\n",
    "graph_q5_ch4 = workflow_q5_ch4.compile()\n",
    "try: display(Image(graph_q5_ch4.get_graph().draw_png()))\n",
    "except Exception as e: print(f\"グラフ描画失敗: {e}\")\n",
    "\n",
    "# --- 5. グラフの実行 ---\n",
    "user_main_request_q5 = \"LangGraphを使ったマルチエージェントシステム構築のベストプラクティスについて包括的に調査し、その結果を詳細な技術レポートとしてまとめてください。\"\n",
    "initial_state_q5_ch4 = {\n",
    "    \"messages\": [HumanMessage(content=user_main_request_q5)], \"user_original_request\": user_main_request_q5,\n",
    "    \"clarified_request\": None, \"research_plan\": None, \"collected_data\": {},\n",
    "    \"analysis_summary\": None, \"report_draft\": None, \"review_feedback\": None,\n",
    "    \"final_report\": None, \"current_phase\": None, \n",
    "    \"current_task_id_being_processed\": None, \"error_message_for_chapter4\": None\n",
    "}\n",
    "thread_q5 = {\"configurable\": {\"thread_id\": f\"collab-research-bot-{uuid4()[:4]}\"}}}\n",
    "\n",
    "print(f\"--- 協調型リサーチボットテスト (リクエスト: {user_main_request_q5}) ---\")\n",
    "final_q5_val = None\n",
    "for event_idx, event in enumerate(graph_q5_ch4.stream(initial_state_q5_ch4, config=thread_q5, recursion_limit=25)):\n",
    "    print(f\"Event {event_idx}: {event}\")\n",
    "    if END in event: final_q5_val = event[END]\n",
    "    print(\"----\");\n",
    "\n",
    "if not final_q5_val: final_q5_val = graph_q5_ch4.get_state(thread_q5).values\n",
    "\n",
    "print(\"\n",
    "  最終成果物:\")\n",
    "if final_q5_val.get(\"final_report\"):\n",
    "    print(f\"    {final_q5_val['final_report']}\")\n",
    "elif final_q5_val.get(\"error_message_for_chapter4\"):\n",
    "    print(f\"    エラー終了: {final_q5_val['error_message_for_chapter4']}\")\n",
    "else:\n",
    "    print(f\"    最終成果物がありませんでした。最終フェーズ: {final_q5_val.get('current_phase')}\")\n",
    "print(\"\n",
    "  収集データ概要:\")\n",
    "for task_id, data_item in final_q5_val.get(\"collected_data\", {}).items():\n",
    "    print(f\"    - {task_id}: {str(data_item)[:100]}...\")\n",
    "``````\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
